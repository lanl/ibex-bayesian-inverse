\documentclass[12pt]{article}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{caption}
\usepackage{dcolumn}
\usepackage{filemod}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{verbatim}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=7in
\textheight=8.75in
\topmargin=-.5in
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\footskip=0.5in

\begin{document}

\title{Computer Model Calibration for Spatially Distributed
Counts} \author{Steven D. Barnett\thanks{Corresponding author
\href{mailto:sdbarnett@vt.edu}{\tt sdbarnett@vt.edu}, Department of Statistics,
Virginia Tech} \and Robert B. Gramacy\thanks{Department of Statistics, Virginia
Tech} \and Lauren J. Beesley\thanks{Statistical Sciences Group, Los Alamos
National Laboratory} \and Dave Osthus\footnotemark[3] \and Yifan Huang\thanks{
Nuclear and Particle Physics, AstroPhysics and Cosmology, Los Alamos National
Laboratory} \and Fan Guo\footnotemark[4] \and Eric J. Zirnstein\thanks{
Department of Astrophysical Sciences, Princeton University} \and Daniel B.
Reisenfeld\thanks{Space Science and Applications Group, Los Alamos National
Laboratory}}
\date{}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
The Interstellar Boundary Explorer (IBEX) satellite detects energetic neutral
atoms (ENAs) and determines the rate at which they are generated in the
heliosphere. Computer models attempt to represent the physical process that
produces heliospheric ENAs through specified model parameters. Computer model
calibration enables statisticians to use data collected in a field experiment
along with simulator output from a variety of parameter settings to make
out-of-sample predictions and learn the posterior distributions of model
parameters. However, calibration typically assumes a Gaussian or continuous
response for both field and computer model data. We introduce a novel Markov
chain Monte Carlo calibration framework that accommodates the spatially
distributed Poisson counts collected by the IBEX satellite. Additionally, the
computer simulation in our application is limited in the amount of runs it can
perform, but each run's output is quite large. Therefore, we propose the use of
the Scaled Vecchia approximation as a Gaussian process (GP) surrogate. We
demonstrate the capability of our proposed framework through multiple simulated
examples and show that we can consistently recover the true parameters in a
discrepancy-free environment and obtain accurate out-of-sample prediction. We
apply this to the IBEX satellite data and the corresponding computer model
output.

\bigskip
\noindent \textbf{Key words:} computer experiments, simulator, Gaussian process
 surrogate, ordinal response, Vecchia approximation, Bayesian inference,
 heliospheric science, IBEX
\end{abstract}

\doublespacing % no double spacing for arXiv

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Layout
%
% SECTION 1: Introduction
%% Reference Grosskopf and Bingham paper

% SECTION 2: Review:
%% A general review of computer model calibration
%% Review Kennedy O'Hagan framework
%% Maybe introduce a toy example (could be the mean from the poisson example, treated as normal with noise)

%% SECTION 2.1: Gaussian Process Surrogates
%%% Motivate need for surrogate from previous section. Expensive computer model and field data.
%%% Review Gaussian process surrogates and their use in surrogate modeling

%% SECTION 2.2: Approximated Gaussian Process Surrogates
%%% List versions of approximations for GPs

%%% SECTION 2.2.1: SEPIA
%%% SEPIA. Review Dave Higdon's 2004 and 2008 papers. PCA

%% Limitations: 
%% Poisson response. Clunky way to deal with large data. Not generalizable.

% SECTION 3: Methods

%%% SECTION 3.1: Generalized Computer Model Calibration
%%%%% Maybe introduce toy example here
%%%%% Rehash the need to update KOH to account for Poisson data
%%%%% Explain the use of a no-bias set up to initially prove the capability
%%%%% Briefly show the math behind this

%%% SECTION 3.2: Poisson Response Examples
%%%%% Results of 1D Poisson calibration (prediction, distribution of calibration parameters)
%%%%% Results of 2D Poisson calibration (prediction, distribution of calibration parameters)

%%% SECTION 3.3: Vecchia Approximation
%%%%%
%%%%%
%%%%%

%%% SECTION 3.4: Scaled Vecchia Approximation
%%%%%
%%%%%
%%%%%

%%% SECTION 3.5: Scaled Vecchia Approximation
%%%%% Go back to 1D and 2D Poisson examples, but now with limited model runs
%%%%% Tease that this example is easy because the amount of data we have is small
%%%%% Increase to large data example. Show inability of framework to fit surrogate without Vecchia

% SECTION 4: Heliospheric Science
%%% Introduce the idea of the heliosphere?

%%% SECTION 4.1: Computer Model
%%% Introduce the computer model

%%% SECTION 4.2: Benchmark against synthetic data
%%% Run and evaluate experiements

% SECTION 5: IBEX SKY MAP - Real data
%%% Introduce the satellite
%%% Introduce what our data looks like

%%% SECTION 5.1: Benchmark against real data data
%%% Run and evaluate experiements

%%% SECTION 5.2: Discrepancy
%%% Note that discrepancy was explored
%%% Maybe show some plots that indicate a simple discrepancy is insufficient
%%% Lead into discussion

% SECTION 6: Discussion

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

%  The computer model calibration literature is great when you have noisy,
%   Gaussian field data and smooth computer model you can model with GP(and not TOO big)
%   However, not all calibration fit into that mold.
%  For example, we are working with satelite detector counts. IBEX is giving a lot of count data <- that is big
%   computer model data is big too. other problems don't fit into that mold.
%  Doing it over latitude and longitude
%  WAY different than what I have.
%  Keep in all the cites, but make it more compact.
%  move toy problem to a review
%  either way, get to the novel contribution QUICKER (at least page 2)
%  Go for 3-4 pages. but still give the scientific impact!
%  Include figure 1 <- image of raw data, and maybe a couple model outputs

%  Cites:
%  helps the reader if someone else has done this before
%  maybe the sentence isn't necessary.
%  any paper on calibration with parameters themselves of interest
%  scott's paper has some good cites for this.
%  papers in the intro: Bayarri et al. 2009, Higdon et al. 2004; Brynjarsdottir and O'Hagan, 2014
%  should we deflect from parameter inference??
%  what about additional source of data cite:
%  scott's paper, radiative shock, other one with Derek.

The computer model calibration literature provides statisticians with tools to
improve out-of-sample prediction and estimate unknown calibration parameters
when extensive physical experimentation is infeasible, field data are noisy,
Gaussian observations, and simulator data are both smooth and small enough to
be modeled with a Gaussian process. However, not all scientific problems that
require calibration fit into this mold. For example, our motivating
application produces counts of particles detected by a satelite and
corresponding computer model output that is smooth, but extremely
high-dimensional. This is only one such problem, but certaintly many other
situations fall outside the standard computer model calibration set up.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.74,trim=0 0 65 0,clip=TRUE]{ibex_real1.pdf}
\includegraphics[scale=0.74,trim=35 0 65 0,clip=TRUE]{sim1.pdf}
\includegraphics[scale=0.74,trim=35 0 0 0,clip=TRUE]{sim2.pdf}
\caption{Observed rate of energetic neutral atoms (ENA) detected by the
Interstellar Boundary Explorer (IBEX) satellite (left) and output from two
runs of the IBEX computer simulation at different settings (middle, right).
\label{f:fig1}}
\end{figure}
Statistical computer model calibration utilizes both data collected from a
physical experiment and data generated from a computer simulation
representative of the physical process of interest. Often, the physical
experiment is either too expensive to run a sufficient number of times (e.g.
launching hundreds of IBEX satellites) or is unethical to conduct. In this
way, a computer model representing the physical process allows a practitioner
to learn more about the physical process without the prohibitive expense. Use
of this additional source of data (even with its potential bias) has been
shown to improve prediction and uncertainty quantification out-of-sample.
Additionally, computer model calibration allows scientists to manipulate
parameters that are often impossible to control in a physical experiment (e.g.
gravity). That barrier does not exist in a computer simulation. These
calibration or tuning parameters are often of high scientific interest.
Varying them helps experimenters explore more of the input space, understand
the distribution of these parameters, and even make inferential claims about
them. Parameters in computer simulations developed by space scientists in our
motivating example that potentially explain observed phenomena fall in this
category.

Computer experiments \citep{santner2018design} do not face the same ethical
and practical obstacles of physical experiments, but they can be
computationally expensive. Although some simulations are very quick to
evaluate \citep{higdoncalib2004} some take hours, or even days, to complete.
In our motivating example, an individual run of the computer model can take up
to 16 hours, limiting the supply of computer model data. In such situations, a
GP surrogate \citep{sacks1989computerexp, gramacy2020surrogates} is often fit
to the computer output to provide fast predictions out of sample. Adding
another layer of complexity, the output from our simulation is not only
expensive to obtain, but is extremely high dimensional. Each run of the
simulation produces a surface represented by a vector of 16,200 points. GPs
struggle with such big data because they must store and decompose matrices
that grow quadratically with training data size. \citet{higdoncalib2008}
mitigate this with a basis representation via principal components to reduce
the dimension of the data. However, much has changed recently in the landscape
of GP approximation.

The canonical framework for statistical computer model calibration was
proposed by \citet{kennedyohagan2001} (hereafter referred to as KOH). However,
KOH relies on modeling the simulation runs and field data jointly via a
multivariate normal representation. The field data collected by IBEX comes in
as Poisson-distributed counts, preventing use of KOH. Recent research has been
done to generalize calibration to non-Gaussian responses
\citep{grosskopfcountcalib2020}, but empirical examples and applications in
this work are restricted to one-dimensional input spaces. Expanding to higher
dimensions allows broader application and brings with it multiple challenges,
including the need to scale to larger datasets. No approximation method was
necessary in \citet{grosskopfcountcalib2020}'s implementation. As mentioned
previously, \citet{higdoncalib2008} employed PCA to reduce the dimensionality
of the data in a calibration setting. We propose a modern approach using the
Scaled Vecchia approximation \citep{scaledvecchiakatzfuss2022}. In fact, the
application of the Scaled Vecchia approximation in computer model calibration
was suggested by the authors themselves, noting its ability to scale well.
Therefore, our contributions are three-fold: 1) Introduce a MCMC computer
model calibration framework for any type of physical response in high
dimensions (we focus on Poisson-distributed data), 2) Allow for processing
large-scale, high-dimensional computer model output in a reasonable time
frame, and 3) For the first time, illustrate how IBEX data in its raw form can
be used to make inferential claims about the heliosphere.

Our paper is outlined as follows: We review statistical computer model
calibration in Section \ref{sec:review} and focus on the use of GPs as
surrogates of simulator output. Section \ref{sec:gen_calib} introduces a novel
Markov chain Monte Carlo framework (MCMC) as an extension of KOH to account
for non-Gaussian field data \ref{sec:poisson_calib} in computer model
calibration. Section \ref{sec:vecchia} proposes the Scaled Vecchia
Approximation \citep{scaledvecchiakatzfuss2022} as a way to account for
high-dimensional output from the computer model. We illustrate the efficacy of
our method through empirical results for small and large problems in Section
\ref{sec:vecchia}. We finish our contribution through innovative
application to the real IBEX satellite data in Section \ref{sec:ibex}. Section
\ref{sec:discuss} follows up and concludes our paper with a discussion. Code
reproducing all examples is provided at
\url{https://github.com/lanl/IBEX_Calibration}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 2: Review:
\section{Review}
\label{sec:review}

%------------------------------------------------------------------------------
%% SECTION 2.1 - Bayesian Inverse Problems
\subsection{Bayesian Inverse Problems}
\label{sec:bayes_inv}
%------------------------------------------------------------------------------

%% Review Bayesian inverse problems
Bayesian statistical inversion (BSI) is a technique employed by statisticians to pair
observations collected from a physical experiement with a physics-based model to
solve the inverse problem. That is, given some noisy observations from an assumed
model, statisticians seek to determine information about the data-generating process.
The goals are two-fold: 1) estimate the distribution of the parameters that govern
the physical model, and 2) if parameters are known but input is unknown, reconstruct
the inputs from which the observations originated. The motivation for such a method
stems from the inability to either control or precisely measure the factors (e.g. the
force of gravity) in a physical experiment. Therefore, in an effort to gain more
insight, scientists develop mathematical models, often resulting in a computer
simulation, that attempt to represent that physical process. As a result of increased
access to supercomputing resources, computer models have increased in complexity over
the past two decades and are able to produce extremely high-fidelity output. Data
collected from a physical experiment can then be augmented by output from this
computer simulation. In this context, practitioners can vary those parameters that
cannot be controlled in a physical environment. By comparing observations to
simulator output generated via unique combinations of model parameters, statisticians
can estimate the true, underlying parameter values. In BSI, this results in a
distribution of likely values over some subset of the possible inputs. Another
advantage of this additional source of data is that scientists are able to make
better predictions for the behavior of a physical process outside sampled locations.
In either case, BSI deepens our knowledge of physical processes.

Consider observations $Y_F$ collected from a physical experiment at input locations
$X_F$, as depicted in the left panel of Figure \ref{f:toy_calib}. Data $Y_M$ from a
computer model supplement $Y_F$ as we attempt to uncover the underlying real process
$y_R$, shown in black in the center panel, governed by parameters $\boldsymbol
\theta$. Equation \ref{eq:bsi} sets up the BSI framework.
\begin{align}
Y_F(\boldsymbol x) &= y_R + \epsilon \nonumber \\ 
y_R &= Y_M + \delta_b && \begin{bmatrix} Y_F \\ Y_M \end{bmatrix} \sim \left(\begin{matrix}0 \\ 0\end{matrix}, \begin{bmatrix} \Sigma_F & \Sigma_{FM} \\ \Sigma_M & \Sigma_{Mb} \end{bmatrix}\right)
\label{eq:bsi}
\end{align}
%% Review Kennedy O'Hagan framework
In 2001, \citeauthor{kennedyohagan2001} introduced what has since become the
canonical framework for computer model calibration. In this framework, field
data and computer simulation output are modeled jointly as following a
multivariate normal distribution. \citeauthor{kennedyohagan2001} make two key
suggestions: 1) Computer models, despite improvements in recent years, are
still inherently biased. It's virtually impossible to exactly replicate a
physical process through simulation. 2) Although often easier to conduct than
physical experiments, computer models are still expensive in terms of
computation, necesitating a surrogate model for the simulated output to obtain
additional predictions. Equation \ref{eq:koh} lays out the format of their
setup, with notation we introduced earlier.
%% Maybe introduce a toy example (could be the mean from the poisson example, treated as normal with noise)
Consider the left panel of Figure \ref{f:toy_calib}. Red stars denote field
observations of some true, unknown process at different locations $X$. Note
that these data are observed with some noise. Additionally, multiple
observations are made at each location. This is common, due to the expense of
changing configurations of a physical experiment, but it need not be the case.
Dashed, green lines denote individual runs of a computer simulation, at
different settings of parameters $\boldsymbol{\theta}$. Lastly, the black bold
line shows the true, underlying process we want to estimate. Modeling the data
as \citet{kennedyohagan2001} did, we can make field predictions out-of-sample
and estimate the posterior distributions of $\boldsymbol{\theta}$. Predictions
and posterior estimates are shown in the middle and right panels of Figure
\ref{f:toy_calib}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.19,trim=0 0 65 0,clip=TRUE]{toy_calibration.jpg}
\includegraphics[scale=0.19,trim=35 0 65 0,clip=TRUE]{toy_calib_pred.jpg}
\includegraphics[scale=0.19,trim=35 0 0 0,clip=TRUE]{toy_calib_dist.jpg}
\caption{Toy calibration example. Observations (left), predictions (middle),
and posterior distribution (right).
\label{f:toy_calib}}
\end{figure}

% Now, let's go back to our example in Section \ref{sec:poisson_calib}. There, we were
% given a certain amount of observed field data and access to a computer model that
% could immediately spit out results for any unique combination of parameters we were
% interested in. Here, we only have the observed field data and a fixed number of runs
% from our computer model, usually evaluated over some space-filling design.
% Specifically, we use a Latin hypercube sample \citep{mckay1979} of size 40 over our
% two parameters, $\mu$ and $\nu$. So, we need to fit a GP on these 40 runs and make
% predictions at other input combinations of interest using \ref{eq:mvn_eq}.


%------------------------------------------------------------------------------
%% SECTION 2.1 - Gaussian Process Surrogates:
\subsection{Gaussian Process Surrogates}
\label{sec:gp_surr}
%------------------------------------------------------------------------------

%%% Motivate need for Gaussian process surrogates. Expensive computer model and field data.
In most scientific applications, the computer model of interest is
computationally expensive. Due to complex and costly operations (e.g. solving
partial differential equations), simulations can take hours, days, or even
weeks to complete \citep{santner2018design}. This limits the ability of a
practitioner to thoroughly explore the input parameter space of the computer
model, likely reducing both predictive performance for unsampled inputs and
the accuracy of parameter estimates. Therefore, we need a model (what we call
a surrogate, or emulator) fit to the simulator output to make accurate and
reliable predictions for out-of-sample values of our computer model
parameters. Any statistical model can serve as a surrogate, ranging from a
linear regression to complex neural networks \citep{??}. However, the
canonical surrogate for computer experiments and computer model calibration
has come to be the Gaussian process \citep{gramacy2020surrogates}.

%% Review Gaussian processes and their use in surrogate modeling
A Gaussian process, hereafter referred to as a GP, is a realization of a
multivariate normal distribution, governed by some mean and covariance
functions. GPs are fit to observed data and used to make predictions at new
input locations, relying on the conditional multivariate normal or
\textit{kriging} equations \citep{matheron1963principles}. For example,
suppose we have a vector of observations $Y$ at input locations $X$. $Y$ is an
$n \times 1$ vector and $X$ is an $n \times p$ matrix. We make the assumption
that $Y$ is normally distributed with some mean $(\mu(X))$ and covariance
$(\Sigma(X))$, both functions of $X$. Predicting at a new location $x$ is
straightforward. Eq.~(\ref{eq:mvn_eq}) articulates how to obtain mean
predictions $\mu^*(x)$ and corresponding uncertainty quantification
$\Sigma^*(x)$.
\begin{align}
Y(X) &\sim \mathcal{N}\!\left(\mu(X), \Sigma(X)\right) \nonumber \\
y^*(x) \mid Y(X) &\sim \mathcal{N}\!\left(\mu^*(x), \Sigma^*(x)\right) \nonumber \\
\mu^*(x)&=\mu(x) + \Sigma(x, X) \Sigma(X)^{-1} (Y - \mu(X)) \nonumber \\
\Sigma^*(x)&=\Sigma(x, x) - \Sigma(x, X) \Sigma(X)^{-1} \Sigma(X, x) \label{eq:mvn_eq}
\end{align}
Although a generic mean function is specified in Eq.~(\ref{eq:mvn_eq}), it is
common to set $\mu(x)=0$. In practice, this simply implies centering $Y$ by
subtracting off the sample mean, $\overline{Y}$. In this zero-mean context,
all the action takes place in the covariance function. The covariance function
typically depends on pairwise distances between input locations $x$ and $x'$.
Many covariance kernels exists, such as the Gaussian, Exponential, Matern. We
use the Matern kernel in our work, of which both the Gaussian and Exponential
kernels are special cases. The behavior of these kernels is determined by
three hyperparameters: a nugget $g$, a lengthscale $\theta$, and a scale
$\tau^2$. In the Matern kernel, an additional parameter ($\nu$) exists, which
controls the smoothness of the estimated surface. But we will not estimate the
smoothness parameter here and instead set $\nu =
\frac{5}{2}$ (\ref{eq:matern}), following common practice due to it allowing
the kernel function to be expressed in a simpler form. All hyperparameters can
be estimated using your preferred method (i.e. maximum likelihood, method of
moments). We will use Bayesian methods, specifically MCMC.
\begin{align}
\Sigma(x, x'|\sigma^2, \theta, \nu, g) &= \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left(\sqrt{2\nu} \frac{\|x-x'\|}{\theta}\right)^2 K_{\nu} \left(\sqrt{2 \nu}
\frac{\|x - x'\|}{\theta}\right) + g \cdot \mathbb{I} \nonumber \\
&= \sigma^2 \left(1 + \frac{\sqrt{5} \cdot \|x - x'\|}{\theta} +
\frac{5\cdot\|x-x'\|^2}{3\theta^2} \right) \text{exp}\left(-\frac{\sqrt{5} \cdot \|x
- x'\|}{\theta}\right) + g \delta_{x, x'}
\label{eq:matern}
\end{align}
GPs provide a flexible, nonparametric regression tool. In addition to computer
experiments, they are utilized in geostatistics
\citep{matheron1963principles}, spatial statistics
\citep{cressie2011statistics}, time series \citep{roberts2013gaussian},
optimization \citep{jones1998efficient}, and machine learning
\citep{rasmussen2003gaussian}. The reason for their popularity as surrogate
models in the world of computer experiments is their ability to interpolate
between observed data points (i.e. setting nugget $g = 0$). Although
stochastic computer models are becoming more common, computer experiments are
often deterministic, motivating the desire for a model that interpolates. GPs
are then useful by providing predictive surfaces that give higher estimates of
variance away from observed data, and virtually no variance at observed
locations. In fact, perhaps the hallmark of GPs is that they naturally provide
excellent uncertainty quantification for out-of-sample predictions, a matter
of importance to scientists exploring these computer models. Therefore, GPs
are widely used in computer model calibration, modeling both the simulator of
interest and a discrepancy term introduced in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%------------------------------------------------------------------------------
\subsubsection{Principal Components Analysis}
\label{sec:pca}
%------------------------------------------------------------------------------

In the early 2000s, \citet{higdoncalib2008} faced a similar challenge. Expensive
computer models at Los Alamos National Laboratory (LANL) produced limited runs with
extremely high-dimensional output in the form of images, shape description, time
series, etc. In order to efficiently fit a surrogate model to the simulator, they
needed to perform an approximation to limit the size of the data. As a solution, they
employed principal components analysis (PCA) to reduce the dimensionality of the
response. Specifically, they represented the output of a simulator model as the sum
of $n_k$ basis functions $\textbf{k}_1, \textbf{k}_2, \ldots , \textbf{k}_{n_k}$,
each scaled by a corresponding weight $w_i$, as shown in Eq. \ref{eq:higdon_pca}.
Instead of modeling the high-dimensional response, the GP only modeled the small set
of component weights. Bayesian inference was used to obtain the posterior
distribution of those component weights. Combining estimates the weights with the
calculated basis functions allowed for predictions out of sample. As a result of this
approximation, the dimension of the covariance matrix $\Sigma(X)$ in the predictive
equations of \ref{eq:mvn_eq} remained small.
\begin{align}
\textbf{y}(\textbf{x}, \textbf{u}) \approx \sum_{i=1}^{n_k} \textbf{k}_i \cdot w_i (\textbf{x}, \textbf{u}), \quad n_k \ll n \ \nonumber \\
w_i (\textbf{x}, \textbf{u}) \sim GP(\textbf{0}, \Sigma_{w_i}),
\label{eq:higdon_pca}
\end{align}
where $\Sigma_{w_i}$, dependent only on a limited number of combinations of
$\textbf{x}$ and $\textbf{u}$, is a $n_k \times n_k$ matrix. Hence, the decomposition
of $\Sigma_{w_i}$ avoids the O($n^3$) bottleneck in computation and can be completed
in a reasonable amount of time.

We propose a simpler approach, devoid of the definition and selection of how many
basis functions to use. Although this basis representation was successful, we argue
that what \citet{higdoncalib2008} really wanted to do was represent the functional
output as a univariate response. That is, replicate the entry in the input matrix $X$
for each response the number of times equal to the length of the output vector. In
essence, implicitly encode the index (e.g. pixel in image, moment in time series)
into the response vector for each observation in the input matrix. We believe that
had \citet{higdoncalib2008} had access to the capacity of modern-day computing, along
with the novel methodology proposed since the early 2000s that leverages that
compute, they would have steered their implementation in a significantly different
way. We propose that approach next and illustrate the improved performance in both
predictive accuracy and uncertainty quantification, along with a reduction in
computational expense.

%% Limitations:
%% Poisson response. Clunky way to deal with large data. Not generalizable.

\section{Methods}
\label{sec:methods}

%------------------------------------------------------------------------------
\subsection{Vecchia Approximation}
\label{sec:vecchia}
%------------------------------------------------------------------------------

%%% List versions of approximations for GPs
Perhaps the primary weakness of GPs is the need to decompose an $n \times n$
covariance matrix, making it difficult to scale to large datasets. Many methods have
been introduced to address this computational burden. These include, but are not
limited to, fixed-rank kriging \citep{cressie2008fixed}, inducing points
\citep{banerjee2008gaussian}, compactly supported kernels
\citep{kaufman2011efficient}, divide-and-conquer \citep{gramacy2015local}, and
nearest neighbors \citep{datta2016hierarchical, wu2022variational}. In the context of
computer model calibration, \citet{higdoncalib2008} use a basis representation via
principal components to accommodate high-dimensional output. However, with the
dramatic increase in computing capacity over the past decade and the aforementioned
advances in methodology, we believe there are better GP approximations available to
model the output of a computer simulation. We propose the recently popularized
Vecchia approximation \citep{vecchia1988estimation, katzfuss2020vecchia,
katzfuss2021general, zhang2022multi}.

The Vecchia approximation relies on the ability to write a likelihood function as the
product of conditional likelihoods as shown below.
\begin{align}
L(Y)&=\prod_{i=1}^n L(Y_i \mid Y_{g(i)})
\hspace{0.3em} \mbox{ where } g(1)=\emptyset \hspace{0.3em} \mbox{ and } g(i)=\{1, 2,
\ldots, i-1\} \nonumber \\
&\approx \prod_{i=1}^{n}L(Y_i \mid Y_{h(i)}) \hspace{0.3em} \mbox{ where } h(i)
\subset \{1, 2, \ldots, i-1\} \hspace{0.3em} \mbox{ and } |h(i)| = m \ll n
\label{eq:vecchia}
\end{align}
Reducing the size of the conditioning set ($h(i)$ above) induces sparsity in the
precision matrix, speeding up the evaluation of the likelihood. This takes an
operation of order $O(n^3)$ down to $O(nm^3)$, motivating small values of $m$. As the
size of the conditioning set $m$ increases, the quality of the approximation
improves, until $n=m-1$, and the likelihood is no longer an approximation. We
find a value of $m < 30$ is more than sufficient. The conditioning set can be
constructed in many ways, but we default to a nearest-neighbors approach, which has
been done previously \citep{sauer2023active}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalized Computer Model Calibration}
\label{sec:gen_calib}

%% Rehash the need to update KOH to account for Poisson data
%% Explain the use of a no-bias set up to initially prove the capability
%% Briefly show the math behind this

Computer models are not limited to modeling physical processes that generate a
continous response. For instance, a field experiment may return a binary
response, such as success or failure. Categorical responses may expand beyond
two classes, representing a multinomial reponse. We focus on the
Poisson-distributed counts in this paper, but our framework can be generalized
to any response.

% Introduce a charicature of the IBEX problem
%% Progression of toy example:
%%% 1) Limited field data, unlimited, low-dimensional simulator output
%%% 2) Limited field data, limited, low-dimensional simulator output
%%% 3) Limited field data, limited, high-dimensional simulator output

Now, before we get ahead of ourselves, let's start with a simple problem.
Suppose we are given counts collected from a physical experiment at different
input locations ($x_i$), as shown in Figure \ref{f:logit1_examp}. We know that
the underlying mean process is of the form $\lambda_i=\mu + \nu f(x_i)$. We are
unable to modify $\mu$ and $\nu$ in a physical setting, but we can vary these
values in a trivial, one-line ``computer model.'' If we consider all unique
combinations for the values of $\mu \in \{5, 7.5, 10, 12.5, 15\}$ and $\nu \in
\{0.25, 0.625, 1, 1.375, 1.75\}$, we end up with 25 different functional
evaluations (depicted as gray lines in \ref{f:logit1_examp}). Our goal is
two-fold: 1) Make accurate predictions at new, unobserved physical locations
($x_i'$) with good uncertainty quantification (UQ), and 2) Estimate the
distribution of calibration parameters $\mu$ and $\nu$ to better understand
how the mean process is constructed.

Consider the output displayed in Figure \ref{f:logit1_examp}. Suppose we are given
counts collected from a physical experiment at different input locations ($x_i$). We
know that the underlying mean process is of the form $\lambda_i=f(x_i, \mu, \nu)$. We
are unable to modify $\mu$ and $\nu$ in a physical setting, but we can vary these
values in a computer model. We run the simulation for a variety of unique
combinations for the values of $\mu$ and $\nu$, resulting in 25 different functional
evaluations (depicted as gray lines in \ref{f:logit1_examp}). Our goal is two-fold:
1) Make accurate predictions at new, unobserved physical locations ($x_i'$) with good
uncertainty quantification (UQ), and 2) Estimate the distribution of calibration
parameters $\mu$ and $\nu$ to better understand how the mean process is constructed.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_obs.pdf}
\caption{Counts from a Poisson process at varying $x_i$ locations along with
evaluations of $\lambda_i=f(x_i, \mu, \nu)$ for different values of $\mu$ and
$\nu$.
\label{f:logit1_examp}}
\end{figure}

The canonical KOH computer model calibration framework is limited to a
continuous response from both the computer model and the field experiment, due
to it's reliance on modeling the two outputs jointly via a multivariate normal
distribution. Faced with spatially distributed counts from the IBEX satellite,
we could not apply KOH to our problem. We propose a fully Bayesian Markov
chain Monte Carlo (MCMC) framework to sample from the posterior distribution
of our calibration parameters. At each MCMC iteration, we propose new
calibration parameters, generate computer model output at those new settings,
and evaluate the Poisson likelihood of the observed data, given the simulated
mean surface.

%%%%% Explain the use of a no-bias set up to initially prove the capability
KOH makes the presumably correct assumption that bias is inherent in any computer
model attempting to represent a physical process. Therefore, KOH include a term for
the discrepancy between model and reality. We initially ignore the bias to simplify
the setup and prove the framework's capability. We will address discrepancy later in
the paper.

%%%%% Briefly show the math behind this

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poisson Counts Response}
\label{sec:poisson_calib}
\begin{itemize}
  \item Show diagram of updated framework (McMC with Poisson likelihood)
  \item What if we have unlimited runs of the simulation?? Dave's 2004 paper
  \item Set it up to have a Poisson response
  \item What if we need a surrogate? Computer model takes a long time to run
  \item Set it up to have a Poisson response, but now our data is limited
\end{itemize}

\subsubsection{Examples}

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_est.pdf}
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_post_draws.pdf}
\caption{Evaluations of model simulation at posterior draws (left) and
posterior draws of calibration parameters $\mu$ and $\nu$ (right).
\label{f:calib1d_nosurr}}
\end{figure}

\begin{itemize}
  \item Results of 2D Poisson calibration (prediction, distribution of calibration
  parameters)
\end{itemize}

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5, trim=30 15 40 0,clip=TRUE]{logit2_obs.pdf}
\includegraphics[scale=0.5, trim=30 15 95 0,clip=TRUE]{logit2_model1.pdf}
\includegraphics[scale=0.5, trim=75 15 40 0,clip=TRUE]{logit2_model2.pdf}
\caption{Poisson draws from a mean process over $x_1$ and $x_2$ (left).
Evaluations of a computer model at different settings of four parameters
($\mu_1$, $\nu_1$, $\mu_2$, $\nu_2$) attempting to represent the true mean
process (middle, right).
\label{f:logit2_examp}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Surrogate Modeling}
\label{sec:surr_mod}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Poisson Calibration with GP Surrogate Example}

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_est_wsurr.pdf}
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_post_draws_wsurr.pdf}
\caption{Predictions of GP surrogate at novel $X$ locations with accepted proposals
of calibration parameters (left). Posterior draws of calibration parameters (right).
\label{f:calib1d_surr}}
\end{figure}

The left panel of \ref{f:calib1d_surr} shows evaluations of our GP surrogate at
posterior draws of the calibration parameters, shown on the right. These predictions
are highly accurate, although that is not the focus of our discussion here, so we
forego displaying results. Using those predictions from our surrogate, we can see
that the MCMC does a good job of honing in on the true combination of $\mu$ and
$\nu$. This is evidenced in the right panel of \ref{f:calib1d_surr}. Although the
posterior distribution is not exactly centered on the truth, it covers the truth with
a 95\% credible interval. These draws translate to predictions for what our computer
model would output, through the use of our GP surrogate. The range of evaluations
indicates that use of the surrogate allowed the MCMC to do extensive exploration of
at unobserved inputs, far moreso than the 40 training runs. Most importantly, we see
that our algorithm is able to combine the limited field data with the limited
simulator output to capture the true, underlying mean process.

We can do the same thing in higher dimension, although it's harder to visualize, even
in 2D. But the more pressing barrier to overcome is the need for more runs of the
simulation in higher dimensions in order to obtain good predictions from a surrogate.
In the example above, the number of computer model evaluations was limited ($n <
100$). As mentioned before, computational limits ensure that this is typically the
case in computer model calibration. But note that the output of our computer is
functional, i.e. each response is a vector. Above, each simulation was evaluated at
20 separate points. Therefore, we are really modeling 800 ($40
\times 20$) responses. Fitting a GP on 800 data points is already stretching the
limitations of a GP (decomposing a $800 \times 800$ matrix). But consider our second
example (\ref{f:logit2_examp}). To get 10 unique values of four parameters ($\mu_1$,
$\nu_1$, $\mu_2$, $\nu_2$), a typical threshold for a good surrogate, we'd need
upwards of 10,000 simulator runs. Combine that with a $20 \times 20$ grid over $x_1$
and $x_2$ and our response vector will be greater than 200,000. Even if we used an
smaller LHS space-filling design of size $n=100$, the response vector would consist of
over 40,000 points. In that case, a GP would not feasibly be able to model each
individual point as a separate response. For this problem, we review what has been done
historically with high dimensional output in a calibration setting. Then, we will
propose a modern and better-perfoming approach.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_est.pdf}
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_post_draws.pdf}
\caption{2D POISSON CALIBRATION EXAMPLE WITH GP SURROGATE. Maybe include computation time panel?
\label{f:calib2d_surr}}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=0 10 15 50,clip=TRUE]{ibex_surr_rmse_bp.pdf}
\includegraphics[scale=0.6, trim=0 10 15 50,clip=TRUE]{ibex_surr_crps_bp.pdf}
\caption{Predictive performance for four methods (Scaled Vecchia
Gaussian Processes, Locally Approximated Gaussian Processes (lagp), Fully
Bayesian Gaussian Processes (deepgp1), and SEPIA) compared on RMSE and CRPS on
the IBEX simulator. Each of 66 runs is held out once as test data, with the 65
remaining runs acting as training data.
\label{f:surr_bakeoff}}
\end{figure}

We considered multiple surrogate implementations with GP approximations to model the
computer simulation, such as the {\tt R} packages {\tt laGP} and {\tt deepgp}.
However, due to its superior performance in predictive accurary, uncertainty
quantification, and execution time, we landed on the Scaled Vecchia Approximation
\citep{scaledvecchiakatzfuss2022} to model the limited simulator output and make
predictions at new locations. In fact, when discussing further work,
\citet{scaledvecchiakatzfuss2022} suggest that their method would work as an
effective tool for use in computer model calibration.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 10 15 50,clip=TRUE]{logit1_est.pdf}
\includegraphics[scale=0.6, trim=5 10 15 50,clip=TRUE]{logit1_est.pdf}
\caption{VECCHIA vs. SEPIA vs. DEEPGP vs. LAGP Timing. This could look a lot like
Annie's plot in her Vecchia. One panel has each method, showing the exponential
increase. And one panel has just Vecchia and SEPIA on larger and larger datasets.
\label{f:surr_bakeoff}}
\end{figure}

In Figure \ref{f:surr_bakeoff} we show the results of a bakeoff between each
surrogate model considered on the IBEX simulated data. We will leave the details of
the data to \ref{sec:ibex_sim}. The only important things to note are that there are 66
unique model runs (corresponding to unique combination of the input parameters) and
each run produces a vector of 16,200 values. Quick math comes out to 1,069,200 unique
observations, clearly much too large for an ordinary GP to handle. But even most
approximate GPs struggle with a dataset of this size. For our bakeoff, we conducted
hold-one-out cross-validation. Therefore, each boxplot in Figure \ref{f:surr_bakeoff}
encompasses 66 points. Each method (besides SEPIA) uses a neighborhood size of
$m=25$. Although {\tt laGP} benefits from parallelization, to keep the playing field
level each method was restricted to using one core. {\tt BayesGP} uses MCMC, so due
to time constaints, only 1000 iterations were allowed, with a burn in of 500 and
thinning by 10. It's clear the Scaled Vecchia is superior in all measures. {\tt laGP}
has no fitting time, as each call to predict encompasses the ``fitting'' process.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_est.pdf}
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_post_draws.pdf}
\caption{2D POISSON CALIBRATION EXAMPLE WITH VECCHIA GP SURROGATE. Maybe include
computation time panel here as well. Maybe do calibration with other surrogates?
\label{f:calib2d_surr_vecchia}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interstellar Boundary Explorer}
\label{sec:ibex}

In 2008, NASA launched the IBEX satellite into Earth's orbit as part of their
Small Explorer program \citep{mccomas2009aIBEX}. IBEX's mission is to collect
data that will deepen the scientific community's understanding of the
heliosphere, the egg-shaped bubble encapsulating our solar system. The
heliosphere is created from the solar wind. The solar wind, continously
emitted from the Sun, is primarily made up of hydrogen ions, or lone protons.
These ions travel through the solar system and out into interstellar space. As
the ions reach the boundary of our solar system, they encounter the
termination shock, where they slow down, absorb significant amounts of energy,
and become highly energetic, charged atoms. These hydrogen ions form the
heliosphere and act as a barrier between our solar system and interstellar
space. While in this region called the heliopause (the outer edge of the
heliosphere), the ions can interact with other particles in the interstellar
medium. Sometimes these encounters result in electron exchange, converting the
charged atoms into what heliospheric physicists call energetic neutral atoms
(ENAs). Without charge, these ENAs travel in a straight line, unaffected by
magnetic fields. Some ENAs cross back into the solar system and, depending on
their path, can be detected in Earth's orbit. Determining the rate at which
ENAs are generated in different parts of the heliosphere is key to
understanding the makeup of this region at the boundary of our solar system
and interstellar space.

IBEX records the energy level and approximate location of origin for each ENA
that enters the satellite, providing sufficient data to estimate the rate at
which ENAs are created throughout the heliopause. Specifically, the IBEX
satellite contains an instrument called the IBEX-Hi ENA imager
\citep{funsten2009IBEXHiENA}, which includes a series of plates
\citep{moran2024statistical} that the ENAs travel through to be detected. The
IBEX satellite rotates on an axis, and over a period of six months, is able to
point at every part of the sky for some interval, or exposure time. This
spatial map of ENA rates (referred to by space scientists as a \textit{sky
map}) is used to analyze, make conjectures about, and ultimately develop
theories for the heliosphere, its many properties, and the processes that
govern its creation.

The heliosphere is like a boat moving through water. Here the water represents
interstellar medium. As a boat moves through water, the water is disturbed.
However, the turbulence created is not equal at each location where the boat is
in contact with the water. Turbulence is greater at the front (bow) and back
(stern) than on the sides (port and starboard) of the ship. Prior to the launch
of the IBEX satellite, space scientists had the same expectation for the
heliosphere. Higher rates of ENAs being generated would exist at the stern
(space scientists call this the \textit{nose}) and bow (\textit{tail}) of the
helisophere due to its movement through interstellar space. Data observed found
this to be true. These higher rates are referred to as \textit{globally
distributed flux (GDF)}. But in a completely unexpected finding
\citep{mccomas2009bIBEX}, IBEX also recorded a string of higher rates of ENAs
\citep{fuselier2009IBEXribbon} being generated that curved around the
heliosphere. Scientists now refer to this phenomenom as the \textit{ribbon}.
For the past fifteen years, space scientists have conducted extensive research
and proposed dozens of theories to explain the existence of the ribbon and the
physical process that generates it \citep{mccomas2014ibex, zirnstein2018role,
zirnstein2019strong, zirnstein2021dependence}.

% Introduce existence of computer model simulations (ribbon, GDF).
In conjunction with proposing explanations for the existence of the ribbon and
GDF, theoretical physicists have developed computer models to simulate the
creation of sky maps. These computer simulations rely on a number of
parameters that can be varied, thus modifying the shape and intensity of both
ribbon and GDF. Although many simulations exist, we focus on two that are
publicly available: a GDF model proposed by \citet{zirnstein2021heliosheath}
and a ribbon-only model developed by \citet{zirnsteinGDFsims2015} representing
the proposed Spatial Retention model. The ribbon-only model relies on two
parameters, parallel mean free path and ratio. So far, minimal work has been
done to validate theories or further understand the heliosphere by pairing the
computer model output with real data collected by IBEX.

Scientific investigation of the GDF and ribbon currently involves a long,
complex, and at times cumbersome process. For one, the noisy and irregular
nature of the data has made it hard for space scientists to separate signal
from noise and perform their analyses. First, a smooth surface is generated
from the raw satellite data. Once smooth \textit{sky maps} are created, they
are separated into GDF-only and ribbon-only maps \citet{beesleyribbonsep2023}.
With the source of ENAs separated into two distinct maps, space scientists
perform analysis and conduct research exclusively for either the GDF or ribbon
with the respective map. We believe this process can be improved in two ways.
First, both the response surface modeling and ribbon separation rely on many
assumptions about the data and thus depart from the data generation process
itself. Second, we believe inference about the heliosphere can be done in a
more holistic way using the data in its original form, as counts of ENAs given
some exposure time, regardless of source.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{IBEX Sky Map}
\label{sec:ibex_sky}

Over the past 15 years, physicists at Princeton University and Los Alamos National
Lab have developed theoretical models to explain the existence of the GDF and ribbon,
along with their corresponding shape and intensity. The shape and intensity of the
GDF and ribbon are controlled by four paramters: parallel mean free path, ratio,
parameter3, and parameter4. We leave a more in depth physical interpretation of these
parameters to more appropriate venues (??cite). However, as an example, the parallel
mean free path represents the distance a hydrogen ion travels beyond the heliopause
before interacting with another particle, receiving an electron, and becoming an ENA.
Physicists engaged in this work have developed computer models to represent the
physical model they have proposed. Therefore, given specific values for our four
paramters, we can run the model and output a vector of proposed ENA rates at specific
geographical coordinates. The resolution of the output can be varied, but typical
runs output ~16,000 rates, corresponding to a 2 degree grid over latitude and
longitude.

As stated previously, computer model calibration is concerned with leveraging
simulator output to make more accurate predictions at out-of-sample field locations.
Additionally, the distribution of calibration parameters can be estimated. The latter
purpose is the primary interest of physicists involved in this project, as estimation
of these parameters will give credence to what model best represents the data.
However, we will also show that using computer model output enables us to make better
predictions on holdout satellite data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic IBEX Data}
\label{sec:ibex_sim}

In order to showcase and validate our proposed methodology on the IBEX data, we will
first test on simulated sky maps. To do this, we will treat one output from the
computer model simulation as the ``truth.'' Using this output as the underlying mean
ENA rate, we will draw Poisson counts with exposure time specified by the real IBEX
data. Figure \ref{f:ibex_synth_ex} shows both the ``truth'' from the simulation (top
right) and the ``observed'' simulated data. Note that the spread of the data is as if
it came from orbits of the IBEX satellite, along with strings of missing data along
certain orbits.

Now, treating the simulated data as if it came from the satellite, we run our Poisson
computer model calibration code and estimate the underlying parameters. In Figure
\ref{f:ibex_synth_ex}, the bivariate posterior distribution of parallel mean free
path and ratio are displayed. We can see that our framework does a good job of
obtaining the true combination of parameters, with the posterior mode lying near the
truth. Additionally, the truth falls squarely in the 95\% credible interval. As a
sanity check, we take the value of each parameter at the posterior mode and insert
those into our fitted surrogate model. The predicted output is shown in the bottom
right of Figure \ref{f:ibex_synth_ex}. It is obvious that there are some differences
between the estimated sky map and the ``truth.'' The shape and intensity of the
ribbon in each sky map is slightly different. However, it is also abundantly clear
that each map (truth and estimated) could have reasonably generated the data we
``observe''in the top left plot. Therefore, we can confidently assert that our
framework recovers the truth.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.25,trim=0 0 0 0,clip=TRUE]{sim_ibex_real_calib.png}
\caption{Simulated satellite data based on an output from the computer model (top
left). True simulator output (top right). Estimated calibration parameters (bottom
left). Estimated output via surrogate at estimated calibration parameters (bottom
right).
\label{f:ibex_synth_ex}}
\end{figure}

We can apply this method to each unique combination of calibration parameters within
our simulator output. Due to the inability for the model to provide good
distributions for bounday parameters, we don't consider combinations for which at
least one parameter is on its edge. For instance, when parallel mean free path is
either 500 or 300 and/or ratio is either 0.0001 and 0.1. This results in 36 unique
combinations. For each combination, we follow the same procedure as described above.
First, we elect one unique combination and create simulated observed satellite data
based on the computer model. Next, we remove that combination from the training data
used to fit our Scaled Vecchia Gaussian Process surrogate model. And finally, we run
our calibration framework to retrieve estimates for the calibration parameters. Our
results are shown in Figure \ref{f:synth_estimates_all}. In each plot of the
posterior distributions, we have included the true combination of parameters that
generated the simulated data. Our 95\% credible intervals encompass the truth in 34
of the 36 of the iterations, near the nominal coverage rate we'd expect.

In addition to estimating the calibration parameters, we can utilize our framework to
achieve better prediction at out-of-sample field locations. While we have not dealt
with real data from the satellite just yet, we can simulate satellite data as before
and run cross-validation on holdout sets.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3,trim=0 0 0 0,clip=TRUE]{calib_estimates_all.png}
\caption{Posterior distributions for estimated calibration parameters for each unique
combination of values that has been treated as a holdout set.
\label{f:synth_estimates_all}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Real IBEX Data}
\label{sec:ibex_real}
\begin{itemize}
  \item Illustrate the gap between computer model output and real data
  \item Recall how we will account for discrepancy (PCA?, GP?)
  \item Perform tests on examples where we know what the bias is. How well
    does our method account for this?
  \item Lack of competitors? (could we compare to calibration of
    ribbon-separated maps)
  \item Run our code on IBEX real data.
  \item We could display predictions for different values of the calibration
    parameters. Not sure if this is of interest.
  \item Display distributions on calibration parameters.
\end{itemize}

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3,trim=0 0 0 0,clip=TRUE]{calib_estimates_real.png}
\caption{Posterior distributions for estimated calibration parameters for each year of data IBEX has collected.
\label{f:real_estimates_all}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SECTION 5.2: Discrepancy
%%% Note that discrepancy was explored
%%% Maybe show some plots that indicate a simple discrepancy is insufficient

\subsection{Discrepancy Analysis}
\label{sec:discrepancy}
\begin{itemize}
  \item Illustrate the gap between computer model output and real data
  \item Recall how we will account for discrepancy (PCA?, GP?)
  \item Perform tests on examples where we know what the bias is. How well
    does our method account for this?
\end{itemize}

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3,trim=0 0 0 0,clip=TRUE]{discrepancy.png}
\caption{\textbf{Top row:} Estimated sky maps at values of parameters calibration
returned. \textbf{Middle row:} Satellite data for years 2009-2011. \textbf{Bottom
row:} Difference between top two rows.
\label{f:ibex_discrep}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discuss}

We have introduced an effective tool for computer model calibration where 1) data
from a field experiment is not normally distributed and 2) computer model output is
extremely high-dimensional, demanding an approximation for the Gaussian process
surrogate. In simulated experiments, our framework provides accurate predictions at
unsampled input locations. Additionally, when provided with data where the underlying
calibration parameters are known, we can recover the truth at the nominal rate. We
applied this methodology to data collected from the IBEX satellite, and corresponding
simulator output generating heliospheric sky maps. When faced with real data from the
satellite, our method shows that large discrepancies exist between the computer model
and the physical process it attempts to represent. Further collaboration is needed to
inform the theoretical astrophysicists responsible for model development where the
model is lacking, and improve its fidelity to the truth.

Our framework is only applied to counts assumed to come from a Poisson distributed
process. We encourage other practitioners to use this work in other contexts where
data from the field may come from some other distribution (e.g. Negative Binomial,
Bernoulli, Exponential, etc.). We recognize the limitations of our method. First, the
Scaled Vecchia GP surrogate for the computer model fails to provide full uncertainty
quantification on its parameter estimates. A fully Bayesian framework could be
developed, perhaps with the use of a deep Gaussian process to handle the inherent
non-stationarity in the response surface. Finally, this project motivates the need
for a more exhaustive procedure to perform hypothesis testing for which theoretical
model is most probable, considering the data observed.

\subsection*{Acknowledgments}

RBG and SDB are grateful for funding from NSF CMMI 2152679. This work has been
approved for public release under LA-UR-??-?????. SDB, DO and LJB were funded
by Laboratory Directed Research and Development (LDRD) Project 20220107DR.

\bibliography{poisson_calib}
\bibliographystyle{jasa}

\appendix

\end{document}

%% STEPS:
%% - Write discussion
%% - Redesign intro to be more like Bobby suggested
%% - Review computer model calibration (lit review)
%% - Review KOH (lit review)
%% - Review Higdon SEPIA (lit review)
%% - Introduce generalized computer model calibration (can't model joint MVN)
%%%% - 10/23
%% - Create and introduce 1D poisson example
%% - Write section on GP surrogates
%%%% - 10/23
%% - Expand on toy example and write about needing a surrogate
%% - Talk about Scaled Vecchia
%% - Create and introduce 2D poisson toy example
%% - Write about IBEX stuff
%% - Explain experiment with synthetic
%% - Explain experiment with real data (need for discrepancy)
