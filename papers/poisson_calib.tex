\documentclass[12pt]{article}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{caption}
\usepackage{dcolumn}
\usepackage{filemod}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{verbatim}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=7in
\textheight=8.75in
\topmargin=-.5in
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\footskip=0.5in

\begin{document}

\title{Computer Model Calibration for Spatially Distributed
Counts} \author{Steven D. Barnett\thanks{Corresponding author
\href{mailto:sdbarnett@vt.edu}{\tt sdbarnett@vt.edu}, Department of Statistics,
Virginia Tech} \and Robert B. Gramacy\thanks{Department of Statistics, Virginia
Tech} \and Lauren J. Beesley\thanks{Statistical Sciences Group, Los Alamos
National Laboratory} \and Dave Osthus\footnotemark[3] \and Yifan Huang\thanks{
Nuclear and Particle Physics, AstroPhysics and Cosmology, Los Alamos National
Laboratory} \and Fan Guo\footnotemark[4] \and Eric J. Zirnstein\thanks{
Department of Astrophysical Sciences, Princeton University} \and Daniel B.
Reisenfeld\thanks{Space Science and Applications Group, Los Alamos National
Laboratory}}

\date{\today}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
The Interstellar Boundary Explorer (IBEX) satellite detects energetic neutral
atoms (ENAs) and determines the rate at which they are generated in the
heliosphere. Computer models attempt to represent the physical process that
produces heliospheric ENAs through specified model parameters. Computer model
calibration enables statisticians to use data collected in a field experiment
along with simulator output from a variety of parameter settings to make
out-of-sample predictions and learn the posterior distributions of model
parameters. However, calibration typically assumes a Gaussian or continuous
response for both field and computer model data. We introduce a novel Markov
chain Monte Carlo calibration framework that accommodates the spatially
distributed Poisson counts collected by the IBEX satellite. Additionally, the
computer simulation in our application is limited in the amount of runs it can
perform, but each run's output is quite large. Therefore, we propose the use of
the Scaled Vecchia approximation as a Gaussian process (GP) surrogate. We
demonstrate the capability of our proposed framework through multiple simulated
examples and show that we can consistently recover the true parameters in a
discrepancy-free environment and obtain accurate out-of-sample prediction. We
apply this to the IBEX satellite data and the corresponding computer model
output.

\bigskip
\noindent \textbf{Key words:} computer experiments, simulator, Gaussian process
 surrogate, ordinal response, Vecchia approximation, Bayesian inference,
 heliospheric science, IBEX
\end{abstract}

\doublespacing % no double spacing for arXiv

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

The solar wind, continously emitted from the Sun, is primarily made up of
hydrogen ions, or lone protons. These ions travel through the solar system and
out into interstellar space. As the ions reach the boundary of our solar
system, they encounter the termination shock, where they slow down, absorb
significant amounts of energy, and become highly energetic, charged atoms.
These hydrogen ions form an egg-shaped bubble encapsulating our solar system
called the heliosphere, which acts as a barrier between our solar system and
interstellar space. While in this region called the heliopause (the outer edge
of the heliosphere), the ions can interact with other particles in the
interstellar medium. Sometimes these encounters result in electron exchange,
converting the charged atoms into what heliospheric physicists call energetic
neutral atoms (ENAs). Without charge, these ENAs travel in a straight line,
unaffected by magnetic fields. Some ENAs cross back into the solar system and,
depending on their path, can be detected in Earth's orbit. Determining the rate
at which ENAs are generated in different parts of the heliosphere is key to
understanding the makeup of this region at the boundary of our solar system and
interstellar space.

In this pursuit, the National Aeronautics and Space Administration (NASA)
launched the Interstellar Boundary Explorer (IBEX) into Earth's orbit in 2008
as part of their Small Explorer program \citep{mccomas2009aIBEX}. IBEX
continuously detects ENAs and records their energy level and approximate
location of origin, providing sufficient data to estimate the rate at which
ENAs are created throughout the heliopause. Specifically, the IBEX satellite
contains an instrument called the IBEX-Hi ENA imager
\citep{funsten2009IBEXHiENA}, which includes a series of plates
\citep{moran2024statistical} that the ENAs travel through to be detected. The
IBEX satellite rotates on an axis, and over a period of six months, is able to
point at every part of the sky for some interval, or exposure time. This
spatial map of ENA rates (referred to by space scientists as a \textit{sky
map}) is used to analyze, make conjectures about, and ultimately develop
theories for the heliosphere, its many properties, and the processes that
govern its creation.

The heliosphere is like a boat moving through water. Here the water represents
interstellar medium. As a boat moves through water, the water is disturbed.
However, the turbulence created is not equal at each location where the boat is
in contact with the water. Turbulence is greater at the front (bow) and back
(stern) than on the sides (port and starboard) of the ship. Prior to the launch
of the IBEX satellite, space scientists had the same expectation for the
heliosphere. Higher rates of ENAs being generated would exist at the stern
(space scientists call this the \textit{nose}) and bow (\textit{tail}) of the
helisophere due to its movement through interstellar space. Data observed found
this to be true. These higher rates are referred to as \textit{globally
distributed flux (GDF)}. But in a completely unexpected finding
\citep{mccomas2009bIBEX}, IBEX also recorded a string of higher rates of ENAs
\citep{fuselier2009IBEXribbon} being generated that curved around the
heliosphere. Scientists now refer to this phenomenom as the \textit{ribbon}.
For the past fifteen years, space scientists have conducted extensive research
and proposed dozens of theories to explain the existence of the ribbon and the
physical process that generates it \citep{mccomas2014ibex, zirnstein2018role,
zirnstein2019strong, zirnstein2021dependence}.

%%% No calibration going on in IBEX (super novel)
Scientific investigation of the GDF and ribbon currently involves a long,
complex, and at times cumbersome process. For one, the noisy and irregular
nature of the data has made it hard for space scientists to separate signal
from noise and perform their analyses. For the first 15 years of its mission,
IBEX relied soley on maps produced by the IBEX Science Operations Center
(ISOC). ISOC maps are generated using a simple aggregation of the raw data. But
these maps still retain a significant amount of noise and are fairly low
resolution (each pixel represents a $6\degree \times 6\degree$ area of the
sky). To remedy this problem \citet{osthustheseus2023} propose a method called
Theseus, which takes the raw data and generates a much smoother representation
of the surface, using a series of Generative Additive and Projection Pursuit
Regression models. Once Theseus maps are created, they are separated into
GDF-only and ribbon-only maps \citet{beesleyribbonsep2023}. With the source of
ENAs separated into two distinct maps, space scientists perform analysis and
conduct research exclusively for either the GDF or ribbon with the respective
map. We believe this process can be improved in two ways. First, both Theseus
and ribbon separation rely on many assumptions about the data and thus depart
from the data generation process itself. Second, we believe inference about the
heliosphere can be done in a more holistic way using the data in its original
form, as counts of ENAs given some exposure time, regardless of source.

% Introduce existence of computer model simulations (ribbon, GDF).
In conjunction with proposing explanations for the existence of the ribbon and
GDF, theoretical physicists have develped computer models to simulate the
creation of sky maps. These computer simulations rely on a number of parameters
that can be varied, thus modifying the shape and intensity of both ribbon and
GDF. Although many simulations exist, we focus on two that are publicly
available: a GDF model proposed by \citet{zirnsteinGDFsims2015} that relies on
two parameters, param1 and param2, and \citet{zirnsteinGDFsims2015}'s model
that generates ribbon-only maps representing the proposed Spatial Retention
model. The ribbon-only model relies on two parameters, parallel mean free path
and ratio. So far, minimal work has been done to validate theories or further
understand the heliosphere by pairing the computer model output with real data
collected by IBEX.

Statistical computer model calibration utilizes both data collected from a
physical experiment and data generated from a computer simulation
representative of the physical process of interest. Often, the physical
experiment is either too expensive to run a sufficient number of times (e.g.
launching hundreds of IBEX satellites) or it is unethical to conduct. In this
way, a computer model representing the physical process allows a practitioner
to learn more about the physical process without the prohibitive expense. Use
of this additional source of data (even with its potential bias) has been
shown to improve prediction and uncertainty quantification out-of-sample. For
example, it is often impossible to control the values of certain variables in
a physical experiment (e.g. gravity), but that barrier does not exist in a
computer simulation. These calibration or tuning parameters can also be of
high scientific interest. Varying them helps experimenters explore more of the
input space, understand the distribution of these parameters, and even make
inferential claims about them. Parameters in computer simulations developed by
space scientists to explain the existence of the GDF and ribbon fall in this
category.

Computer experiments \citep{santner2018design} do not face the same ethical
and practical obstacles of physical experiments. Although some simulations are
very quick to evaluate \citep{higdoncalib2004}, frequently they can be
computationally expensive. Some computer simulations take hours, or even days,
to complete. In our case, an individual run of the IBEX computer model to
produce a simulated sky map can take up to 16 hours, limiting the supply of
computer model data. In such situations, a GP surrogate
\citep{sacks1989computerexp, gramacy2020surrogates} is often fit to the
limited computer output to provide fast predictions out of sample. Adding
another layer of complexity, the output from the IBEX simulation is not only
expensive to obtain, but is extremely high dimensional. Each run of the
simulation produces a sky map represented by a vector of 16,200 ENA rates. GPs
struggle with such big data because they must store and decompose matrices
that grow quadratically with training data size. \citet{higdoncalib2008}
mitigate this with a basis representation via principal components to reduce
the dimension of the data. However, much has changed recently in the landscape
of GP approximation.

% Introduce a charicature of the IBEX problem
Now, before we get ahead of ourselves, let's start with a simple problem.
Suppose we are given counts collected from a physical experiment at different
input locations ($x_i$), as shown in Figure \ref{f:logit1_examp}. We know that
the underlying mean process is of the form $\lambda_i=\mu + \nu f(x_i)$. We are
unable to modify $\mu$ and $\nu$ in a physical setting, but we can vary these
values in a trivial, one-line ``computer model.'' If we consider all unique
combinations for the values of $\mu \in \{5, 7.5, 10, 12.5, 15\}$ and $\nu \in
\{0.25, 0.625, 1, 1.375, 1.75\}$, we end up with 25 different functional
evaluations (depicted as gray lines in \ref{f:logit1_examp}). Our goal is
two-fold: 1) Make accurate predictions at new, unobserved physical locations
($x_i'$) with good uncertainty quantification (UQ), and 2) Estimate the
distribution of calibration parameters $\mu$ and $\nu$ to better understand
how the mean process is constructed.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_examp.pdf}
\caption{Counts from a Poisson process at varying $x_i$ locations along with
evaluations of $\lambda_i=\mu + \nu f(x_i)$ for different values of $\mu$ and
$\nu$.
\label{f:logit1_examp}}
\end{figure}

The canonical framework for statistical computer model calibration was proposed
by \citet{kennedyohagan2001} (hereafter
referred to as KOH). However, KOH relies on modeling the simulation runs and field data
jointly via a multivariate normal representation. The field data collected by
IBEX comes in as Poisson-distributed counts, preventing use of KOH. Recent
research has been done to generalize calibration to non-Gaussian responses
\citep{grosskopfcountcalib2020}, but empirical examples and applications in
this work are restricted to one-dimensional input spaces. Expanding to higher
dimensions allows broader application and brings with it multiple challenges,
including the need to scale larger datasets. No approximation method was
necessary in \citet{grosskopfcountcalib2020}'s implementation. As mentioned
previously, \citet{higdoncalib2008} employed PCA to reduce the dimensionality
of the data in a calibration setting. We propose a modern approach using the
Scaled Vecchia approximation \citep{scaledvecchiakatzfuss2022}. In fact, the
application of the Scaled Vecchia approximation in computer model calibration
was suggested by the authors themselves, noting its ability to scale well.
Therefore, contributions are three-fold: 1) Introduce a MCMC computer model
calibration framework for any type of physical response in high dimensions (we
focus on Poisson-distributed data), 2) Allow for processing large-scale,
high-dimensional computer model output in a reasonable time frame, and 3) For
the first time, illustrate how IBEX data in its raw form can be used to make
inferential claims about the heliosphere.

Our paper is outlined as follows: We review statistical computer model
calibration in Section \ref{sec:review} and focus on the use of GPs as
surrogates of simulator output. Section \ref{sec:gen_calib} introduces a novel
Markov chain Monte Carlo framework (MCMC) as an extension of KOH to account for
non-Gaussian data \ref{sec:poisson_calib} in computer model calibration.
Section \ref{sec:scale_vec} proposes the Scaled Vecchia Approximation
\citep{scaledvecchiakatzfuss2022} as a way to account for large-scale data in
the computer model output. We illustrate the efficacy of our method through
empirical results for small and large problems in Section
\ref{sec:empir_results}. We finish our contribution through novel application
to the real IBEX satellite data in Section \ref{sec:ibex}. Section
\ref{sec:discuss} follows up and concludes our paper with a discussion. Code
reproducing all examples is provided at
\url{https://github.com/lanl/IBEX_Calibration}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computer Model Calibration}
\label{sec:review}

\begin{itemize}
  \item Review computer model calibration
  \item Review Kennedy O'Hagan framework
  \item Review Dave Higdon's SEPIA approach
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized Computer Model Calibration}
\label{sec:gen_calib}

\begin{itemize}
  \item Rehash the need to update KOH to account for Poisson data
  \item Explain the use of a no-bias set up to initially prove the capability
  \item Briefly show the math behind this
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poisson Counts Response}
\label{sec:poisson_calib}
\begin{itemize}
  \item Show diagram of updated framework (McMC with Poisson likelihood)
  \item What if we have unlimited runs of the simulation?? Dave's 2004 paper
  \item Set it up to have a Poisson response
  \item What if we need a surrogate? Computer model takes a long time to run
  \item Set it up to have a Poisson response, but now our data is limited
\end{itemize}

\subsubsection{Examples}

\begin{itemize}
  \item Results of 1D Poisson calibration (prediction, distribution of calibration parameters)
  \item Results of 2D Poisson calibration (prediction, distribution of calibration parameters)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Process Surrogates}
\begin{itemize}
  \item Often our computer model is expensive to run
  \item We need a surrogate
  \item Review Gaussian processes and their use in surrogate modeling
\end{itemize}

\subsubsection{Example}

\begin{itemize}
  \item Go back to 1D and 2D Poisson examples, but now with limited model runs
  \item Tease that this example is easy because the amount of data we have is
  small
\end{itemize}

\subsection{Scaled Vecchia Gaussian Process Surrogate}
\label{sec:scale_vec}

\begin{itemize}
  \item Increase to large data example.
  \item Show inability of framework to fit surrogate without Vecchia
  \item Introduce Scaled Vecchia
\end{itemize}

\subsubsection{Example}

\begin{itemize}
  \item 2D Poisson example with large computer model output
  \item Maybe show some timings here (how timing of non-Vecchia explodes)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{IBEX Sky Map}
\label{sec:ibex}

\begin{itemize}
  \item Go in depth on development of GDF and ribbon computer models
  \item Explain the parameters that govern the computer model output
  \item Talk about IBEX interested in calibration parameters, not just prediction
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic IBEX Data}
\label{sec:ibex_sim}

\begin{itemize}
  \item Run tests on noisy maps generated from known simulations
  \item Display distributions on calibration parameters that show the method
    is able to recover the truth
  \item Show prediction results on hold out data (RMSE? Score? Visual?)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Real IBEX Data}
\label{sec:ibex_real}
\begin{itemize}
  \item Illustrate the gap between computer model output and real data
  \item Recall how we will account for discrepancy (PCA?, GP?)
  \item Perform tests on examples where we know what the bias is. How well
    does our method account for this?
  \item Lack of competitors? (could we compare to calibration of
    ribbon-separated maps)
  \item Run our code on IBEX real data.
  \item We could display predictions for different values of the calibration
    parameters. Not sure if this is of interest.
  \item Display distributions on calibration parameters.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discuss}

\begin{itemize}
  \item Recap what we are proposing in the paper
  \item Highlight performance of calibration framework in prediction and
    estimation of calibration parameters
  \item Point out need to collaborate with theorists to improve computer
    model and get better estimates of calibration parameters
  \item Discuss a need to build out a more exhaustive procedure to perform
    hypothesis testing on which theoretical model is most probable to
    explain the data
  \item Acknowledge limitations (lack of fulll UQ with SVecchia estimates)
  \item Propose a fully Bayesian framework (teasing use of deep GPs)
  \item Can use in other non-Gaussian response settings
\end{itemize}

\subsection*{Acknowledgments}

RBG and SDB are grateful for funding from NSF CMMI 2152679. This work has been
approved for public release under LA-UR-??-?????. SDB, DO and LJB were funded
by Laboratory Directed Research and Development (LDRD) Project 20220107DR.

\bibliography{poisson_calib}
\bibliographystyle{jasa}

\appendix

\end{document}
