\documentclass[12pt]{article}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{caption}
\usepackage{dcolumn}
\usepackage{filemod}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{verbatim}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=7in
\textheight=8.75in
\topmargin=-.5in
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\footskip=0.5in

\begin{document}

\title{Computer Model Calibration for Spatially Distributed
Counts} \author{Steven D. Barnett\thanks{Corresponding author
\href{mailto:sdbarnett@vt.edu}{\tt sdbarnett@vt.edu}, Department of Statistics,
Virginia Tech} \and Robert B. Gramacy\thanks{Department of Statistics, Virginia
Tech} \and Lauren J. Beesley\thanks{Statistical Sciences Group, Los Alamos
National Laboratory} \and Dave Osthus\footnotemark[3] \and Yifan Huang\thanks{
Nuclear and Particle Physics, AstroPhysics and Cosmology, Los Alamos National
Laboratory} \and Fan Guo\footnotemark[4] \and Eric J. Zirnstein\thanks{
Department of Astrophysical Sciences, Princeton University} \and Daniel B.
Reisenfeld\thanks{Space Science and Applications Group, Los Alamos National
Laboratory}}

\date{\today}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
The Interstellar Boundary Explorer (IBEX) satellite detects energetic neutral
atoms (ENAs) and determines the rate at which they are generated in the
heliosphere. Computer models attempt to represent the physical process that
produces heliospheric ENAs through specified model parameters. Computer model
calibration enables statisticians to use data collected in a field experiment
along with simulator output from a variety of parameter settings to make
out-of-sample predictions and learn the posterior distributions of model
parameters. However, calibration typically assumes a Gaussian or continuous
response for both field and computer model data. We introduce a novel Markov
chain Monte Carlo calibration framework that accommodates the spatially
distributed Poisson counts collected by the IBEX satellite. Additionally, the
computer simulation in our application is limited in the amount of runs it can
perform, but each run's output is quite large. Therefore, we propose the use of
the Scaled Vecchia approximation as a Gaussian process (GP) surrogate. We
demonstrate the capability of our proposed framework through multiple simulated
examples and show that we can consistently recover the true parameters in a
discrepancy-free environment and obtain accurate out-of-sample prediction. We
apply this to the IBEX satellite data and the corresponding computer model
output.

\bigskip
\noindent \textbf{Key words:} computer experiments, simulator, Gaussian process
 surrogate, ordinal response, Vecchia approximation, Bayesian inference,
 heliospheric science, IBEX
\end{abstract}

\doublespacing % no double spacing for arXiv

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

The solar wind, continously emitted from the Sun, is primarily made up of
hydrogen ions, or lone protons. These ions travel through the solar system and
out into interstellar space. As the ions reach the boundary of our solar
system, they encounter the termination shock, where they slow down, absorb
significant amounts of energy, and become highly energetic, charged atoms.
These hydrogen ions form an egg-shaped bubble encapsulating our solar system
called the heliosphere, which acts as a barrier between our solar system and
interstellar space. While in this region called the heliopause (the outer edge
of the heliosphere), the ions can interact with other particles in the
interstellar medium. Sometimes these encounters result in electron exchange,
converting the charged atoms into what heliospheric physicists call energetic
neutral atoms (ENAs). Without charge, these ENAs travel in a straight line,
unaffected by magnetic fields. Some ENAs cross back into the solar system and,
depending on their path, can be detected in Earth's orbit. Determining the rate
at which ENAs are generated in different parts of the heliosphere is key to
understanding the makeup of this region at the boundary of our solar system and
interstellar space.

In this pursuit, the National Aeronautics and Space Administration (NASA)
launched the Interstellar Boundary Explorer (IBEX) into Earth's orbit in 2008
as part of their Small Explorer program \citep{mccomas2009aIBEX}. IBEX
continuously detects ENAs and records their energy level and approximate
location of origin, providing sufficient data to estimate the rate at which
ENAs are created throughout the heliopause. Specifically, the IBEX satellite
contains an instrument called the IBEX-Hi ENA imager
\citep{funsten2009IBEXHiENA}, which includes a series of plates
\citep{moran2024statistical} that the ENAs travel through to be detected. The
IBEX satellite rotates on an axis, and over a period of six months, is able to
point at every part of the sky for some interval, or exposure time. This
spatial map of ENA rates (referred to by space scientists as a \textit{sky
map}) is used to analyze, make conjectures about, and ultimately develop
theories for the heliosphere, its many properties, and the processes that
govern its creation.

The heliosphere is like a boat moving through water. Here the water represents
interstellar medium. As a boat moves through water, the water is disturbed.
However, the turbulence created is not equal at each location where the boat is
in contact with the water. Turbulence is greater at the front (bow) and back
(stern) than on the sides (port and starboard) of the ship. Prior to the launch
of the IBEX satellite, space scientists had the same expectation for the
heliosphere. Higher rates of ENAs being generated would exist at the stern
(space scientists call this the \textit{nose}) and bow (\textit{tail}) of the
helisophere due to its movement through interstellar space. Data observed found
this to be true. These higher rates are referred to as \textit{globally
distributed flux (GDF)}. But in a completely unexpected finding
\citep{mccomas2009bIBEX}, IBEX also recorded a string of higher rates of ENAs
\citep{fuselier2009IBEXribbon} being generated that curved around the
heliosphere. Scientists now refer to this phenomenom as the \textit{ribbon}.
For the past fifteen years, space scientists have conducted extensive research
and proposed dozens of theories to explain the existence of the ribbon and the
physical process that generates it \citep{mccomas2014ibex, zirnstein2018role,
zirnstein2019strong, zirnstein2021dependence}.

%%% No calibration going on in IBEX (super novel)
Scientific investigation of the GDF and ribbon currently involves a long,
complex, and at times cumbersome process. For one, the noisy and irregular
nature of the data has made it hard for space scientists to separate signal
from noise and perform their analyses. For the first 15 years of its mission,
IBEX relied soley on maps produced by the IBEX Science Operations Center
(ISOC). ISOC maps are generated using a simple aggregation of the raw data. But
these maps still retain a significant amount of noise and are fairly low
resolution (each pixel represents a $6\degree \times 6\degree$ area of the
sky). To remedy this problem \citet{osthustheseus2023} propose a method called
Theseus, which takes the raw data and generates a much smoother representation
of the surface, using a series of Generative Additive and Projection Pursuit
Regression models. Once Theseus maps are created, they are separated into
GDF-only and ribbon-only maps \citet{beesleyribbonsep2023}. With the source of
ENAs separated into two distinct maps, space scientists perform analysis and
conduct research exclusively for either the GDF or ribbon with the respective
map. We believe this process can be improved in two ways. First, both Theseus
and ribbon separation rely on many assumptions about the data and thus depart
from the data generation process itself. Second, we believe inference about the
heliosphere can be done in a more holistic way using the data in its original
form, as counts of ENAs given some exposure time, regardless of source.

Statistical computer model calibration utilizes both data collected from a
physical experiment and data generated from a computer simulation
representative of the physical process of interest. Often, the physical
experiment is either too expensive to run a sufficient number of times (e.g.
launching hundreds of IBEX satellites) or it is unethical to conduct. In this
way, a computer model representing the physical process allows a practitioner
to learn more about the physical process without the prohibitive expense. Use
of this additional source of data (even with its potential bias) has been
shown to improve prediction and uncertainty quantification out-of-sample. For
example, it is often impossible to control the values of certain variables in
a physical experiment (e.g. gravity), but that barrier does not exist in a
computer simulation. These calibration or tuning parameters can also be of
high scientific interest. Varying them helps experimenters explore more of the
input space, understand the distribution of these parameters, and even make
inferential claims about them. Parameters in computer simulations developed by
space scientists to explain the existence of the GDF and ribbon fall in this
category.

Computer experiments \citep{santner2018design} do not face the same ethical
and practical obstacles of physical experiments. Although some simulations are
very quick to evaluate \citep{higdoncalib2004}, frequently they can be
computationally expensive. Some computer simulations take hours, or even days,
to complete. In our case, an individual run of the IBEX computer model to
produce a simulated sky map can take up to 16 hours, limiting the supply of
computer model data. In such situations, a GP surrogate
\citep{sacks1989computerexp, gramacy2020surrogates} is often fit to the
limited computer output to provide fast predictions out of sample. Adding
another layer of complexity, the output from the IBEX simulation is not only
expensive to obtain, but is extremely high dimensional. Each run of the
simulation produces a sky map represented by a vector of 16,200 ENA rates. GPs
struggle with such big data because they must store and decompose matrices
that grow quadratically with training data size. \citet{higdoncalib2008}
mitigate this with a basis representation via principal components to reduce
the dimension of the data. However, much has changed recently in the landscape
of GP approximation.

% Introduce a charicature of the IBEX problem
Now, before we get ahead of ourselves, let's start with a simple problem.
Suppose we are given counts collected from a physical experiment at different
input locations ($x_i$), as shown in Figure \ref{f:logit1_examp}. We know that
the underlying mean process is of the form $\lambda_i=\mu + \nu f(x_i)$. We are
unable to modify $\mu$ and $\nu$ in a physical setting, but we can vary these
values in a trivial, one-line ``computer model.'' If we consider all unique
combinations for the values of $\mu \in \{5, 7.5, 10, 12.5, 15\}$ and $\nu \in
\{0.25, 0.625, 1, 1.375, 1.75\}$, we end up with 25 different functional
evaluations (depicted as gray lines in \ref{f:logit1_examp}). Our goal is
two-fold: 1) Make accurate predictions at new, unobserved physical locations
($x_i'$) with good uncertainty quantification (UQ), and 2) Estimate the
distribution of calibration parameters $\mu$ and $\nu$ to better understand
how the mean process is constructed.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6, trim=5 15 15 50,clip=TRUE]{logit1_examp.pdf}
\caption{Counts from a Poisson process at varying $x_i$ locations along with
evaluations of $\lambda_i=\mu + \nu f(x_i)$ for different values of $\mu$ and
$\nu$.
\label{f:logit1_examp}}
\end{figure}

The canonical framework for statistical computer model calibration was proposed
by \citet{kennedyohagan2001} (hereafter
referred to as KOH). However, KOH relies on modeling the simulation runs and field data
jointly via a multivariate normal representation. The field data collected by
IBEX comes in as Poisson-distributed counts, preventing use of KOH. Recent
research has been done to generalize calibration to non-Gaussian responses
\citep{grosskopfcountcalib2020}, but empirical examples and applications in
this work are restricted to one-dimensional input spaces. Expanding to higher
dimensions allows broader application and brings with it multiple challenges,
including the need to scale larger datasets. No approximation method was
necessary in \citet{grosskopfcountcalib2020}'s implementation. As mentioned
previously, \citet{higdoncalib2008} employed PCA to reduce the dimensionality
of the data in a calibration setting. We propose a modern approach using the
Scaled Vecchia approximation \citep{scaledvecchiakatzfuss2022}. In fact, the
application of the Scaled Vecchia approximation in computer model calibration
was suggested by the authors themselves, noting its ability to scale well.
Therefore, contributions are three-fold: 1) Introduce a MCMC computer model
calibration framework for any type of physical response in high dimensions (we
focus on Poisson-distributed data), 2) Allow for processing large-scale,
high-dimensional computer model output in a reasonable time frame, and 3) For
the first time, illustrate how IBEX data in its raw form can be used to make
inferential claims about the heliosphere.

Our paper is outlined as follows: We review statistical computer model
calibration in Section \ref{sec:review} and focus on the use of GPs as
surrogates of simulator output. Section \ref{sec:gen_calib} introduces a novel
Markov chain Monte Carlo framework (MCMC) as an extension of KOH to account for
non-Gaussian data \ref{sec:poisson_calib} in computer model calibration.
Section \ref{sec:scale_vec} proposes the Scaled Vecchia Approximation
\citep{scaledvecchiakatzfuss2022} as a way to account for large-scale data in
the computer model output. We illustrate the efficacy of our method through
empirical results for small and large problems in Section
\ref{sec:empir_results}. We finish our contribution through novel application
to the real IBEX satellite data in Section \ref{sec:ibex}. Section
\ref{sec:discuss} follows up and concludes our paper with a discussion. Code
reproducing all examples is provided at
\url{https://github.com/lanl/IBEX_Calibration}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computer Model Calibration}
\label{sec:review}

%% Review computer model calibration
Computer model calibration is a technique employed by statisticians to pair data
collected from a physical experiment. Computer models attempt to represent a physical
process that is infeasible to run, due to a number of reasons (e.g. physical expense,
ethics). Data collected from a physical experiment can be augmented by output from a
computer simulation. As a result of increased access to supercomputing resources,
computer models have increased in complexity (cite??) over the past two decades to
produce extremely high-fidelity output. Scientists are able to learn more about a
physical process using such an augmented dataset. Additionally, computer models allow
practitioners to vary certain parameters that cannot be controlled in a physical
experiment (e.g. gravity). Therefore, in addition to helping achieve better
prediction out-of-sample, the computer model output allows statisticians to estimate
the distributions of certain unknown parameters.

%% Review Kennedy O'Hagan framework
\citet{kennedyohagan2001} introduced what has become the canonical framework for
computer model calibration. Field data and computer simulation output are modeled
jointly as following a multivariate normal distribution. \citet{kennedyohagan2001}
make two key suggestions: 1) Computer models, despite improvements in recent years,
are still inherently biased. It's virtually impossible to exactly replicate a
physical process through simulation. 2) Although often easier to conduct than
physical experiements, computer models are still expensive in terms of computation,
necesitating a surrogate model for the simulated output to obtain additional
predictions.

%% Review Dave Higdon's SEPIA approach

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized Computer Model Calibration}
\label{sec:gen_calib}

%% Rehash the need to update KOH to account for Poisson data
%% Explain the use of a no-bias set up to initially prove the capability
%% Briefly show the math behind this

Computer models are not limited to modeling physical processes that generate a
continous response. For instance, a field experiment may return a binary
response, such as success or failure. Categorical responses may expand beyond
two classes, representing a multinomial reponse. We focus on the
Poisson-distributed counts in this paper, but our framework can be generalized
to any response.

The canonical KOH computer model calibration framework is limited to a
continuous response from both the computer model and the field experiment, due
to it's reliance on modeling the two outputs jointly via a multivariate normal
distribution. Faced with spatially distributed counts from the IBEX satellite,
we could not apply KOH to our problem. We propose a fully Bayesian Markov
chain Monte Carlo (MCMC) framework to sample from the posterior distribution
of our calibration parameters. At each MCMC iteration, we propose new
calibration parameters, generate computer model output at those new settings,
and evaluate the Poisson likelihood of the observed data, given the simulated
mean surface.

As a side note, we point out that we do not include a term for the bias
inherent in the computer model. We do this initially to simplify the setup and
prove the framework's capability. We will address discrepancy later in the
paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poisson Counts Response}
\label{sec:poisson_calib}
\begin{itemize}
  \item Show diagram of updated framework (McMC with Poisson likelihood)
  \item What if we have unlimited runs of the simulation?? Dave's 2004 paper
  \item Set it up to have a Poisson response
  \item What if we need a surrogate? Computer model takes a long time to run
  \item Set it up to have a Poisson response, but now our data is limited
\end{itemize}

\subsubsection{Examples}

\begin{itemize}
  \item Results of 1D Poisson calibration (prediction, distribution of calibration parameters)
  \item Results of 2D Poisson calibration (prediction, distribution of calibration parameters)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Process Surrogates}

%% Often our computer model is expensive to run
%% We need a surrogate
%% Review Gaussian processes and their use in surrogate modeling

Often our computer models take a long time to run. Due to complex and
expensive operations, such as solving partial differential equations,
simulations can take hours, days, or even weeks to complete. In our IBEX
application, each run of the the computer model theorists have developed takes
upwards of 64 hours. This limits the amount of simulated data that can be
produced. But to be effective, computer model calibration needs to be able to
explore the entire input space of the model parameters. Therefore, we need a
surrogate to make accurate and reliable out-of-sample predictions. The
canonical surrogate for computer experiments and computer model calibration is
the Gaussian process.

A Gaussian process (GP) is a realization of a multivariate normal distribution
with some mean and covariance functions. GPs are fit to observed data and then
used to make predictions at new input locations, relying on the conditional
multivariate normal equations. GPs rely on the estimation of three
hyperparameters that can be estimated using maximum likelihood estimators,
method of moments estimators, or in our case, Bayesian MCMC methods. GPs
provide a flexible, nonparamteric regression tool. GPs were popularized as
surrogates for computer experiments due to their ability to interpolate
between observed data points. Computer experiments are typically
deterministic, although stochastic computer models are becoming increasingly
common. GPs can provide predictive surfaces that give higher estimates of
variance away from observed data, and virtually no variance at observed
locations.

\subsubsection{Example}

\begin{itemize}
  \item Go back to 1D and 2D Poisson examples, but now with limited model runs
  \item Tease that this example is easy because the amount of data we have is
  small
\end{itemize}

\begin{itemize}
  \item Increase to large data example.
  \item Show inability of framework to fit surrogate without Vecchia
  \item Introduce Scaled Vecchia
\end{itemize}

\subsection{Scaled Vecchia Gaussian Process Surrogates}
\label{sec:scale_vec}

One weakness of GPs is the need to invert an $n \times n$ covariance matrix,
making it difficult to scale to large datasets, such as that from our sky map
simulator. Many methods have been introduced to address this computational
burden. These include, but are not limited to, fixed-rank kriging
\citep{cressie2008fixed}, inducing points \citep{banerjee2008gaussian},
compactly supported kernels \citep{kaufman2011efficient}, divide-and-conquer
\citep{gramacy2008bayesian, gramacy2015local}, and nearest neighbors
\citep{datta2016hierarchical, wu2022variational}. In the context of computer
model calibration, \citet{higdoncalib2008} use a basis representation via
principal components to accommodate high-dimensional output. However, with the
dramatic increase in computing capacity over the past decade and the
aforementioned advances in methodology, we believe there are better GP
approximations available to model the output of a computer simulation. We
propose the recently popularized Vecchia approximation
\citep{vecchia1988estimation, katzfuss2020vecchia, katzfuss2021general,
zhang2022multi}. In particular, we utilize the Scaled Vecchia Approximation
\citep{scaledvecchiakatzfuss2022} to model the limited simulator output and
make predictions at new locations. In discussing further work,
\citet{scaledvecchiakatzfuss2022} proposed their method as an effective tool
for use in computer model calibration.

The Vecchia approximation relies on the ability to write a likelihood function as the
product of conditional likelihoods as shown below.
\begin{align}
L(Y)&=\prod_{i=1}^n L(Y_i \mid Y_{g(i)})
\hspace{0.3em} \mbox{ where } g(1)=\emptyset \hspace{0.3em} \mbox{ and } g(i)=\{1, 2, \ldots, i-1\}\\
&\approx \prod_{i=1}^{n}L(Y_i \mid Y_{h(i)}) \hspace{0.3em} \mbox{ where } h(i) \subset \{1, 2, \ldots, i-1\} \hspace{0.3em} \mbox{ and } |h(i)| = m \ll n \nonumber \label{eq:vecchia}
\end{align}
Reducing the size of the conditioning set ($h(i)$ above) induces sparsity in
the precision matrix, speeding up the evaluation of the likelihood. This takes
an operation of order $O(n^3)$ down to $O(nm^3)$, motivating small values of
$m$. It's clear that as the size of the conditioning set $m$ increases, the
quality of the approximation improves, but we find a value of $m < 30$ is more
than sufficient. The conditioning set can be constructed in many ways, but we
default to a nearest-neighbors approach, which has been done previously
\citep{sauer2023active}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interstellar Boundary Explorer}
\label{sec:ibex}

In 2008, NASA launched the IBEX satellite into Earth's orbit as part of their
Small Explorer program \citep{mccomas2009aIBEX}. IBEX's mission is to collect
data that will deepen the scientific community's understanding of the
heliosphere, the egg-shaped bubble encapsulating our solar system. The
heliosphere is created from the solar wind. The solar wind, continously
emitted from the Sun, is primarily made up of hydrogen ions, or lone protons.
These ions travel through the solar system and out into interstellar space. As
the ions reach the boundary of our solar system, they encounter the
termination shock, where they slow down, absorb significant amounts of energy,
and become highly energetic, charged atoms. These hydrogen ions form the
heliosphere and act as a barrier between our solar system and interstellar
space. While in this region called the heliopause (the outer edge of the
heliosphere), the ions can interact with other particles in the interstellar
medium. Sometimes these encounters result in electron exchange, converting the
charged atoms into what heliospheric physicists call energetic neutral atoms
(ENAs). Without charge, these ENAs travel in a straight line, unaffected by
magnetic fields. Some ENAs cross back into the solar system and, depending on
their path, can be detected in Earth's orbit. Determining the rate at which
ENAs are generated in different parts of the heliosphere is key to
understanding the makeup of this region at the boundary of our solar system
and interstellar space.

IBEX records the energy level and approximate location of origin for each ENA
that enters the satellite, providing sufficient data to estimate the rate at
which ENAs are created throughout the heliopause. Specifically, the IBEX
satellite contains an instrument called the IBEX-Hi ENA imager
\citep{funsten2009IBEXHiENA}, which includes a series of plates
\citep{moran2024statistical} that the ENAs travel through to be detected. The
IBEX satellite rotates on an axis, and over a period of six months, is able to
point at every part of the sky for some interval, or exposure time. This
spatial map of ENA rates (referred to by space scientists as a \textit{sky
map}) is used to analyze, make conjectures about, and ultimately develop
theories for the heliosphere, its many properties, and the processes that
govern its creation.

The heliosphere is like a boat moving through water. Here the water represents
interstellar medium. As a boat moves through water, the water is disturbed.
However, the turbulence created is not equal at each location where the boat is
in contact with the water. Turbulence is greater at the front (bow) and back
(stern) than on the sides (port and starboard) of the ship. Prior to the launch
of the IBEX satellite, space scientists had the same expectation for the
heliosphere. Higher rates of ENAs being generated would exist at the stern
(space scientists call this the \textit{nose}) and bow (\textit{tail}) of the
helisophere due to its movement through interstellar space. Data observed found
this to be true. These higher rates are referred to as \textit{globally
distributed flux (GDF)}. But in a completely unexpected finding
\citep{mccomas2009bIBEX}, IBEX also recorded a string of higher rates of ENAs
\citep{fuselier2009IBEXribbon} being generated that curved around the
heliosphere. Scientists now refer to this phenomenom as the \textit{ribbon}.
For the past fifteen years, space scientists have conducted extensive research
and proposed dozens of theories to explain the existence of the ribbon and the
physical process that generates it \citep{mccomas2014ibex, zirnstein2018role,
zirnstein2019strong, zirnstein2021dependence}.

% Introduce existence of computer model simulations (ribbon, GDF).
In conjunction with proposing explanations for the existence of the ribbon and
GDF, theoretical physicists have developed computer models to simulate the
creation of sky maps. These computer simulations rely on a number of
parameters that can be varied, thus modifying the shape and intensity of both
ribbon and GDF. Although many simulations exist, we focus on two that are
publicly available: a GDF model proposed by \citet{zirnstein2021heliosheath}
and a ribbon-only model developed by \citet{zirnsteinGDFsims2015} representing
the proposed Spatial Retention model. The ribbon-only model relies on two
parameters, parallel mean free path and ratio. So far, minimal work has been
done to validate theories or further understand the heliosphere by pairing the
computer model output with real data collected by IBEX.

Scientific investigation of the GDF and ribbon currently involves a long,
complex, and at times cumbersome process. For one, the noisy and irregular
nature of the data has made it hard for space scientists to separate signal
from noise and perform their analyses. First, a smooth surface is generated
from the raw satellite data. Once smooth \textit{sky maps} are created, they
are separated into GDF-only and ribbon-only maps \citet{beesleyribbonsep2023}.
With the source of ENAs separated into two distinct maps, space scientists
perform analysis and conduct research exclusively for either the GDF or ribbon
with the respective map. We believe this process can be improved in two ways.
First, both the response surface modeling and ribbon separation rely on many
assumptions about the data and thus depart from the data generation process
itself. Second, we believe inference about the heliosphere can be done in a
more holistic way using the data in its original form, as counts of ENAs given
some exposure time, regardless of source.

\begin{itemize}
  \item Go in depth on development of GDF and ribbon computer models
  \item Explain the parameters that govern the computer model output
  \item Talk about IBEX interested in calibration parameters, not just prediction
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{IBEX Sky Map}
\label{sec:ibex_sky}

\begin{itemize}
  \item Go in depth on development of GDF and ribbon computer models
  \item Explain the parameters that govern the computer model output
  \item Talk about IBEX interested in calibration parameters, not just prediction
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic IBEX Data}
\label{sec:ibex_sim}

\begin{itemize}
  \item Run tests on noisy maps generated from known simulations
  \item Display distributions on calibration parameters that show the method
    is able to recover the truth
  \item Show prediction results on hold out data (RMSE? Score? Visual?)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Real IBEX Data}
\label{sec:ibex_real}
\begin{itemize}
  \item Illustrate the gap between computer model output and real data
  \item Recall how we will account for discrepancy (PCA?, GP?)
  \item Perform tests on examples where we know what the bias is. How well
    does our method account for this?
  \item Lack of competitors? (could we compare to calibration of
    ribbon-separated maps)
  \item Run our code on IBEX real data.
  \item We could display predictions for different values of the calibration
    parameters. Not sure if this is of interest.
  \item Display distributions on calibration parameters.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discuss}

\begin{itemize}
  \item Recap what we are proposing in the paper
  \item Highlight performance of calibration framework in prediction and
    estimation of calibration parameters
  \item Point out need to collaborate with theorists to improve computer
    model and get better estimates of calibration parameters
  \item Discuss a need to build out a more exhaustive procedure to perform
    hypothesis testing on which theoretical model is most probable to
    explain the data
  \item Acknowledge limitations (lack of fulll UQ with SVecchia estimates)
  \item Propose a fully Bayesian framework (teasing use of deep GPs)
  \item Can use in other non-Gaussian response settings
\end{itemize}

\subsection*{Acknowledgments}

RBG and SDB are grateful for funding from NSF CMMI 2152679. This work has been
approved for public release under LA-UR-??-?????. SDB, DO and LJB were funded
by Laboratory Directed Research and Development (LDRD) Project 20220107DR.

\bibliography{poisson_calib}
\bibliographystyle{jasa}

\appendix

\end{document}

%% STEPS:
%% - Write discussion
%% - Redesign intro to be more like Bobby suggested
%% - Review computer model calibration (lit review)
%% - Review KOH (lit review)
%% - Review Higdon SEPIA (lit review)
%% - Introduce generalized computer model calibration (can't model joint MVN)
%%%% - 10/23
%% - Create and introduce 1D poisson example
%% - Write section on GP surrogates
%%%% - 10/23
%% - Expand on toy example and write about needing a surrogate
%% - Talk about Scaled Vecchia
%% - Create and introduce 2D poisson toy example
%% - Write about IBEX stuff
%% - Explain experiment with synthetic
%% - Explain experiment with real data (need for discrepancy)
