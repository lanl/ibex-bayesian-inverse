\documentclass[12pt]{article}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{caption}
\usepackage{dcolumn}
\usepackage{filemod}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{makecell}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=7in
\textheight=8.75in
\topmargin=-.5in
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\footskip=0.5in

\begin{document}

\title{Bayesian Statistical Inversion for High-Dimensional Computer Model
Output and Spatially Distributed Counts}
\author{Steven D. Barnett\thanks{Corresponding author
\href{mailto:sdbarnett@vt.edu}{\tt sdbarnett@vt.edu}, Department of
Statistics, Virginia Tech} \and Robert B. Gramacy\thanks{Department of
Statistics, Virginia Tech} \and Lauren J. Beesley\thanks{Statistical Sciences
Group, Los Alamos National Laboratory} \and Dave Osthus\footnotemark[3] \and
Yifan Huang\thanks{ Nuclear and Particle Physics, AstroPhysics and Cosmology,
Los Alamos National Laboratory} \and Fan Guo\footnotemark[4] \and Eric J.
Zirnstein\thanks{ Department of Astrophysical Sciences, Princeton University}
\and Daniel B. Reisenfeld\thanks{Space Science and Applications Group, Los
Alamos National Laboratory}}
\date{}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
The Interstellar Boundary Explorer (IBEX) satellite orbits Earth and detects
heliospheric energetic neutral atoms (ENAs) to determine the rate at which
they are generated in the heliopause. Sophisticated computer models attempt to
represent the physical process that produce ENAs through various model
parameters. Statisticians use a Bayesian approach to solve these types of
inverse problems by using data collected in a field experiment along with
simulator output from different combinations of parameter settings to make
out-of-sample field predictions and learn the posterior distributions of model
parameters. However, inverse problems typically assume both a Gaussian
response for both experimental observations and a limited set of computer
model data. We introduce a novel Markov chain Monte Carlo framework that
accommodates the spatially distributed Poisson counts collected by the IBEX
satellite. We also propose the use of the Scaled Vecchia approximation as a
Gaussian process (GP) surrogate for the large-scale computer model training
data. We demonstrate the capability of our proposed framework through multiple
simulated examples and show that we can consistently recover the true
parameters in a discrepancy-free environment and obtain accurate
out-of-sample prediction. We apply this to the raw IBEX satellite data and the
corresponding computer model output.

\bigskip
\noindent \textbf{Key words:} Gaussian process, surrogate modeling,
Poisson, calibration, heliospheric science, IBEX, Vecchia approximation
\end{abstract}

\doublespacing % no double spacing for arXiv

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

%% IBEX
%% - Talk about the satellite
%% - Explain the mission
%% - Introduce the idea of a computer model
%% - Explain desire to understand the distribution of parameters
The National Aeronautics and Space Administration (NASA) launched the
Interstellar Boundary Explorer (IBEX) satellite in 2008 as part of their Small
Explorer program \citep{mccomas2009aIBEX} to deepen our understanding of the
heliosphere. The heliosphere is the bubble formed by the solar wind that
encompasses our solar system and acts as a barrier between our planetary
system and interstellar space. In particular, the behavior at the edge of the
heliosphere, known as the heliopause, is of interest. Here, highly energized
hydrogen ions that make up the solar wind interact with neutral atoms,
occasionally undergo electron exchange, and become neutral themselves. At this
point, these energetic neutral atoms (ENAs) are unaffected by magnetic fields
and therefore travel in a straight line. As a result, some ENAs eventually
come to Earth and can be detected by IBEX. IBEX contains an instrument called
the IBEX-Hi ENA imager \citep{funsten2009IBEXHiENA}, which records the energy
level and approximate location of origin for each ENA that enters the
satellite, providing sufficient data to estimate the rate at which these
particles are created throughout the heliopause. An example of this estimated
surface or image, referred by space scientists as a \textit{sky map}, can be
found in the left panel of Fig. \ref{f:fig1}. \textit{Sky maps} are a tool
used by space scientists as they research the heliosphere and seek to
understand its creation and evolution.

One can think of the heliosphere as a boat moving through water, the water
here representing interstellar space. At the outset of IBEX's mission,
scientists expected to see an elevated rate of ENAs being generated at the
front (nose) and back (tail), much like the turbulent interaction between a
boat and water at its bow and stern. And indeed, that belief was validated by
IBEX's collected data. These regions of raised ENA rates are termed
\textit{globally distributed flux}, or GDF, by the space science community.
More ENAs are produced at the nose and tail where there is an increased number
of collisions between hydrogen ions and neutral particles in the interstellar
medium. However, in a completely unanticipated finding, IBEX also recorded a
thin string of higher rates of ENAs \citep{fuselier2009IBEXribbon} being
created that wraps around the heliosphere. Scientists now refer to this
phenomenom as the \textit{ribbon}. This unique feature of a \textit{sky map}
is clearly visible in Fig. \ref{f:fig1}. Since this discovery, space
scientists have conducted years of extensive research to explain the existence
of the \textit{ribbon} and have proposed dozens of theories to describe the
physical process that generates it \citep{mccomas2014ibex, zirnstein2018role,
zirnstein2019strong, zirnstein2021dependence}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.74,trim=0 0 65 0,clip=TRUE]{ibex_real1.pdf}
\includegraphics[scale=0.74,trim=35 0 65 0,clip=TRUE]{sim1.pdf}
\includegraphics[scale=0.74,trim=35 0 0 0,clip=TRUE]{sim2.pdf}
\caption{Observed rate of energetic neutral atoms (ENA) detected by the
Interstellar Boundary Explorer (IBEX) satellite (left) and output from two
runs of the IBEX computer simulation at different settings (middle, right).
\label{f:fig1}}
\end{figure}

In order to further explore these theories, scientists have developed computer
models to create synthetic \textit{sky maps} based on certain inputs (Figure
\ref{f:fig1}). These computer simulations rely on a number of parameters that
can be varied, thus modifying the shape and intensity of both ribbon and GDF.
Although many simulations exist, we focus on two that are publicly available:
a GDF model proposed by \citet{zirnstein2021heliosheath} and a ribbon-only
model developed by \citet{zirnsteinGDFsims2015}. The ribbon-only model relies
on two parameters, \textit{parallel mean free path} and \textit{ratio}, and
the GDF model relies on two additional parameters, \textit{parameter1} and
\textit{parameter2}. Understaing the distribution of these variables is
instrumental to determining which proposed theory carries the most weight, or
what postulation corresponds to the most probable combination of parameter
values. Although we acknowledge the oversimplification of this description, we
essentially want to provide the ability for space scientists to conduct
hypothesis testing between competing ideas. But so far, minimal work has been
done to validate theories or further understand the heliosphere by pairing the
computer model output with the raw data collected by IBEX.

%% PROBLEM
%% - Classic Inverse Problem
%% - Mathematical model is expensive (and perhaps not accurate)
%% - Use Bayesian methods
Now, this is a classic inverse problem. Given observed data and an assumed
model that generates the data, the goal is to determine the true values of the
unknown parameters in the model. A classic approach to solving the inverse
problem is to evaluate the model with a multitude of unique combinations of
parameter values. Then, select the set of parameter values that minimize the
distance between model output and real observations. However, there are
limitations to this technique. For one, computer models often involve complex
calculations are therefore expensive, sometimes taking hours, days, or weeks
(as is the case in for the IBEX simulation), precluding the execution of an
extensive set of parameter combinations. With limited data, a Bayesian
approach is often better. Applying Bayesian methods allows practitioners to
inform the model with prior information on the parameters. Additionally,
Bayesian procedures treat the parameters as random variables and provide more
information through an estimated posterior distribution over the most likely
values instead of a single point estimate.

%% OTHER WORK / CHALLENGES
Inverse problems are not new, and statisticians have previously employed
Bayesian methods to solve them
\citep{kaipio2011bayesian,stuart2010inverse,knapik2011bayesian}. But the data
collected by IBEX and its simulated representation present a unique challenge:
a non-Gaussian field response, high-dimensional model output, and the
large-scale field and simulated datasets seen in modern-day computing. Table
\ref{tab:prev_work} summarizes the features of the IBEX inverse problem and
reviews the necessary capabilities that are present in previous work. For
instance, \citep{kennedyohagan2001} offer the canonical computer model
calibration framework, which provides a fully Bayesian approach to estimate
the distribution of calibration parameters and improve prediction
out-of-sample. However, their methodology is limited to Gaussian observations
and small, simulator output. \citep{higdoncalib2008}, and its more recent
incarnation SEPIA \citep{gattiker2020lanl}, also employ Bayesian methods and
account for computer model output that is functional. But in addition to the
previous restriction of Gaussian field data, SEPIA is somewhat rigid and
requires the user to specify a number of basis functions to represent the
simulator response. \citep{gramacy2015calibrating} consider computer model
calibration with large-scale output from a computer experiment, necessitating
an approximation. But their implementation is not Bayesian, the scale of data
is still orders of magnitude less than IBEX, and the field data is small.
\citep{grosskopfcountcalib2020} provide a extension of the Kennedy \& O'Hagan
(hereafter referred to as KOH) framework to non-Gaussian data that maintains a
Bayesian approach. But they are limited to small amounts of computer model
data and do not consider higher dimensional responses. In contrast, our
approach checks all the boxes to achieve the goal of IBEX space scientists and
therefore applies to a wider class of problems than previous work.
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|}
\hline
& Bayesian & \makecell{Non-Gaussian \\ Observations} & \makecell{Large-Scale \\ Computer Model \\ Output} & \makecell{High-Dimensional \\ Output} \\
\hline
Our Contribution & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
\citet{kennedyohagan2001} & \checkmark & & & \\
\hline
\citet{higdoncalib2008} & \checkmark & & & \checkmark \\
\hline
\citet{gramacy2015calibrating} & & & \checkmark & \\
\hline
\citet{grosskopfcountcalib2020} & \checkmark & \checkmark & & \\
\hline
\end{tabular}
\caption{A review of functionality to solve statistical inverse problems
present in previous academic works. Check marks indicate that functionality
exists within the cited paper and/or accompanying software. To save space,
only one citation is listed for each row, although we recognize these may be
replace with other studies. Note that our paper offers all required
capabilities.}
\label{tab:prev_work}
\end{table}

%% OUTLINE
%% - Review Bayesian Inverse Problems, GP Surrogates, SEPIA
%% - Introduce our framework
%% - Explain use of Vecchia approximation
%% - Demonstrate on simulated data
%% - Apply to real IBEX data
Our paper is laid out as follows. First, Section \ref{sec:review} reviews some
of the methodology that form the foundation for our work, namely Bayesian
Inverse Problems and Gaussian process surrogate models. We also give a brief
description of the theory behind SEPIA, which could classified as the current
``status quo'' in Bayesian Inverse Problems for high-dimensional computer
model output. In Section \ref{sec:gen_bayes_inv} we introduce our framework
for solving the inverse problem in non-Gaussian contexts. Section
\ref{sec:vecchia} describes our use of the Vecchia approximation to stretch
the capacity of our GP surrogate beyond small simulator training runs. In
Section \ref{sec:sims}, we illustrate the effectiveness of our framework in
discovering the true, underlying model parameters on simulated data. We then
return to our motivation application and apply our methods to the raw IBEX
satellite data in Section \ref{sec:ibex_real}. To conclude, we discuss any
extensions and future work in this area in Section \ref{sec:discuss}. Code
reproducing all examples is provided at
\url{https://github.com/lanl/IBEX_Calibration}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 2: Review:
\section{Review}
\label{sec:review}

%------------------------------------------------------------------------------
%% SECTION 2.1 - Bayesian Inverse Problems
\subsection{Bayesian Inverse Problems}
\label{sec:bayes_inv}
%------------------------------------------------------------------------------

%% BAYESIAN INVERSE PROBLEMS
%% - What is an inverse problem and why do we care?
%% - Obtained observations from a physical process to learn more about it
%% - We have some knowledge of the workings of the true process
%% - We can build a mathematical model, and therefore a computer model
%% - Classic IP: evaluate model as much as possible to find parameters/inputs
%%   that produce output aligned with the observations
Statistical inverse problems involve the collection of observations from a
physical experiment in an effort to learn more about the process that
generates these data points. Typically, a scientist has some knowledge of the
physical or mathematical model (i.e. the data-generating process), but is
seeking to learn more. The goals are two-fold: 1) estimate the parameters (or
distribution of the parameters) that govern the physical model, and 2) if the
inputs to the experiment are unknown but the parameters are known, reconstruct
the inputs from which the observations originated. The motivation for such a
method stems from the inability to either control or precisely measure the
factors (e.g. the force of gravity) in a physical experiment. Therefore, in an
effort to gain more insight, scientists develop mathematical models, often
resulting in a computer simulation, that attempt to represent that physical
process. By comparing observations to simulator output generated via unique
combinations of model parameters, statisticians attempt to learn the true,
underlying parameter values. As mentioned in Section \ref{sec:intro}, the
classical approach is to find the best match between observed data and
extensive computer model via some minimization of error (i.e. RMSE, MAE).

%% - Observations are noisy
%% - Desire full UQ on parameters
%% - Go Bayesian
%% - Show Prior / Posterior
%% - No analytical form, must resort to MCMC
%% - LEAD IN: Computer model is expensive. Need a surrogate.
Bayesian statistical inversion (BSI) entails imposing a prior on model
parameters that embeds a certain amount of subject matter knowledge into the
analysis. Equation \ref{eq:bsi} shows the standard implementation of Bayes'
rule. Here, $\boldsymbol \theta$ represents the model parameters we wish to
infer and obtain uncertainty estimates for and $Y$ is our vector of physical
observations at input locations $X$.
\begin{align}
\pi(\boldsymbol \theta | Y, X) = \frac{\mathcal{L}(Y|\boldsymbol \theta, X)
\pi(\boldsymbol \theta)}{\pi(Y|X)} \propto \mathcal{L}(Y|\boldsymbol \theta,
X) \pi(\boldsymbol \theta)
\label{eq:bsi}
\end{align}
If $\pi(\boldsymbol \theta)$ is conjugate, inference for $\boldsymbol \theta$
is simple and can be done analytically. However, this is often not the case,
requiring an approximation of the posterior (i.e. Laplace, Variational Bayes).
We utilize Markov chain Monte Carlo (MCMC) methods to sample from the
posterior.

%% DO A REVIEW OF OTHER METHODS (KOH? BAYES INV? STUART? TECKENTRUP? COLLINS?)

One difficulty in our situation is the expense of the computer model. As a
result of increased access to supercomputing resources, computer models have
increased in complexity over the past two decades and are able to produce
extremely high-fidelity output. However, even with modern-day computing power,
some simulations can take hours, days, or weeks to complete. This limits the
amount of runs that can be executed and therefore restricts the exploration of
the model parameter space. Therefore, we need a model (what we call a
surrogate, or emulator) fit to the simulator output to make accurate and
reliable predictions for out-of-sample values of our computer model
parameters. Any statistical model can serve as a surrogate, ranging from a
linear regression to complex neural networks. However, the canonical surrogate
for computer experiments and computer model calibration has come to be the
Gaussian process \citep{gramacy2020surrogates}.

%------------------------------------------------------------------------------
%% SECTION 2.2 - Gaussian Process Surrogates:
\subsection{Gaussian Process Surrogates}
\label{sec:gp_surr}
%------------------------------------------------------------------------------

%% GAUSSIAN PROCESS SURROGATES
%% - What is the motivation? Expensive computer experiments
%% - Set of n observations that follow a MVN
%% - Often mean-zero. In practice this means subtracting off the mean
%% - Results in all the action being in the covariance function
%% - Covariance depends on pairwise distance
%% - Explain some different kernels
%% - Talk about prediction. Show Kriging equations
%% - Nonparametric, flexible regression tool
%% - A lot of different applications
%% - Become popular in computer experiments because of interpolation
%% - This is what we want
%% - Bottleneck is O(n^3)
%% - LEAD IN: Output can be large. Need an approximation.
%% Review Gaussian processes and their use in surrogate modeling
A Gaussian process, hereafter referred to as a GP, is a realization of a
multivariate normal distribution, governed by some mean and covariance
functions. GPs are fit to observed data and used to make predictions at new
input locations, relying on the conditional multivariate normal or
\textit{kriging} equations \citep{matheron1963principles}. For example,
suppose we have a vector of observations $Y$ at input locations $X$. $Y$ is an
$n \times 1$ vector and $X$ is an $n \times p$ matrix. We make the assumption
that $Y$ is normally distributed with some mean $(\mu(X))$ and covariance
$(\Sigma(X))$, both functions of $X$. Predicting at a new location $x$ is
straightforward. Eq.~(\ref{eq:mvn_eq}) articulates how to obtain mean
predictions $\mu^*(x)$ and corresponding uncertainty quantification
$\Sigma^*(x)$.
\begin{align}
Y(X) &\sim \mathcal{N}\!\left(\mu(X), \Sigma(X)\right) \nonumber \\
y^*(x) \mid Y(X) &\sim \mathcal{N}\!\left(\mu^*(x), \Sigma^*(x)\right) \nonumber \\
\mu^*(x)&=\mu(x) + \Sigma(x, X) \Sigma(X)^{-1} (Y - \mu(X)) \nonumber \\
\Sigma^*(x)&=\Sigma(x, x) - \Sigma(x, X) \Sigma(X)^{-1} \Sigma(X, x) \label{eq:mvn_eq}
\end{align}
Although a generic mean function is specified in Eq.~(\ref{eq:mvn_eq}), it is
common to set $\mu(x)=0$. In practice, this simply implies centering $Y$ by
subtracting off the sample mean, $\overline{Y}$. In this zero-mean context,
all the action takes place in the covariance function. The covariance function
typically depends on pairwise distances between input locations $x$ and $x'$.
Many covariance kernels exists, such as the Gaussian, Exponential, Matern. We
use the Matern kernel in our work, of which both the Gaussian and Exponential
kernels are special cases. The behavior of these kernels is determined by
three hyperparameters: a nugget $g$, a lengthscale $\theta$, and a scale
$\tau^2$. In the Matern kernel, an additional parameter ($\nu$) exists, which
controls the smoothness of the estimated surface. But we will not estimate the
smoothness parameter here and instead set $\nu =
\frac{5}{2}$ (\ref{eq:matern}), following common practice due to it allowing
the kernel function to be expressed in a simpler form. All hyperparameters can
be estimated using your preferred method (i.e. maximum likelihood, method of
moments). We will use Bayesian methods, specifically MCMC.
\begin{align}
\Sigma(x, x'|\sigma^2, \theta, \nu, g) &= \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left(\sqrt{2\nu} \frac{\|x-x'\|}{\theta}\right)^2 K_{\nu} \left(\sqrt{2 \nu}
\frac{\|x - x'\|}{\theta}\right) + g \cdot \mathbb{I} \nonumber \\
&= \sigma^2 \left(1 + \frac{\sqrt{5} \cdot \|x - x'\|}{\theta} +
\frac{5\cdot\|x-x'\|^2}{3\theta^2} \right) \text{exp}\left(-\frac{\sqrt{5} \cdot \|x
- x'\|}{\theta}\right) + g \delta_{x, x'}
\label{eq:matern}
\end{align}
GPs provide a flexible, nonparametric regression tool. In addition to computer
experiments, they are utilized in geostatistics
\citep{matheron1963principles}, spatial statistics
\citep{cressie2011statistics}, time series \citep{roberts2013gaussian},
optimization \citep{jones1998efficient}, and machine learning
\citep{rasmussen2003gaussian}. The reason for their popularity as surrogate
models in the world of computer experiments is their ability to interpolate
between observed data points (i.e. setting nugget $g = 0$). Although
stochastic computer models are becoming more common, computer experiments are
often deterministic, motivating the desire for a model that interpolates. GPs
are then useful by providing predictive surfaces that give higher estimates of
variance away from observed data, and virtually no variance at observed
locations. In fact, perhaps the hallmark of GPs is that they naturally provide
excellent uncertainty quantification for out-of-sample predictions, a matter
of importance to scientists exploring these computer models. Therefore, GPs
are widely used in computer experiments and Bayesian Inverse Problems.

%------------------------------------------------------------------------------
%% SECTION 2.3 - SEPIA / GPMSA
\subsection{SEPIA / GPMSA}
\label{sec:sepia}
%------------------------------------------------------------------------------

%% SEPIA / GPMSA
%% - O(n^3) bottleneck is nothing new
%% - Higdon et al. faced this at LANL in the early 2000s
%% - Their situation is unique in that n was small, but dimension is high
%% - Same situation as IBEX
%% - Instead of modeling the output as scalar, they did functional
%% - PCA
%%   - Show some equations
%%   - Maybe a graphic
%% - Must choose basis
%% - Used in calibration (an inverse problem), but does not support Poisson
In the early 2000s, \citet{higdoncalib2008} faced a similar challenge. Expensive
computer models at Los Alamos National Laboratory (LANL) produced limited runs with
extremely high-dimensional output in the form of images, shape description, time
series, etc. In order to efficiently fit a surrogate model to the simulator, they
needed to perform an approximation to limit the size of the data. As a solution, they
employed principal components analysis (PCA) to reduce the dimensionality of the
response. Specifically, they represented the output of a simulator model as the sum
of $n_k$ basis functions $\textbf{k}_1, \textbf{k}_2, \ldots , \textbf{k}_{n_k}$,
each scaled by a corresponding weight $w_i$, as shown in Eq. \ref{eq:higdon_pca}.
Instead of modeling the high-dimensional response, the GP only modeled the small set
of component weights. Bayesian inference was used to obtain the posterior
distribution of those component weights. Combining estimates the weights with the
calculated basis functions allowed for predictions out of sample. As a result of this
approximation, the dimension of the covariance matrix $\Sigma(X)$ in the predictive
equations of \ref{eq:mvn_eq} remained small.
\begin{align}
\textbf{y}(\textbf{x}, \textbf{u}) \approx \sum_{i=1}^{n_k} \textbf{k}_i \cdot w_i (\textbf{x}, \textbf{u}), \quad n_k \ll n \ \nonumber \\
w_i (\textbf{x}, \textbf{u}) \sim GP(\textbf{0}, \Sigma_{w_i}),
\label{eq:higdon_pca}
\end{align}
where $\Sigma_{w_i}$, dependent only on a limited number of combinations of
$\textbf{x}$ and $\textbf{u}$, is a $n_k \times n_k$ matrix. Hence, the decomposition
of $\Sigma_{w_i}$ avoids the O($n^3$) bottleneck in computation and can be completed
in a reasonable amount of time.

We propose a simpler approach, devoid of the definition and selection of how many
basis functions to use. Although this basis representation was successful, we argue
that what \citet{higdoncalib2008} really wanted to do was represent the functional
output as a univariate response. That is, replicate the entry in the input matrix $X$
for each response the number of times equal to the length of the output vector. In
essence, implicitly encode the index (e.g. pixel in image, moment in time series)
into the response vector for each observation in the input matrix. We believe that
had \citet{higdoncalib2008} had access to the capacity of modern-day computing, along
with the novel methodology proposed since the early 2000s that leverages that
compute, they would have steered their implementation in a significantly different
way. We propose that approach next and illustrate the improved performance in both
predictive accuracy and uncertainty quantification, along with a reduction in
computational expense.

%% Limitations:
%% Poisson response. Clunky way to deal with large data. Not generalizable.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 3: Methods
\section{Methods}
\label{sec:methods}

%------------------------------------------------------------------------------
%% SECTION 3.1 - Generalized Bayesian Inverse Problems with GP Surrogates
\subsection{Generalized Bayesian Inverse Problems with GP Surrogates}
\label{sec:gen_bayes_inv}
%------------------------------------------------------------------------------

%% GENERALIZED BAYESIAN INVERSE PROBLEMS WITH GP SURROGATES
%% - Return to review, but with general response
Computer simulations are not limited to modeling physical processes that
generate a continous response. For instance, a field experiment may return a
binary response, such as success or failure. Categorical responses may expand
beyond two classes, representing a multinomial reponse. We focus on
Poisson-distributed counts in this paper, but our framework can be generalized
to any response. We revisit Eq. \ref{eq:bsi}, but the Normal likelihood is
replaced with a Poisson likelihood. Our setup is below.
\begin{align}
\pi(\boldsymbol \lambda | Y, X) &= \frac{\mathcal{L}(Y|\boldsymbol \lambda, X)
\pi(\boldsymbol \lambda)}{\pi(Y|X)} \propto \mathcal{L}(Y|\boldsymbol \lambda,
X) \pi(\boldsymbol \lambda) \nonumber \\ &\propto \prod_{i=1}^N
   \frac{\lambda_i^{y_i}e^{-\lambda_i}}{y!} \pi(\boldsymbol \lambda) =
   \prod_{i=1}^N \frac{(r_i t_i)^{y_i}e^{-r_i t_i}}{y!}\pi(\boldsymbol
   \lambda)
\label{eq:bsi_pois}
\end{align}
Note that $\lambda_i$, the mean function at a certain input location $x_i$, is
represented as the product of an underlying rate, $r_i$, and an exposure time,
$t_i$. For unit exposure ($t_i=1$), the mean can simply be represented as
$\lambda_i$. However, in our application the data is given as counts over a
specific time interval.

Eq. \ref{eq:bsi_pois} gives us the form of the posterior. But without a
conjugate prior to provide an analytical solution, we must sample the
posterior of $\boldsymbol \lambda$ using MCMC. The difficulty is in the second
step of the for loop in Algorithm \ref{alg:gibbs}. Here we need to predict
$\boldsymbol \lambda$ at input locations $X$, but with updated calibration
parameters, $u^{(t)}$. Unfortunately, that falls to our computer model, which
is expensive and can take hours or days to complete, making MCMC infeasible.
Therefore, we need a surrogate for the simulator that can execute qucikly and
provide good predictions at new values of the calibration parameters. Here we
utilize a GP surrogate (\ref{sec:gp_surr}).
\begin{algorithm}[ht]
\DontPrintSemicolon
Initialize $u^{(0)}$, $\Sigma^{(0)}$. \\
\For{$t = 1, \dots, T$}{
  Propose $u^{(t)} \mid u^{(t-1)}, \Sigma^{(t-1)}$ via MH, \\
  Predict $\hat{f}(Y, T, X, u^{(t)})$ \\
  Evaluate $\mathcal{L}(Y|\hat{f})$ \\
  Accept or reject $u{(t)}$
  }
\caption{MCMC (Metropolis-within-Gibbs) for Poisson Inverse Bayes.}
\label{alg:gibbs}
\end{algorithm}
Now, before we get ahead of ourselves, letâ€™s expand upon our toy problem
introduced in \ref{sec:bayes_inv}. Instead of directly observing the function
$f(X)$, suppose we are given counts, drawn from a Poisson process with
$\boldsymbol \lambda = f(X)$, as shown in Figure 3. As described in
\ref{sec:bayes_inv}, we know that the underlying mean process is a function of
two parameters, $\mu$ and $\lambda$. We'll skip the idealized situation where
we have access to unlimited evaluations of the computer model. Instead, we'll
fit a GP surrogate to the model evaluations and surrogate predictions at each
MCMC iteration. Results are shown in Figure XX. We can see that our framework
again recovers the true values of the parameters, this time as the underlying
mean function generating counts $Y \sim \text{Poisson}(\lambda)$.

With any moderately-sized training dataset, fitting the surrogate model will
take significantly longer than making predictions at new input locations. Such
is the case for our application, as we will show further in \ref{sec:vecchia}.
Therefore, both fitting a surrogate and making predictions at each MCMC
iteration is unrealistic. So, we adopt the modularized approach of
\citep{bayarri2009modularization}. An initial step is added to Algorithm
\ref{alg:gibbs} to fit a GP surrogate to only the training data. Then, at each
iteration, the same GP surrogate is used to predict at new values of the
calibration parameters. This slight modification is illustrated in Algorithm
\ref{alg:gibbs_mod}.
\begin{algorithm}[ht]
\DontPrintSemicolon
Initialize $u^{(0)}$, $\Sigma^{(0)}$. \\
Fit GP surrogate $\hat{f}(Y, T X)$. \\
\For{$t = 1, \dots, T$}{
  Propose $u^{(t)} \mid u^{(t-1)}, \Sigma^{(t-1)}$ via MH, \\
  Predict $\hat{f}(Y, T, X, u^{(t)})$ \\
  Evaluate $\mathcal{L}(Y|\hat{f})$ \\
  Accept or reject $u{(t)}$
  }
\caption{MCMC (Metropolis-within-Gibbs) for Poisson Inverse Bayes.}
\label{alg:gibbs_mod}
\end{algorithm}
Even with the supposed ``shortcut'' in Algorithm \ref{alg:gibbs_mod}, we get
similar results, as shown in Figure XX. But the computational savings are
dramatic, and will become moreso as we get to the real data. All this to say,
as we make more and more approximations, we can still achieve similar or equal
performance.

% To begin

%% - Use link function
%% - GPMSA / SEPIA cannot do this
%% - Show some equations

%------------------------------------------------------------------------------
%% SECTION 3.1.1 - Examples
\subsubsection{Examples}
\label{sec:gen_bayes_inv_ex}
%------------------------------------------------------------------------------

%% EXAMPLES
%% - 1D example with Poisson response
%%   - note that we are treating the response as a scalar
%%   - response is Poisson
%%   - Graphic showing field / computer model data, fit, and posterior over parameters
%% - 2D example with Poisson response
%%   - Same Thing as above
%%   - LEAD IN: capacity of GP is stretched

%------------------------------------------------------------------------------
%% SECTION 3.2 - Vecchia Approximation
\subsection{Vecchia Approximation}
\label{sec:vecchia}
%------------------------------------------------------------------------------

%% VECCHIA APPROXIMATION
%% - Review improvement in computation
%% - Higdon may have modeled it this way if he had the resources
%% - List GP approximations from past decade
%% - We use Vecchia
%% - Show formulas
%% - Specify that we use Scaled Vecchia approximation, what it is
Perhaps the primary weakness of GPs is the need to decompose an $n \times n$
covariance matrix, making it difficult to scale to large datasets. Many methods have
been introduced to address this computational burden. These include, but are not
limited to, fixed-rank kriging \citep{cressie2008fixed}, inducing points
\citep{banerjee2008gaussian}, compactly supported kernels
\citep{kaufman2011efficient}, divide-and-conquer \citep{gramacy2015local}, and
nearest neighbors \citep{datta2016hierarchical, wu2022variational}. In the context of
computer model calibration, \citet{higdoncalib2008} use a basis representation via
principal components to accommodate high-dimensional output. However, with the
dramatic increase in computing capacity over the past decade and the aforementioned
advances in methodology, we believe there are better GP approximations available to
model the output of a computer simulation. We propose the recently popularized
Vecchia approximation \citep{vecchia1988estimation, katzfuss2020vecchia,
katzfuss2021general, zhang2022multi}.

The Vecchia approximation relies on the ability to write a likelihood function as the
product of conditional likelihoods as shown below.
\begin{align}
L(Y)&=\prod_{i=1}^n L(Y_i \mid Y_{g(i)})
\hspace{0.3em} \mbox{ where } g(1)=\emptyset \hspace{0.3em} \mbox{ and } g(i)=\{1, 2,
\ldots, i-1\} \nonumber \\
&\approx \prod_{i=1}^{n}L(Y_i \mid Y_{h(i)}) \hspace{0.3em} \mbox{ where } h(i)
\subset \{1, 2, \ldots, i-1\} \hspace{0.3em} \mbox{ and } |h(i)| = m \ll n
\label{eq:vecchia}
\end{align}
Reducing the size of the conditioning set ($h(i)$ above) induces sparsity in the
precision matrix, speeding up the evaluation of the likelihood. This takes an
operation of order $O(n^3)$ down to $O(nm^3)$, motivating small values of $m$. As the
size of the conditioning set $m$ increases, the quality of the approximation
improves, until $n=m-1$, and the likelihood is no longer an approximation. We
find a value of $m < 30$ is more than sufficient. The conditioning set can be
constructed in many ways, but we default to a nearest-neighbors approach, which has
been done previously \citep{sauer2023active}.

%------------------------------------------------------------------------------
%% SECTION 3.2.1 - Examples
\subsubsection{Examples}
\label{sec:vecchia_ex}
%------------------------------------------------------------------------------

%% EXAMPLES
%% - 2D example with LOTs of data
%%   - Show results. Compare to previous
%% - Show timing test on simulated data
%%   - Graphic should include fitting and prediction
%%   - Include different number of bases for SEPIA, maybe compared with different m
%%   - Show performance of different models

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 4: Simulations
\section{Simulations}
\label{sec:sims}

%------------------------------------------------------------------------------
%% SECTION 4.1 - Heliospheric Science
\subsection{Heliospheric Science}
\label{sec:helio_science}
%------------------------------------------------------------------------------

%% HELIOSPHERIC SCIENCE
%% - Short review of heliosphere
%% - Behavior of Energetic Neutral Atoms

%% HELIOSPHERE COMPUTER MODEL
%% - Introduce computer model
%% - Explanation of parameters in computer model
%% - Introduce the data from the computer model

%------------------------------------------------------------------------------
%% SECTION 4.2 - Experiment
\subsection{Experiment}
\label{sec:ibex_sim_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from simulated data
%% - Lead-in to real data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 5: Real Data
\section{Real Data}
\label{sec:ibex_real}

%------------------------------------------------------------------------------
%% SECTION 5.1 - IBEX
\subsection{IBEX}
\label{sec:ibex}
%------------------------------------------------------------------------------

%% IBEX
%% - Reintroduce satellite
%% - Explain details of data
%% - Explain the years gathered

%------------------------------------------------------------------------------
%% SECTION 5.2 - EXPERIMENT
\subsection{Experiment}
\label{sec:ibex_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from the real data
%% - Comment on the missalignment between simulator and reality

%------------------------------------------------------------------------------
%% SECTION 5.3 - DISCREPANCY
\subsection{Discrepancy}
\label{sec:ibex_discrep}
%------------------------------------------------------------------------------

%% DISCREPANCY
%% - Refer to KOH, Higdon, and their use of discrepancy
%% - Introduce simple scaling discrepancy (iniclude some equations)
%% - Refer to results in Appendix for simulated data where we discover the true scale
%% - Explain set up for IBEX calibration with discrepancy
%% - Show results
%% - Comment on complexity of discrepancy. Scale isn't sufficient.
%% - Perhaps a stationary GP is not even sufficient

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 6: Discussion
\section{Discussion}
\label{sec:discuss}

Discussion.

\subsection*{Acknowledgments}

RBG and SDB are grateful for funding from NSF CMMI 2152679. This work has been
approved for public release under LA-UR-??-?????. SDB, DO and LJB were funded
by Laboratory Directed Research and Development (LDRD) Project 20220107DR.

\bibliography{ibex_bayes_inv}
\bibliographystyle{jasa}

\appendix

\end{document}
