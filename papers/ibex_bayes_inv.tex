\documentclass[12pt]{article}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{caption}
\usepackage{dcolumn}
\usepackage{filemod}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{makecell}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=7in
\textheight=8.75in
\topmargin=-.5in
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\footskip=0.5in

\begin{document}

\title{Bayesian Statistical Inversion for High-Dimensional Computer Model
Output and Spatially Distributed Counts}
\author{Steven D. Barnett\thanks{Corresponding author
\href{mailto:sdbarnett@vt.edu}{\tt sdbarnett@vt.edu}, Department of
Statistics, Virginia Tech} \and Robert B. Gramacy\thanks{Department of
Statistics, Virginia Tech} \and Lauren J. Beesley\thanks{Statistical Sciences
Group, Los Alamos National Laboratory} \and Dave Osthus\footnotemark[3] \and
Yifan Huang\thanks{ Nuclear and Particle Physics, AstroPhysics and Cosmology,
Los Alamos National Laboratory} \and Fan Guo\footnotemark[4] \and Eric J.
Zirnstein\thanks{ Department of Astrophysical Sciences, Princeton University}
\and Daniel B. Reisenfeld\thanks{Space Science and Applications Group, Los
Alamos National Laboratory}}
\date{}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
The Interstellar Boundary Explorer (IBEX) satellite detects energetic neutral
atoms (ENAs) and determines the rate at which they are generated in the
heliosphere. Computer models attempt to represent the physical process that
produces heliospheric ENAs through specified model parameters. Bayesian
Statistical Inversion enables statisticians to use data collected in a field
experiment along with simulator output from a variety of parameter settings to
make out-of-sample predictions and learn the posterior distributions of model
parameters. However, inverse problems typically assume a Gaussian or
continuous response for both field and computer model data. We introduce a
novel Markov chain Monte Carlo framework that accommodates the spatially
distributed Poisson counts collected by the IBEX satellite. Additionally, the
computer simulation in our application is limited in the amount of runs it can
perform, but each run's output is quite large. Therefore, we propose the use
of the Scaled Vecchia approximation as a Gaussian process (GP) surrogate. We
demonstrate the capability of our proposed framework through multiple
simulated examples and show that we can consistently recover the true
parameters in a discrepancy-free environment and obtain accurate out-of-sample
prediction. We apply this to the IBEX satellite data and the corresponding
computer model output.

\bigskip
\noindent \textbf{Key words:} Gaussian process, surrogate modeling,
calibration, heliospheric science, IBEX, Vecchia approximation
\end{abstract}

\doublespacing % no double spacing for arXiv

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

%% IBEX
%% - Talk about the satellite
%% - Explain the mission
%% - Introduce the idea of a computer model
%% - Explain desire to understand the distribution of parameters
The Interstellar Boundary Explorer (IBEX) satellite was launched in 2008 in an effort
to deepen scientist's understanding of the heliosphere as part of the Small Explorer
program \citep{mccomas2009aIBEX} sponsored by the National Aeronautics and Space
Administration (NASA). The heliosphere is the bubble formed by the solar wind that
encompasses our solar system that forms a barrier between our planetary system and
interstellar space. In particular, the behavior at the edge of the heliosphere, also
known as the heliopause, is of interest. Here, highly energized hydrogen ions in the
solar wind interact with neutral atoms in interstellar space, occasionally undergoing
electron exchange and become neutral themselves. Being unaffected by magnetic fields,
these energetic neutral atoms (ENAs) then travel in a straight line. As a result,
some ENAs make it to Earth and can be detected by IBEX. IBEX contains an instrument
called the IBEX-Hi ENA imager \citep{funsten2009IBEXHiENA}, which records the energy
level and approximate location of origin for each ENA that enters the satellite,
providing sufficient data to estimate the rate at which ENAs are created throughout
the heliopause. An example of this estimated surface or image, referred to as a
\textit{sky map} by space scientists, can be found in the left panel of Fig.
\ref{f:fig1}.

One can think of the heliosphere as a boat moving through water, the water
here representing interstellar space. At the outset of IBEX's mission,
scientists expected to see a higher rate of ENAs being generated at the front
(nose) and back (tail), much like the turbulent interaction between a boat and
water at its bow and stern. And indeed, IBEX validated that belief in its
collected data. This feature is termed \textit{globally distributed flux}, or
GDF, by the space science community. More ENAs are produced at the nose and
tail where there is an increased interaction between hydrogen ions and neutral
particles in the interstellar medium. However, in a completely unanticipated
finding, IBEX also recorded a string of higher rates of ENAs
\citep{fuselier2009IBEXribbon} being created that wraps around the
heliosphere. Scientists now refer to this phenomenom as the \textit{ribbon}.
This unique feature of a \textit{sky map} is clearly visible in Fig.
\ref{f:fig1}. Over the past 20 years, space scientists have conducted
extensive research to explain the existence of the \textit{ribbon} and have
proposed dozens of theories to describe the physical process that generates it
\citep{mccomas2014ibex, zirnstein2018role, zirnstein2019strong,
zirnstein2021dependence}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.74,trim=0 0 65 0,clip=TRUE]{ibex_real1.pdf}
\includegraphics[scale=0.74,trim=35 0 65 0,clip=TRUE]{sim1.pdf}
\includegraphics[scale=0.74,trim=35 0 0 0,clip=TRUE]{sim2.pdf}
\caption{Observed rate of energetic neutral atoms (ENA) detected by the
Interstellar Boundary Explorer (IBEX) satellite (left) and output from two
runs of the IBEX computer simulation at different settings (middle, right).
\label{f:fig1}}
\end{figure}

In order to further explore these theories, scientists have developed computer
models to simulate the creation of a \textit{sky map}. These computer
simulations rely on a number of parameters that can be varied, thus modifying
the shape and intensity of both ribbon and GDF. Although many simulations
exist, we focus on two that are publicly available: a GDF model proposed by
\citet{zirnstein2021heliosheath} and a ribbon-only model developed by
\citet{zirnsteinGDFsims2015} representing the proposed Spatial Retention
model. The ribbon-only model relies on two parameters, parallel mean free path
and ratio, and the GDF model relies on two additional parameters, X1 and X2.
Understaing the distribution of these variables would be instrumental in
determining which proposed theory carries the most weight, or which
combination of parameter values is the most probable. So far, minimal work has
been done to validate theories or further understand the heliosphere by
pairing the computer model output with the raw data collected by IBEX.

%% PROBLEM
%% - Classic Inverse Problem
%% - Mathematical model is expensive (and perhaps not accurate)
%% - Functional output is limited, but in high dimension
%% - Physical observations are counts
Now, this is a classical statistical inverse problem. Given observed data and
an assumed true model that generates the data, the goal is to determine the
values of the parameters in the model. A classic approach to solving the
inverse problem is to evaluate the model with a myriad of unique combination
of paramter values. Then, select the set of parameter values that minimize the
distance between model output and real observations. However, there are
limitations to that technique. For one, computer models are often expensive,
often taking hours, days, or weeks (as is the case in our IBEX example),
precluding the execution of an extensive set of parameter combinations. So, a
Bayesian approach is often better. Applying Bayesian methods allows practitioners
to inform the model with prior information on the paramters. Additionally,
Bayesian statistical inversion (BSI) treats the parameters as random variables
and provides more information through an estimated distribution over the
most likely values.

%% OTHER WORK
%% - Bayesian Inverse Problems (BIP) w/ GP
%%   - Not large scale
%%   - Not Poisson
%% - BIP w/ functional data
%%   - more intuitive and scaleable way
%% - Computer Model Calibration
%%   - SEPIA / GPMSA
%%     - Not Poisson
%%     - Simpler Approach
%% - Insert a table on what is out there
Bayesian statistical inverse problems are not new. But the data collected by
IBEX and its corresponding simulated representation present a unique
challenge. Table \ref{tab:prev_work} summarizes the features necessary to solve the
IBEX inverse problem and reviews the capabilities present in previous work.
\citep{kennedyohagan2001} offer the canonical computer model calibration
framework, which provides a fully Bayesian approach to estimate the
distribution of calibration parameters and improve prediction out-of-sample.
However, their framework is limited to Gaussian observations and small,
simulator output. \citep{higdoncalib2008} also employ Bayesian methods when
computer model output is functional and high-dimensional. In addition to the
previous restriction to Gaussian data, \citet{higdoncalib2008} require the
user to specify a number of basis functions to represent the data.
\citep{gramacy2015calibrating} consider computer model calibration with
large-scale output from a simulator, necessitating an approximation. But their
implementation is not Bayesian, the scale of data is still orders of magnitude
less than IBEX, and the large-scale data only applies to the computer model,
not the field data. \citep{grosskopfcountcalib2020} provide a extension of the
Kennedy \& O'Hagan (hereafter referred to as KOH) framework to non-Gaussian
data that maintains a Bayesian approach. But they are limited to small amounts
of data and do not consider high dimensions or calibration parameters. In
contrast, our approach checks all the boxes to achieve the goal of IBEX space
scientists.
\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
& Bayesian & \makecell{Non-Gaussian \\ Observations} & \makecell{Large-Scale \\ Computer Model \\ Output} & \makecell{High-Dimensional \\ Output} \\
\hline
Our Contribution & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
\citep{kennedyohagan2001} & \checkmark & & & \\
\hline
\citep{higdoncalib2008} & \checkmark & & & \checkmark \\
\hline
\citep{gramacy2015calibrating} & & \checkmark & & \\
\hline
\citep{grosskopfcountcalib2020} & \checkmark & \checkmark & & \\
\hline
\end{tabular}
\caption{A review of functionality for statistical inverse problems in
previous academic works. A check mark indicates that functionality exists
within the paper and/or accompanying software.}
\label{tab:prev_work}
\end{table}

%% OUTLINE
%% - Review Bayesian Inverse Problems, GP Surrogates, SEPIA
%% - Introduce our framework
%% - Explain use of Vecchia approximation
%% - Demonstrate on simulated data
%% - Apply to real IBEX data
Our paper is laid out as follows. First, Section \ref{sec:review} reviews some of the methodology
that form the foundation for our work, namely Bayesian Inverse Problems and
Gaussian process surrogate models. We also provide a brief description of the
SEPIA implementation, the current status quo in Bayesian Inverse Problems. In
Section \ref{sec:gen_bayes_inv} we introduce our framework for solving the
inverse problem in non-Gaussian contexts. Section \ref{sec:vecchia} describes
our use of the Vecchia approximation to stretch the capacity of our GP
surrogate for large-scale simulator output. In Section \ref{sec:sims}, we
illustrate the effectiveness of our framework in discovering the true,
underlying calibration parameters on simulated data. We apply our methods to
the real IBEX satellite data in Section \ref{sec:ibex_real}. To finish off, we
discuss current and future work in this area in Section \ref{sec:discuss}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 2: Review:
\section{Review}
\label{sec:review}

%------------------------------------------------------------------------------
%% SECTION 2.1 - Bayesian Inverse Problems
\subsection{Bayesian Inverse Problems}
\label{sec:bayes_inv}
%------------------------------------------------------------------------------

%% BAYESIAN INVERSE PROBLEMS
%% - What is an inverse problem and why do we care?
%% - Obtained observations from a physical process to learn more about it
%% - We have some knowledge of the workings of the true process
%% - We can build a mathematical model, and therefore a computer model
%% - Classic IP: evaluate model as much as possible to find parameters/inputs
%%   that produce output aligned with the observations
Statistical inverse problems involve the collection of observations from a
physical experiment in an effort to learn more about the process that
generates these data points. Typically, a scientist has some knowledge of the
physical or mathematical model (i.e. the data-generating process), but is
seeking to learn more. The goals are two-fold: 1) estimate the parameters (or
distribution of the parameters) that govern the physical model, and 2) if the
inputs to the experiment are unknown but the parameters are known, reconstruct
the inputs from which the observations originated. The motivation for such a
method stems from the inability to either control or precisely measure the
factors (e.g. the force of gravity) in a physical experiment. Therefore, in an
effort to gain more insight, scientists develop mathematical models, often
resulting in a computer simulation, that attempt to represent that physical
process. By comparing observations to simulator output generated via unique
combinations of model parameters, statisticians attempt to learn the true,
underlying parameter values. As mentioned in Section \ref{sec:intro}, the
classical approach is to find the best match between observed data and
extensive computer model via some minimization of error (i.e. RMSE, MAE).

%% - Observations are noisy
%% - Desire full UQ on parameters
%% - Go Bayesian
%% - Show Prior / Posterior
%% - No analytical form, must resort to MCMC
%% - LEAD IN: Computer model is expensive. Need a surrogate.
Bayesian statistical inversion (BSI) entails imposing a prior on model
parameters that embeds a certain amount of subject matter knowledge into the
analysis. Equation \ref{eq:bsi} shows the standard implementation of Bayes'
rule. Here, $\boldsymbol \theta$ represents the model parameters we wish to
infer and obtain uncertainty estimates for and $Y$ is our vector of physical
observations at input locations $X$.
\begin{align}
\pi(\boldsymbol \theta | Y, X) = \frac{\mathcal{L}(Y|\boldsymbol \theta, X)
\pi(\boldsymbol \theta)}{\pi(Y|X)} \propto \mathcal{L}(Y|\boldsymbol \theta,
X) \pi(\boldsymbol \theta)
\label{eq:bsi}
\end{align}
If $\pi(\boldsymbol \theta)$ is conjugate, inference for $\boldsymbol \theta$
is simple and can be done analytically. However, this is often not the case,
requiring an approximation of the posterior (i.e. Laplace, Variational Bayes).
We utilize Markov chain Monte Carlo (MCMC) methods to sample from the
posterior.

%% DO A REVIEW OF OTHER METHODS (KOH? BAYES INV? STUART? TECKENTRUP? COLLINS?)

One difficulty in our situation is the expense of the computer model. As a
result of increased access to supercomputing resources, computer models have
increased in complexity over the past two decades and are able to produce
extremely high-fidelity output. However, even with modern-day computing power,
some simulations can take hours, days, or weeks to complete. This limits the
amount of runs that can be executed and therefore restricts the exploration of
the model parameter space. Therefore, we need a model (what we call a
surrogate, or emulator) fit to the simulator output to make accurate and
reliable predictions for out-of-sample values of our computer model
parameters. Any statistical model can serve as a surrogate, ranging from a
linear regression to complex neural networks. However, the canonical surrogate
for computer experiments and computer model calibration has come to be the
Gaussian process \citep{gramacy2020surrogates}.

%------------------------------------------------------------------------------
%% SECTION 2.2 - Gaussian Process Surrogates:
\subsection{Gaussian Process Surrogates}
\label{sec:gp_surr}
%------------------------------------------------------------------------------

%% GAUSSIAN PROCESS SURROGATES
%% - What is the motivation? Expensive computer experiments
%% - Set of n observations that follow a MVN
%% - Often mean-zero. In practice this means subtracting off the mean
%% - Results in all the action being in the covariance function
%% - Covariance depends on pairwise distance
%% - Explain some different kernels
%% - Talk about prediction. Show Kriging equations
%% - Nonparametric, flexible regression tool
%% - A lot of different applications
%% - Become popular in computer experiments because of interpolation
%% - This is what we want
%% - Bottleneck is O(n^3)
%% - LEAD IN: Output can be large. Need an approximation.
%% Review Gaussian processes and their use in surrogate modeling
A Gaussian process, hereafter referred to as a GP, is a realization of a
multivariate normal distribution, governed by some mean and covariance
functions. GPs are fit to observed data and used to make predictions at new
input locations, relying on the conditional multivariate normal or
\textit{kriging} equations \citep{matheron1963principles}. For example,
suppose we have a vector of observations $Y$ at input locations $X$. $Y$ is an
$n \times 1$ vector and $X$ is an $n \times p$ matrix. We make the assumption
that $Y$ is normally distributed with some mean $(\mu(X))$ and covariance
$(\Sigma(X))$, both functions of $X$. Predicting at a new location $x$ is
straightforward. Eq.~(\ref{eq:mvn_eq}) articulates how to obtain mean
predictions $\mu^*(x)$ and corresponding uncertainty quantification
$\Sigma^*(x)$.
\begin{align}
Y(X) &\sim \mathcal{N}\!\left(\mu(X), \Sigma(X)\right) \nonumber \\
y^*(x) \mid Y(X) &\sim \mathcal{N}\!\left(\mu^*(x), \Sigma^*(x)\right) \nonumber \\
\mu^*(x)&=\mu(x) + \Sigma(x, X) \Sigma(X)^{-1} (Y - \mu(X)) \nonumber \\
\Sigma^*(x)&=\Sigma(x, x) - \Sigma(x, X) \Sigma(X)^{-1} \Sigma(X, x) \label{eq:mvn_eq}
\end{align}
Although a generic mean function is specified in Eq.~(\ref{eq:mvn_eq}), it is
common to set $\mu(x)=0$. In practice, this simply implies centering $Y$ by
subtracting off the sample mean, $\overline{Y}$. In this zero-mean context,
all the action takes place in the covariance function. The covariance function
typically depends on pairwise distances between input locations $x$ and $x'$.
Many covariance kernels exists, such as the Gaussian, Exponential, Matern. We
use the Matern kernel in our work, of which both the Gaussian and Exponential
kernels are special cases. The behavior of these kernels is determined by
three hyperparameters: a nugget $g$, a lengthscale $\theta$, and a scale
$\tau^2$. In the Matern kernel, an additional parameter ($\nu$) exists, which
controls the smoothness of the estimated surface. But we will not estimate the
smoothness parameter here and instead set $\nu =
\frac{5}{2}$ (\ref{eq:matern}), following common practice due to it allowing
the kernel function to be expressed in a simpler form. All hyperparameters can
be estimated using your preferred method (i.e. maximum likelihood, method of
moments). We will use Bayesian methods, specifically MCMC.
\begin{align}
\Sigma(x, x'|\sigma^2, \theta, \nu, g) &= \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left(\sqrt{2\nu} \frac{\|x-x'\|}{\theta}\right)^2 K_{\nu} \left(\sqrt{2 \nu}
\frac{\|x - x'\|}{\theta}\right) + g \cdot \mathbb{I} \nonumber \\
&= \sigma^2 \left(1 + \frac{\sqrt{5} \cdot \|x - x'\|}{\theta} +
\frac{5\cdot\|x-x'\|^2}{3\theta^2} \right) \text{exp}\left(-\frac{\sqrt{5} \cdot \|x
- x'\|}{\theta}\right) + g \delta_{x, x'}
\label{eq:matern}
\end{align}
GPs provide a flexible, nonparametric regression tool. In addition to computer
experiments, they are utilized in geostatistics
\citep{matheron1963principles}, spatial statistics
\citep{cressie2011statistics}, time series \citep{roberts2013gaussian},
optimization \citep{jones1998efficient}, and machine learning
\citep{rasmussen2003gaussian}. The reason for their popularity as surrogate
models in the world of computer experiments is their ability to interpolate
between observed data points (i.e. setting nugget $g = 0$). Although
stochastic computer models are becoming more common, computer experiments are
often deterministic, motivating the desire for a model that interpolates. GPs
are then useful by providing predictive surfaces that give higher estimates of
variance away from observed data, and virtually no variance at observed
locations. In fact, perhaps the hallmark of GPs is that they naturally provide
excellent uncertainty quantification for out-of-sample predictions, a matter
of importance to scientists exploring these computer models. Therefore, GPs
are widely used in computer experiments and Bayesian Inverse Problems.

%------------------------------------------------------------------------------
%% SECTION 2.3 - SEPIA / GPMSA
\subsection{SEPIA / GPMSA}
\label{sec:sepia}
%------------------------------------------------------------------------------

%% SEPIA / GPMSA
%% - O(n^3) bottleneck is nothing new
%% - Higdon et al. faced this at LANL in the early 2000s
%% - Their situation is unique in that n was small, but dimension is high
%% - Same situation as IBEX
%% - Instead of modeling the output as scalar, they did functional
%% - PCA
%%   - Show some equations
%%   - Maybe a graphic
%% - Must choose basis
%% - Used in calibration (an inverse problem), but does not support Poisson
In the early 2000s, \citet{higdoncalib2008} faced a similar challenge. Expensive
computer models at Los Alamos National Laboratory (LANL) produced limited runs with
extremely high-dimensional output in the form of images, shape description, time
series, etc. In order to efficiently fit a surrogate model to the simulator, they
needed to perform an approximation to limit the size of the data. As a solution, they
employed principal components analysis (PCA) to reduce the dimensionality of the
response. Specifically, they represented the output of a simulator model as the sum
of $n_k$ basis functions $\textbf{k}_1, \textbf{k}_2, \ldots , \textbf{k}_{n_k}$,
each scaled by a corresponding weight $w_i$, as shown in Eq. \ref{eq:higdon_pca}.
Instead of modeling the high-dimensional response, the GP only modeled the small set
of component weights. Bayesian inference was used to obtain the posterior
distribution of those component weights. Combining estimates the weights with the
calculated basis functions allowed for predictions out of sample. As a result of this
approximation, the dimension of the covariance matrix $\Sigma(X)$ in the predictive
equations of \ref{eq:mvn_eq} remained small.
\begin{align}
\textbf{y}(\textbf{x}, \textbf{u}) \approx \sum_{i=1}^{n_k} \textbf{k}_i \cdot w_i (\textbf{x}, \textbf{u}), \quad n_k \ll n \ \nonumber \\
w_i (\textbf{x}, \textbf{u}) \sim GP(\textbf{0}, \Sigma_{w_i}),
\label{eq:higdon_pca}
\end{align}
where $\Sigma_{w_i}$, dependent only on a limited number of combinations of
$\textbf{x}$ and $\textbf{u}$, is a $n_k \times n_k$ matrix. Hence, the decomposition
of $\Sigma_{w_i}$ avoids the O($n^3$) bottleneck in computation and can be completed
in a reasonable amount of time.

We propose a simpler approach, devoid of the definition and selection of how many
basis functions to use. Although this basis representation was successful, we argue
that what \citet{higdoncalib2008} really wanted to do was represent the functional
output as a univariate response. That is, replicate the entry in the input matrix $X$
for each response the number of times equal to the length of the output vector. In
essence, implicitly encode the index (e.g. pixel in image, moment in time series)
into the response vector for each observation in the input matrix. We believe that
had \citet{higdoncalib2008} had access to the capacity of modern-day computing, along
with the novel methodology proposed since the early 2000s that leverages that
compute, they would have steered their implementation in a significantly different
way. We propose that approach next and illustrate the improved performance in both
predictive accuracy and uncertainty quantification, along with a reduction in
computational expense.

%% Limitations:
%% Poisson response. Clunky way to deal with large data. Not generalizable.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 3: Methods
\section{Methods}
\label{sec:methods}

%------------------------------------------------------------------------------
%% SECTION 3.1 - Generalized Bayesian Inverse Problems with GP Surrogates
\subsection{Generalized Bayesian Inverse Problems with GP Surrogates}
\label{sec:gen_bayes_inv}
%------------------------------------------------------------------------------

%% GENERALIZED BAYESIAN INVERSE PROBLEMS WITH GP SURROGATES
%% - Return to review, but with general response
Computer simulations are not limited to modeling physical processes that
generate a continous response. For instance, a field experiment may return a
binary response, such as success or failure. Categorical responses may expand
beyond two classes, representing a multinomial reponse. We focus on
Poisson-distributed counts in this paper, but our framework can be generalized
to any response. We revisit Eq. \ref{eq:bsi}, but the Normal likelihood is
replaced with a Poisson likelihood. Our setup is below.
\begin{align}
\pi(\boldsymbol \lambda | Y, X) &= \frac{\mathcal{L}(Y|\boldsymbol \lambda, X)
\pi(\boldsymbol \lambda)}{\pi(Y|X)} \propto \mathcal{L}(Y|\boldsymbol \lambda,
X) \pi(\boldsymbol \lambda) \nonumber \\ &\propto \prod_{i=1}^N
   \frac{\lambda_i^{y_i}e^{-\lambda_i}}{y!} \pi(\boldsymbol \lambda) =
   \prod_{i=1}^N \frac{(r_i t_i)^{y_i}e^{-r_i t_i}}{y!}\pi(\boldsymbol
   \lambda)
\label{eq:bsi_pois}
\end{align}
Note that $\lambda_i$, the mean function at a certain input location $x_i$, is
represented as the product of an underlying rate, $r_i$, and an exposure time,
$t_i$. For unit exposure ($t_i=1$), the mean can simply be represented as
$\lambda_i$. However, in our application the data is given as counts over a
specific time interval.

Eq. \ref{eq:bsi_pois} gives us the form of the posterior. But without a
conjugate prior to provide an analytical solution, we must sample the
posterior of $\boldsymbol \lambda$ using MCMC. The difficulty is in the second
step of the for loop in Algorithm \ref{alg:gibbs}. Here we need to predict
$\boldsymbol \lambda$ at input locations $X$, but with updated calibration
parameters, $u^{(t)}$. Unfortunately, that falls to our computer model, which
is expensive and can take hours or days to complete, making MCMC infeasible.
Therefore, we need a surrogate for the simulator that can execute qucikly and
provide good predictions at new values of the calibration parameters. Here we
utilize a GP surrogate (\ref{sec:gp_surr}).
\begin{algorithm}[ht]
\DontPrintSemicolon
Initialize $u^{(0)}$, $\Sigma^{(0)}$. \\
\For{$t = 1, \dots, T$}{
  Propose $u^{(t)} \mid u^{(t-1)}, \Sigma^{(t-1)}$ via MH, \\
  Predict $\hat{f}(Y, T, X, u^{(t)})$ \\
  Evaluate $\mathcal{L}(Y|\hat{f})$ \\
  Accept or reject $u{(t)}$
  }
\caption{MCMC (Metropolis-within-Gibbs) for Poisson Inverse Bayes.}
\label{alg:gibbs}
\end{algorithm}
Now, before we get ahead of ourselves, letâ€™s expand upon our toy problem
introduced in \ref{sec:bayes_inv}. Instead of directly observing the function
$f(X)$, suppose we are given counts, drawn from a Poisson process with
$\boldsymbol \lambda = f(X)$, as shown in Figure 3. As described in
\ref{sec:bayes_inv}, we know that the underlying mean process is a function of
two parameters, $\mu$ and $\lambda$. We'll skip the idealized situation where
we have access to unlimited evaluations of the computer model. Instead, we'll
fit a GP surrogate to the model evaluations and surrogate predictions at each
MCMC iteration. Results are shown in Figure XX. We can see that our framework
again recovers the true values of the parameters, this time as the underlying
mean function generating counts $Y \sim \text{Poisson}(\lambda)$.

With any moderately-sized training dataset, fitting the surrogate model will
take significantly longer than making predictions at new input locations. Such
is the case for our application, as we will show further in \ref{sec:vecchia}.
Therefore, both fitting a surrogate and making predictions at each MCMC
iteration is unrealistic. So, we adopt the modularized approach of
\citep{bayarri2009modularization}. An initial step is added to Algorithm
\ref{alg:gibbs} to fit a GP surrogate to only the training data. Then, at each
iteration, the same GP surrogate is used to predict at new values of the
calibration parameters. This slight modification is illustrated in Algorithm
\ref{alg:gibbs_mod}.
\begin{algorithm}[ht]
\DontPrintSemicolon
Initialize $u^{(0)}$, $\Sigma^{(0)}$. \\
Fit GP surrogate $\hat{f}(Y, T X)$. \\
\For{$t = 1, \dots, T$}{
  Propose $u^{(t)} \mid u^{(t-1)}, \Sigma^{(t-1)}$ via MH, \\
  Predict $\hat{f}(Y, T, X, u^{(t)})$ \\
  Evaluate $\mathcal{L}(Y|\hat{f})$ \\
  Accept or reject $u{(t)}$
  }
\caption{MCMC (Metropolis-within-Gibbs) for Poisson Inverse Bayes.}
\label{alg:gibbs_mod}
\end{algorithm}
Even with the supposed ``shortcut'' in Algorithm \ref{alg:gibbs_mod}, we get
similar results, as shown in Figure XX. But the computational savings are
dramatic, and will become moreso as we get to the real data. All this to say,
as we make more and more approximations, we can still achieve similar or equal
performance.

% To begin

%% - Use link function
%% - GPMSA / SEPIA cannot do this
%% - Show some equations

%------------------------------------------------------------------------------
%% SECTION 3.1.1 - Examples
\subsubsection{Examples}
\label{sec:gen_bayes_inv_ex}
%------------------------------------------------------------------------------

%% EXAMPLES
%% - 1D example with Poisson response
%%   - note that we are treating the response as a scalar
%%   - response is Poisson
%%   - Graphic showing field / computer model data, fit, and posterior over parameters
%% - 2D example with Poisson response
%%   - Same Thing as above
%%   - LEAD IN: capacity of GP is stretched

%------------------------------------------------------------------------------
%% SECTION 3.2 - Vecchia Approximation
\subsection{Vecchia Approximation}
\label{sec:vecchia}
%------------------------------------------------------------------------------

%% VECCHIA APPROXIMATION
%% - Review improvement in computation
%% - Higdon may have modeled it this way if he had the resources
%% - List GP approximations from past decade
%% - We use Vecchia
%% - Show formulas
%% - Specify that we use Scaled Vecchia approximation, what it is

%------------------------------------------------------------------------------
%% SECTION 3.2.1 - Examples
\subsubsection{Examples}
\label{sec:vecchia_ex}
%------------------------------------------------------------------------------

%% EXAMPLES
%% - 2D example with LOTs of data
%%   - Show results. Compare to previous
%% - Show timing test on simulated data
%%   - Graphic should include fitting and prediction
%%   - Include different number of bases for SEPIA, maybe compared with different m
%%   - Show performance of different models

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 4: Simulations
\section{Simulations}
\label{sec:sims}

%------------------------------------------------------------------------------
%% SECTION 4.1 - Heliospheric Science
\subsection{Heliospheric Science}
\label{sec:helio_science}
%------------------------------------------------------------------------------

%% HELIOSPHERIC SCIENCE
%% - Short review of heliosphere
%% - Behavior of Energetic Neutral Atoms

%% HELIOSPHERE COMPUTER MODEL
%% - Introduce computer model
%% - Explanation of parameters in computer model
%% - Introduce the data from the computer model

%------------------------------------------------------------------------------
%% SECTION 4.2 - Experiment
\subsection{Experiment}
\label{sec:ibex_sim_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from simulated data
%% - Lead-in to real data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 5: Real Data
\section{Real Data}
\label{sec:ibex_real}

%------------------------------------------------------------------------------
%% SECTION 5.1 - IBEX
\subsection{IBEX}
\label{sec:ibex}
%------------------------------------------------------------------------------

%% IBEX
%% - Reintroduce satellite
%% - Explain details of data
%% - Explain the years gathered

%------------------------------------------------------------------------------
%% SECTION 5.2 - EXPERIMENT
\subsection{Experiment}
\label{sec:ibex_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from the real data
%% - Comment on the missalignment between simulator and reality

%------------------------------------------------------------------------------
%% SECTION 5.3 - DISCREPANCY
\subsection{Discrepancy}
\label{sec:ibex_discrep}
%------------------------------------------------------------------------------

%% DISCREPANCY
%% - Refer to KOH, Higdon, and their use of discrepancy
%% - Introduce simple scaling discrepancy (iniclude some equations)
%% - Refer to results in Appendix for simulated data where we discover the true scale
%% - Explain set up for IBEX calibration with discrepancy
%% - Show results
%% - Comment on complexity of discrepancy. Scale isn't sufficient.
%% - Perhaps a stationary GP is not even sufficient

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 6: Discussion
\section{Discussion}
\label{sec:discuss}

Discussion.

\subsection*{Acknowledgments}

RBG and SDB are grateful for funding from NSF CMMI 2152679. This work has been
approved for public release under LA-UR-??-?????. SDB, DO and LJB were funded
by Laboratory Directed Research and Development (LDRD) Project 20220107DR.

\bibliography{ibex_bayes_inv}
\bibliographystyle{jasa}

\appendix

\end{document}
