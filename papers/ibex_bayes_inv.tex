\documentclass[12pt]{article}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{caption}
\usepackage{dcolumn}
\usepackage{filemod}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{makecell}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=7in
\textheight=8.75in
\topmargin=-.5in
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\footskip=0.5in

\begin{document}

\title{Bayesian Statistical Inversion for High-Dimensional Computer Model
Output and Spatially Distributed Counts}
\author{Steven D. Barnett\thanks{Corresponding author
\href{mailto:sdbarnett@vt.edu}{\tt sdbarnett@vt.edu}, Department of
Statistics, Virginia Tech} \and Robert B. Gramacy\thanks{Department of
Statistics, Virginia Tech} \and Lauren J. Beesley\thanks{Statistical Sciences
Group, Los Alamos National Laboratory} \and Dave Osthus\footnotemark[3] \and
Yifan Huang\thanks{ Nuclear and Particle Physics, AstroPhysics and Cosmology,
Los Alamos National Laboratory} \and Fan Guo\footnotemark[4] \and Eric J.
Zirnstein\thanks{ Department of Astrophysical Sciences, Princeton University}
\and Daniel B. Reisenfeld\thanks{Space Science and Applications Group, Los
Alamos National Laboratory}}
\date{}

\maketitle

%\vspace{-0.5cm}

\begin{abstract}
Data collected by the Interstellar Boundary Explorer (IBEX) satellite,
recording heliospheric energetic neutral atoms (ENAs), exhibit a phenomena
that have caused space scientists to revise hypotheses about the physical
processes, and computer simulations under those models, in play at the
boundary of our solar system.  Evaluating the fit of these computer models
involves tuning their parameters to observational data from IBEX. This would
be a classic (Bayesian) inverse problem if not for three challenges: (1) the
computer simulations are slow, limiting the size of campaigns of runs; so (2)
surrogate modeling is essential but outputs are high-resolution images
thwarting conventional methods; and (3) IBEX observations are counts, whereas
most inverse problem techniques assume Gaussian field data. To fill that gap
we propose a novel approach to Bayesian inverse problems coupling a Poisson
response with a sparse Gaussian process surrogate using the Vecchia
approximation.  We demonstrate the capibilities of our proposed framework,
which compare favorably to alternatives, through multiple simulated
examples in terms of recovering ``true'' computer model parameters and accurate
out-of-sample prediction. We then apply this new technology to IBEX satellite
data and associted computer models developed at Los Alamos National Lab.

\bigskip
\noindent \textbf{Key words:} Gaussian process, surrogate modeling,
Poisson, calibration, heliospheric science, IBEX, Vecchia approximation
\end{abstract}

\doublespacing % no double spacing for arXiv

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

%% IBEX
%% - Talk about the satellite
%% - Explain the mission
%% - Introduce the idea of a computer model
%% - Explain desire to understand the distribution of parameters
The National Aeronautics and Space Administration (NASA) launched the
Interstellar Boundary Explorer (IBEX) satellite in 2008 as part of their Small
Explorer program \citep{mccomas2009aIBEX} to deepen our understanding of the
heliosphere. The heliosphere is the bubble formed by the solar wind that
encompasses our solar system and acts as a barrier to interstellar space. Of
particular interest is the behavior at the edge of the heliosphere, known as
the heliopause. Here, highly energized hydrogen ions that make up the solar
wind interact with neutral atoms, occasionally undergo electro exchange, and
become neutral themselves. These energetic neutral atoms (ENAs) are unaffected
by magnetic fields and therefore travel in a straight line.

Some ENAs eventually make their way to Earth and can be detected by an
instrument on the IBEX satellite called the IBEX-Hi ENA imager
\citep{funsten2009IBEXHiENA}. This apparatus records the energy level and
approximate location of origin for each ENA that enters the detector. IBEX's
raw collected data consist of ENA counts and exposure times for each area of
the sky at which the satellite points, providing sufficient information to
estimate the rate at which these particles are created throughout the
heliopause. An example of this estimated surface or image, referred to by
space scientists as a \textit{sky map}, can be found in the left panel of Fig.
\ref{f:fig1}. Sky maps are integral to better understand the heliosphere's
creation and evolution.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.75,trim=0 0 65 0,clip=TRUE]{ibex_real1.pdf}
\includegraphics[scale=0.75,trim=35 0 65 0,clip=TRUE]{sim1.pdf}
\includegraphics[scale=0.75,trim=35 0 20 0,clip=TRUE]{sim2.pdf}
\caption{Observed ENA rate detected by the
IBEX satellite (left) and output from two IBEX  simuations under
different parameter settings (middle, right).
\label{f:fig1}}
\end{figure}

One can think of the heliosphere as a ``boat'' moving through interstellar
space. At the outset of IBEX's mission, scientists expected to see an elevated
rate of ENAs being generated at the front (nose) and back (tail), much like
the turbulent interaction with water at a boat's bow and stern. And indeed,
that belief was validated by IBEX data. These predicted regions of raised ENA
rates are known as \textit{globally distributed flux}, or GDF. At the nose and
tail there are an increased number of collisions between hydrogen ions and
neutral particles in the interstellar medium.

However, in a completely unanticipated finding, IBEX also recorded a thin
string of higher rates of ENAs \citep{fuselier2009IBEXribbon} wrapping around
the heliosphere. Scientists now refer to this phenomenom as the
\textit{ribbon}. This unique feature of a sky map is clearly visible in Fig.
\ref{f:fig1}. Since this discovery, space scientists have proposed several
theories attempting to describe the physical process that generates the ribbon
\citep{mccomas2014ibex, zirnstein2018role, zirnstein2019strong,
zirnstein2021dependence}.

Computer models encapsulating competing theories have been built which
generate high-resolution (i.e, a high-dimensional response) synthetic sky maps
(middle and right panels of Figure \ref{f:fig1}). These simulations are
extremely sophisticated and involve many complex and expensive operations,
such as the solving of partial differential equations. Consequently, executing
a single run of the computer model at a specified set of model parameters can
take thousands of core hours.  This severely limits the number of unique 
runs generating simulated sky in various conditions. 

Our work here focuses on two, publicly accessible computer models: a GDF
model proposed by \citet{zirnstein2021heliosheath}; and a ribbon-only model
developed by \citet{zirnsteinGDFsims2015}. These simulators take inputs that
can be varied to modify the shape and intensity of both ribbon and GDF. The
ribbon-only model relies on two parameters, \textit{parallel mean free path}
and \textit{ratio}, while the GDF model takes \textit{kappa} and
\textit{pickup ions (PUI) cooling index} as inputs. Dialing in good settings
for these input parameters, in light of observed ENA counts from IBEX, is
instrumental in furthering theory development and validation.

Space scientists at Los Alamos National Lab (LANL) wish to solve this
``parameter tuning'' exercise as a Bayesian inverse problem
\citep{kaipio2011bayesian,stuart2010inverse,knapik2011bayesian}, obtaining a
posterior distribution over likely settings.  However, the exposure and count
data IBEX provides, along with computer simulated rates, present some
unique challenges: 1) computationally intensive models limiting simulation,
necessitating surrogate modeling; 2) high-dimensional model output (simulated
sky maps, each containing tens of thousands of pixels), thwarting conventional
surrogate modeling techniques; 3) a non-Gaussian field response (Poisson
counts).  It is worth noting that Bayesian posterior sampling, say via
Markov chain Monte Carlo (MCMC), compounds computational bottlenecks (2).

We propose an integrated framework for Bayesian inverse problems that overcomes all
of these obstacles together, as illustrated in the top row of Table
\ref{tab:prev_work}. First, we introduce a Poisson observational model that correctly
represents the particle counts observed by IBEX. Our framework employs MCMC to
furnish posterior samples of unknown model parameters, which determine the underlying
mean of our Poisson model, given observed data and estimate their full joint
posterior distribution. We fit a GP surrogate or emulator on a limited set of
expensive, high-dimensional IBEX computer simulations using the Vecchia
\citep{katzfuss2021general,scaledvecchiakatzfuss2022} approximation to aid our MCMC
in exploring the space of possible sky maps underlying observed ENA counts. And
lastly, instead of modeling the maps as high-dimensional vectors, we treat each pixel
representing an ENA rate as a scalar response of interest, differing from previous
approaches for functional computer model output and allowing us to leverage recent
advances in sparse GP surrogate modeling.

Methods exist separately, in the literature, to address needs in each of the
aforementioned situations 1)--3), but we are not aware of anything in the
intersection. Table \ref{tab:prev_work} summarizes the features of the IBEX inverse
problem and reviews the necessary capabilities, alongside recent papers offering
partial solutions.  References provided are representative.  We acknowledge that
there are, in most cases, several works that may fit the bill.  Each row in the table
may be summarized as follows.

\begin{table}[ht!]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|}
\hline
& Bayesian & \makecell{Non-\\Gaussian} & \makecell{Large-Scale \\ Simulation \\ Output} & \makecell{High-Dim \\ Response} \\
\hline
Our Contribution & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
\citet{kennedyohagan2001} & \checkmark & & & \\
\hline
\citet{higdoncalib2008} & \checkmark & & & \checkmark \\
\hline
\citet{gramacy2015calibrating} & & & \checkmark & \\
\hline
\citet{grosskopfcountcalib2020} & \checkmark & \checkmark & & \\
\hline
\end{tabular}
\caption{Check marks indicate that functionality exists within the cited paper and/or
accompanying software. To save space, only one citation is listed for each
row.}
\label{tab:prev_work}
\end{table}

\citet{kennedyohagan2001} offer the canonical computer model calibration
framework, which provides a fully Bayesian approach to estimate the posterior
distribution of calibration parameters and improve prediction at new,
unobserved field locations. However, their methodology is limited to Gaussian
field observations and small, simulator output. \citet{higdoncalib2008} and
SEPIA \citep{gattiker2020lanl}, its more recent incarnation, also employ
Bayesian methods while accounting for computer model output that is
functional. But SEPIA is somewhat rigid, does not scale well with added model
runs, and requires the user to specify a number of basis functions to
represent the simulator response. \citet{gramacy2015calibrating} consider
computer model calibration with large-scale output from a computer experiment,
necessitating an approximation. But their implementation is not Bayesian and
the scale of simulator data is still orders of magnitude less than IBEX.
\citet{grosskopfcountcalib2020} provide an extension of the Kennedy \& O'Hagan
(hereafter referred to as KOH) framework to non-Gaussian data that maintains a
Bayesian approach. But they are limited to small amounts of computer model
data and do not consider higher dimensional responses. In contrast, our
approach checks all the boxes to achieve the goal of IBEX space scientists and
therefore applies to a wider class of problems than previous work.

%% OUTLINE
%% - Review Bayesian Inverse Problems, GP Surrogates, SEPIA
%% - Introduce our framework
%% - Explain use of Vecchia approximation
%% - Demonstrate on simulated data
%% - Apply to real IBEX data
Our paper is laid out as follows. First, Section \ref{sec:review} reviews methods
that form the foundation for our work, namely Bayesian Inverse Problems and Gaussian
process surrogate models. We also give a brief description of the methodology and
implementation behind SEPIA, which could be classified as the current ``status quo''
in Bayesian Inverse Problems for high-dimensional computer model output. In Section
\ref{sec:gen_bayes_inv} we introduce our framework for solving the inverse problem in
non-Gaussian contexts. Section \ref{sec:vecchia} describes our use of the Vecchia
approximation to stretch the capacity of a GP surrogate beyond small simulator
training runs. In Section \ref{sec:sims}, we illustrate the effectiveness of our
framework in discovering the true, underlying model parameters on simulated data from
the IBEX computer model. We then return to our motivating application and apply our
methods to the raw IBEX satellite data in Section \ref{sec:ibex_real}. To conclude,
we discuss any extensions and future work in Section \ref{sec:discuss}. Our
implementation may be found at \url{https://github.com/lanl/IBEX_Calibration} along
with code reproducing all included examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 2: Review:
\section{Review}
\label{sec:review}

%------------------------------------------------------------------------------
%% SECTION 2.1 - Bayesian Inverse Problems
\subsection{Bayesian Inverse Problems}
\label{sec:bayes_inv}
%------------------------------------------------------------------------------

%% BAYESIAN INVERSE PROBLEMS
%% - What is an inverse problem and why do we care?
%% - Obtained observations from a physical process to learn more about it
%% - We have some knowledge of the workings of the true process
%% - We can build a mathematical model, and therefore a computer model
%% - Classic IP: evaluate model as much as possible to find parameters/inputs
%%   that produce output aligned with the observations
Inverse problems involve the collection of observations from a physical
experiment in an effort to learn more about the process that generates those
data points. Typically, a scientist or researcher has some knowledge of the
physical or mathematical model (i.e, the data-generating process), but is
seeking to gain a more detailed understanding. The goal is often one of two
cases: 1) estimate the parameters (or distribution of the parameters) that
govern the physical model, or 2) if the researcher lacks some knowledge about
how the process was initiated, but the parameters are known, reconstruct the
inputs from which the observations originated. In this paper, we are focused
on the former, discovering the distribution of unknown parameters. The
motivation for such a method stems from the inability to either control or
precisely measure the factors (e.g. the force of gravity) in a physical
environment. Therefore, in order to acquire more insight, scientists develop
mathematical models that attempt to represent that physical process. Often,
this results in a computer simulation. By comparing observations to simulator
output generated via unique combinations of model parameters, statisticians
attempt to learn the true, underlying parameter values. As mentioned in
Section \ref{sec:intro}, the classical approach is to find the best match
between observed data and extensive computer model via some minimization of
error (e.g. least squares, maximum likelihood).

%% - Observations are noisy
%% - Desire full UQ on parameters
%% - Go Bayesian
%% - Show Prior / Posterior
%% - No analytical form, must resort to MCMC
%% - LEAD IN: Computer model is expensive. Need a surrogate.
Bayes' rule, as put forth in Equation \ref{eq:bsi}, illustrates the Bayesian
approach. First, practioners embed a certain amount of subject matter
knowledge into the analysis by imposing a prior $\pi(\boldsymbol \theta)$ on
model parameters. Second, Bayesians assume that the model parameters
themselves are random variables, and therefore obtain their full joint
posterior distribution, instead of basic point estimates and confidence
intervals. In Equation \ref{eq:bsi}, $\boldsymbol
\theta$ represents the parameters of a mathematical model that we wish to
infer, $Y$ is our vector of physical observations, and $X$ is the set of input
locations. Here we assume that the field data is Gaussian, as indicated by the
$\mathcal{N}$ subscript on the likelihood.
\begin{align}
\pi(\boldsymbol \theta | Y, X) = \frac{\mathcal{L}_\mathcal{N}(Y|\boldsymbol
\theta, X)
\pi(\boldsymbol \theta)}{\pi(Y|X)} \propto \mathcal{L}_\mathcal{N}(Y|\boldsymbol \theta,
X) \pi(\boldsymbol \theta)
\label{eq:bsi}
\end{align}
If $\pi(\boldsymbol \theta)$ is conjugate, inference for $\boldsymbol \theta$
is simple and can be done analytically. However, this is often not the case,
requiring an approximation of the posterior (i.e.~Laplace, Variational Bayes).
Our framework utilizes Markov chain Monte Carlo (MCMC) methods to sample from
the posterior.

Before we go further, let's start with a very simple example. Consider the
left panel of Figure \ref{f:toy_calib}. Red stars denote field observations of
some true, unknown process at different locations $X$. Note that these data
are observed with some noise. Additionally, multiple observations are made at
each location. This is common, due to the expense of changing configurations
of a physical experiment, but it need not be the case. Light gray lines denote
individual runs of a computer simulation, at different settings of parameters
$\boldsymbol{\theta}$. Finally, the black, bold, dashed line shows the true,
underlying process (e.g.~mean) we want to estimate. Modeling the data as
\citet{kennedyohagan2001} did, we can make field predictions out-of-sample and
estimate the posterior distributions of $\boldsymbol{\theta}$. Predictions and
posterior estimates are shown in the middle and right panels of Figure
\ref{f:toy_calib}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.51,trim=0 0 25 0,clip=TRUE]{logit1_obs.pdf}
\includegraphics[scale=0.51,trim=30 0 25 0,clip=TRUE]{logit1_est.pdf}
\includegraphics[scale=0.51,trim=30 0 0 0,clip=TRUE]{logit1_post_draws.pdf}
\caption{A simple inverse problem. Field observations and sample of computer
model runs (left), model evaluations at posterior draws and mean (middle), and
posterior distribution of model parameters (right).
\label{f:toy_calib}}
\end{figure}

%% DO A REVIEW OF OTHER METHODS (KOH? BAYES INV? STUART? TECKENTRUP? COLLINS?)

One difficulty in our situation is the expense of the computer model. As a
result of expanded access to supercomputing resources, computer models have
increased in complexity over the past two decades and are able to produce
extremely high-fidelity output. However, even with modern-day computing power,
some simulations can take hours, days, or weeks to complete. This limits the
amount of runs that can be executed and therefore restricts the exploration of
the model parameter space. Therefore, we need a model (what we call a
surrogate, or emulator) fit to the simulator output to make accurate and
reliable predictions for out-of-sample values of our computer model
parameters. Any statistical model can serve as a surrogate, ranging from
linear regression to complex neural networks. However, the canonical surrogate
for computer experiments and computer model calibration has come to be the
Gaussian process \citep{gramacy2020surrogates}.

%------------------------------------------------------------------------------
%% SECTION 2.2 - Gaussian Process Surrogates:
\subsection{Gaussian Process Surrogates}
\label{sec:gp_surr}
%------------------------------------------------------------------------------

%% GAUSSIAN PROCESS SURROGATES
%% - What is the motivation? Expensive computer experiments
%% - Set of n observations that follow a MVN
%% - Often mean-zero. In practice this means subtracting off the mean
%% - Results in all the action being in the covariance function
%% - Covariance depends on pairwise distance
%% - Explain some different kernels
%% - Talk about prediction. Show Kriging equations
%% - Nonparametric, flexible regression tool
%% - A lot of different applications
%% - Become popular in computer experiments because of interpolation
%% - This is what we want
%% - Bottleneck is O(n^3)
%% - LEAD IN: Output can be large. Need an approximation.
%% Review Gaussian processes and their use in surrogate modeling
A Gaussian process is a realization of a multivariate normal distribution,
governed by some mean and covariance functions. GPs are fit to observed data
and used to make predictions at new input locations, relying on the
conditional multivariate normal or \textit{kriging} equations
\citep{matheron1963principles}. For example, suppose we have a vector of
observations $Y$ at input locations $X$. $Y$ is an $n \times 1$ vector and $X$
is an $n \times p$ matrix. We make the assumption that $Y$ is normally
distributed with some mean $\mu(X)$ and covariance $\Sigma(X)$, both functions
of $X$. Then, predicting at a new location $x$ is relatively straightforward.
Eq.~(\ref{eq:mvn_eq}) articulates how to obtain mean predictions $\mu^*(x)$
and corresponding uncertainty quantification estimates $\Sigma^*(x)$.
\vspace{-0.75cm}

\begin{minipage}[t]{0.25\linewidth}
\begin{align}
Y(X) &\sim \mathcal{N}\!\left(\mu(X), \Sigma(X)\right) \nonumber \\
y^*(x) \mid Y(X) &\sim \mathcal{N}\!\left(\mu^*(x), \Sigma^*(x)\right) \nonumber
\end{align}
\end{minipage}
\quad
\begin{minipage}[t]{0.58\linewidth}
\begin{align}
\mu^*(x) &= \mu(x) + \Sigma(x, X) \Sigma(X)^{-1} (Y - \mu(X)) \nonumber \\
\Sigma^*(x) &= \Sigma(x, x)
  - \Sigma(x, X) \Sigma(X)^{-1} \Sigma(X, x) \hspace{0.05em}
\label{eq:mvn_eq}
\end{align}
\end{minipage}
\vspace{1cm}

Although a generic mean function is specified in Eq.~\ref{eq:mvn_eq}, it is
common to set $\mu(X)=0$. In practice, this simply implies centering $Y$ by
subtracting off the sample mean, $\overline{Y}$. In this zero-mean context,
all the action takes place in the covariance function. The covariance function
typically depends on pairwise distances between input locations $x$ and $x'$.
Many covariance kernels exists, such as the Gaussian, Exponential, Matern. We
use the Matern kernel in our work, of which both the Gaussian and Exponential
kernels are special cases. The Matern kernel is defined as
\begin{align}
\Sigma(x, x'|\sigma^2, \theta, \nu, g) &= \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left(\sqrt{2\nu} \frac{\|x-x'\|}{\theta}\right)^2 K_{\nu} \left(\sqrt{2 \nu}
\frac{\|x - x'\|}{\theta}\right) + g \cdot \mathbb{I} \nonumber \\
&= \sigma^2 \left(1 + \frac{\sqrt{5} \cdot \|x - x'\|}{\theta} +
\frac{5\cdot\|x-x'\|^2}{3\theta^2} \right) \text{exp}\left(-\frac{\sqrt{5} \cdot \|x
- x'\|}{\theta}\right) + g \delta_{x, x'},
\label{eq:matern}
\end{align}
where $K_{\nu}$ is the modified Bessel function of the second kind. The
behavior of these kernels is determined by three hyperparameters: a nugget
$g$, lengthscale $\theta$, and scale $\tau^2$. In the Matern kernel, an
additional parameter ($\nu$) exists, which controls the smoothness of the
estimated surface. But we will not estimate the smoothness parameter here and
instead set $\nu =
\frac{5}{2}$ (Eq.~\ref{eq:matern}), following common practice due to it
allowing the kernel function to be expressed in a simpler form. All
hyperparameters can be estimated using your preferred method (i.e. maximum
likelihood, method of moments). We will use Bayesian methods, specifically
MCMC.

GPs provide a flexible, nonparametric regression tool. In addition to computer
experiments, they are utilized in geostatistics
\citep{matheron1963principles}, spatial statistics
\citep{cressie2011statistics}, time series \citep{roberts2013gaussian},
optimization \citep{jones1998efficient}, and machine learning
\citep{rasmussen2003gaussian}. The reason for their popularity as surrogate
models in the world of computer experiments is their ability to interpolate
between observed data points (i.e. setting nugget $g = 0$). Although
stochastic computer models are becoming more common, computer experiments are
often deterministic, motivating the desire for a model that interpolates. GPs
are then useful by providing predictive surfaces that give higher estimates of
variance away from observed data, and virtually no variance at observed
locations. In fact, perhaps the hallmark of GPs is that they naturally provide
excellent uncertainty quantification for out-of-sample predictions, a matter
of importance to scientists exploring these computer models. Therefore, GPs
are widely used in computer experiments and Bayesian Inverse Problems, as
illustrated next.

%------------------------------------------------------------------------------
%% SECTION 2.3 - SEPIA / GPMSA
\subsection{SEPIA / GPMSA}
\label{sec:sepia}
%------------------------------------------------------------------------------

%% SEPIA / GPMSA
%% - O(n^3) bottleneck is nothing new
%% - Higdon et al. faced this at LANL in the early 2000s
%% - Their situation is unique in that n was small, but dimension is high
%% - Same situation as IBEX
%% - Instead of modeling the output as scalar, they did functional
%% - PCA
%%   - Show some equations
%%   - Maybe a graphic
%% - Must choose basis
%% - Used in calibration (an inverse problem), but does not support Poisson
In the early 2000s, \citet{higdoncalib2008} faced the challenge of estimating
the parameters of a physical process from which they could gather only a small
set of observations, but that could be represented by a simulation. But these
expensive computer models at Los Alamos National Laboratory (LANL) produced
limited runs with extremely high-dimensional output in the form of images,
shape description, time series, etc. So in order to efficiently fit a
surrogate model to the simulator, they needed to perform an approximation to
limit the size of the data. As a solution, they employed principal components
analysis (PCA) to reduce the dimensionality of the computer model response.
Specifically, they represented the vector output of a simulator model as the
sum of $n_k$ basis functions $\textbf{k}_1, \textbf{k}_2, \ldots ,
\textbf{k}_{n_k}$, each scaled by a corresponding weight $w_i$, as shown in
\begin{align}
\boldsymbol y(\boldsymbol x, \boldsymbol u) &\approx \sum_{i=1}^{n_k} \boldsymbol k_i \cdot w_i (\boldsymbol x, \boldsymbol u), \quad n_k \ll n \ \nonumber \\
w_i (\boldsymbol x, \boldsymbol u) &\sim GP(\boldsymbol 0, \Sigma_{w_i}),
\label{eq:higdon_pca}
\end{align}
where $\boldsymbol x$ is a vector of controllable inputs and $\boldsymbol u$
is the vector of model parameters we are interested in estimating.
$\Sigma_{w_i}$, dependent only on a limited number of combinations of
$\boldsymbol x$ and $\boldsymbol u$, is a $n_k \times n_k$ matrix. Hence, the
decomposition of $\Sigma_{w_i}$ avoids the $O(n^3)$ bottleneck in computation
and can be completed in a reasonable amount of time. Instead of modeling the
high-dimensional response $\boldsymbol y$, the GP only modeled the small set
of component weights. Bayesian inference was used to obtain the posterior
distribution of these component weights. Combining estimates of the weights
with the calculated basis functions allowed for predictions out of sample. As
a result of this approximation, the dimension of the covariance matrices
$\Sigma(X)$ and $\Sigma(X^*)$ in Eq.~\ref{eq:mvn_eq} remained small and able
to be decomposed quickly.

We propose a simpler approach in Section \ref{sec:vecchia}, devoid of the
definition and selection of how many basis functions to use. Although this
basis representation was and still is successful in many applications, we
argue that what \citet{higdoncalib2008} really wanted to do was represent the
functional output as a univariate response. That is, replicate the entry in
the input matrix $X$ for each response the number of times equal to the length
of the output vector. In essence, implicitly encode the index (e.g.~pixel in
image, moment in time series) into the response vector for each observation in
the input matrix. We believe that had \citet{higdoncalib2008} had access to
the capacity of modern-day computing, along with the novel methodology
proposed since the early 2000s that leverages that compute, they would have
steered their implementation in a significantly different way. We will show
that our approach results in equal or improved or in both predictive accuracy
and uncertainty quantification, along with a significant reduction in
computational expense.

%% Limitations:
%% Poisson response. Clunky way to deal with large data. Not generalizable.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 3: Methods
\section{Methods}
\label{sec:methods}

%------------------------------------------------------------------------------
%% SECTION 3.1 - Generalized Bayesian Inverse Problems with GP Surrogates
\subsection{Generalized Bayesian Inverse Problems with GP Surrogates}
\label{sec:gen_bayes_inv}
%------------------------------------------------------------------------------


%% Talk about computer simulations with non-Gaussian response
%% Introduce equation
%% ?? Maybe specify priors here??
%% Talk about need for MCMC <- Algorithm 1 <- POLISH
%%%% Here mention that we are using modular Bayes
%% Go back to 1D toy problem, but with GP surrogate and Poisson data

%% GENERALIZED BAYESIAN INVERSE PROBLEMS WITH GP SURROGATES
%% - Return to review, but with general response
Computer simulations are not limited to modeling physical processes that
generate a continous response. For instance, a field experiment may return a
binary response, such as success or failure. Categorical responses may expand
beyond two classes, representing a multinomial reponse. In this paper we focus
on Poisson-distributed counts due to our motivating application, but our
framework can easily be generalized to any response. Revisiting Eq.
\ref{eq:bsi}, we replace the Normal likelihood with a Poisson likelihood, and
substitute $\boldsymbol \lambda$ for $\boldsymbol \theta$, as shown below.
\begin{align} %% IDEA: specify priors we are using here?
\pi(\boldsymbol \lambda | \boldsymbol y, X) &= \frac{\mathcal{L}(\boldsymbol y|\boldsymbol \lambda, X)
\pi(\boldsymbol \lambda)}{\pi(\boldsymbol y|X)} \propto \mathcal{L}(\boldsymbol y|\boldsymbol \lambda,
X) \pi(\boldsymbol \lambda) \nonumber \\ &\propto \left[ \prod_{i=1}^N
   \frac{\lambda_i^{y_i}e^{-\lambda_i}}{y_i!} \right] \pi(\boldsymbol \lambda) =
   \left[ \prod_{i=1}^N \frac{(r_i t_i)^{y_i}e^{-r_i t_i}}{y_i!} \right]
   \pi(\boldsymbol
   \lambda)
\label{eq:bsi_pois}
\end{align}
Note that $\lambda_i$, the mean function at a certain input location $x_i$, is
represented as the product of an underlying rate, $r_i$, and an exposure time,
$t_i$. For unit exposure ($t_i=1$), the mean can simply be represented as
$\lambda_i$. However, in our application we are not given unit exposure, but
each data point is a total count over a specific time interval.

Eq.~\ref{eq:bsi_pois} does not tell the entire story, as we (or the scientists
we collaborate with) are not necessarily interested in the posterior
distribution of $\boldsymbol \lambda$. The parameters of interest are those that determine what the value of $\boldsymbol \lambda$ is, not $\boldsymbol \lambda$ itself. So, we rewrite Eq. \ref{eq:bsi_pois} as
\begin{align} %% IDEA: specify priors we are using here?
\pi(\boldsymbol \theta | \boldsymbol y, X) &= \frac{\mathcal{L}(\boldsymbol y|\boldsymbol \theta, X)
\pi(\boldsymbol \theta)}{\pi(\boldsymbol y|X)} \propto \mathcal{L}(\boldsymbol y|\boldsymbol \theta,
X) \pi(\boldsymbol \theta) \nonumber \\ &\propto \left[ \prod_{i=1}^N
   \frac{f_{\lambda}(\theta_i)^{y_i}e^{-f_{\lambda}(\theta_i)}}{y_i!} \right] \pi(\boldsymbol \theta) =
   \left[ \prod_{i=1}^N \frac{(f_r(\theta_i) t_i)^{y_i}e^{-f_r(\theta_i) t_i}}{y_i!} \right]
   \pi(\boldsymbol
   \theta),
\label{eq:bsi_pois2}
\end{align}
where $f()$ is simulator, which gives us the form of the posterior for the
computer model parameters. But without a conjugate prior to provide an
analytical solution, and the infeasibility of marginalizing out $\boldsymbol
\theta$, we must sample the posterior of $\boldsymbol \theta$ using MCMC.
The difficulty is in the second step of the for loop in Algorithm
\ref{alg:gibbs}. Here we need to predict $\boldsymbol
\lambda$ at input locations $X$, but with proposed model parameters, $\boldsymbol \theta^*$.
Unfortunately, that falls to our computer model, which is expensive and can
take hours or days to complete, making MCMC impractical. Therefore, we need a
surrogate for the simulator that 1) can be fit to training data in a
reasonable time frame, and 2) can provide good predictions at new values of
the calibration parameters quickly. Here we utilize a GP surrogate
(\ref{sec:gp_surr}).
\begin{algorithm}[ht]
\DontPrintSemicolon
Fit Scaled Vecchia GP surrogate $\hat{f}_r()$ to simulator output $Y_m, X_m$. \\ 
Initialize $\boldsymbol \theta^{(0)}$, $\Sigma_{\theta}^{(0)}$. \\
\For{$t = 1, \dots, T$}{
  Propose new model parameters $\boldsymbol \theta^{(t)} \mid \boldsymbol \theta^{(t-1)}$ via MH, \\
  Make predictions $\boldsymbol \lambda^{(t)} = \hat{f}_r(\boldsymbol \theta^{(t)}, X)$, \\
  Evaluate $\mathcal{L}(Y|\boldsymbol \lambda^{(t)}, T)$ where $T$ is the vector of exposure times paired with $Y$, \\
  Accept or reject $\boldsymbol \theta^{(t)}$
  }
\caption{MCMC (Metropolis-within-Gibbs) for Poisson Inverse Bayes.}
\label{alg:gibbs}
\end{algorithm}

%% - 1D example with Poisson response
%%   - note that we are treating the response as a scalar
%%   - response is Poisson
%%   - Graphic showing field / computer model data, fit, and posterior over parameters
Now, before we get ahead of ourselves, letâ€™s build on the toy problem we
introduced in \ref{sec:bayes_inv}. Instead of observing the function $f(X)$
with random, Gaussian noise, suppose we are given counts, drawn from a Poisson
process with $\boldsymbol \lambda = f(X)$, as shown in the left panel of
Figure 3. As described in \ref{sec:bayes_inv}, we know that $f(X)$ is a
function of two parameters, $\mu$ and $\lambda$. We'll skip the idealized
situation where we have access to unlimited evaluations of the computer model
and instead, fit a GP surrogate to a limited run of model evaluations we can
use to make a surrogate prediction at each MCMC iteration. Results are shown
in the middle panel of Figure \ref{f:toy_calib_pois}. We can see that the MCMC
thoroughly explores the model input space and that our framework picks
parameter values that again estimate the simulator output almost exactly. The
posterior mean, shown in the right panel, falls close to the true values of
the parameters, this time as the underlying mean function generating counts
$\boldsymbol y
\sim \text{Poisson}(\boldsymbol \lambda)$. We recognize that there is more distance
between the truth and our estimate than before, but we attribute that extra
uncertainty to using a surrogate rather than the computer model itself.%% IDEA: fit a surrogate to the data itself. maybe use GPVecchia
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.51,trim=0 0 25 0,clip=TRUE]{logit1_pois_obs.pdf}
\includegraphics[scale=0.51,trim=30 0 25 0,clip=TRUE]{logit1_pois_est.pdf}
\includegraphics[scale=0.51,trim=30 0 0 0,clip=TRUE]{logit1_pois_post_draws.pdf}
\caption{An inverse problem for the underlying mean of a Poisson process.
Counts observed in the field and a sample of computer model runs (left), model
evaluations of the underlying mean at posterior draws and the mean estimate
(middle), and posterior distribution of model parameters (right).
\label{f:toy_calib_pois}}
\end{figure}

Note that we adopt the modularized approach of
\citet{Liu2009ModularizationIB}, by both necessity and preference. That is,
we fit our surrogate model to simulator data only, not incorporating field
data. First, \citet{kennedyohagan2001} only apply their method to situations
when both Gaussian computer model output and Gaussian field responses. This
allows joint modelling of simulator and field data. We are unable to do this
with Poisson counts as the field response. Additionally, although we do not
address a discrepancy between simulator and reality as
\citet{kennedyohagan2001} do, which better estimation of is a key part of the
motivation \citet{Liu2009ModularizationIB} cite to their modularized
approach, we share the belief that predictions for our surrogate for the
computer model should only be informed by the simulator output itself.

%------------------------------------------------------------------------------
%% SECTION 3.1.1 - A Larger Problem
\subsubsection{A Larger Problem}
\label{sec:gen_bayes_inv_large}
%------------------------------------------------------------------------------

%% Align with IBEX example <- increase dimension of response
%% Explain the need for more training data
%% Explain the difference between functional and scalar
%% Hold that thought on SUPER large data
%% Show panel of responses and simulator output
%% Show panel of results

%% IDEA: consider something similar to our ibex example
%% EXAMPLE
%% - 2D example with Poisson response
%%   - Same Thing as above
%%   - LEAD IN: capacity of GP is stretched
Now, let's shift gears and jump ahead to our motivating example that includes a
higher dimensional response. Although the problem is harder to visualize with
a higher dimensional response, even in 2D, our framework and approach is
identical.  But the more pressing barrier to overcome is the need for more runs
of the simulation in order to obtain good predictions from a surrogate due to
the increased dimensionality. In the example above, the number of computer
model evaluations was limited ($n < 100$). As mentioned before, computational
limits ensure that this is typically the case in surrogate modeling of
computer simulations. But note that the output of our computer model is
functional, i.e. each response is a vector. Above, each simulation was
evaluated at 20 separate points. Therefore, although the number of runs is
small, we are really modeling 800 ($40
\times 20$) responses. A training set of 800 data points is already stretching
the limitations of a GP (decomposing a $800 \times 800$ matrix). But consider
the simulated \textit{sky maps} in the middle and right panels of Figure
\ref{f:fig1}. We leave a complete introduction of the IBEX simulator to
\ref{sec:helio_comp_model}, but it's clear the output is high resolution. In
fact, the two degree grid over latitude and longitude (under the hood the
output is in spherical coordinates $x$, $y$, and $z$) results in $16,200$
unique locations, or pixels. Therefore, even modeling one image results in a
$16,200 \times 16,200$ grid. Combine that with $n=66$ unique pairs of model
parameters and our response vector will have greater than one million
elements. Even if we used a smaller LHS space-filling design of size $n=15$
over the model parameters, the response vector would still consist of over
200,000 points. In that case, a GP could not feasibly model each individual
point as a separate response. We reviewed what has been done historically or
high dimensional output in an inverse problem setting in Section
\ref{sec:sepia}. Next, we will propose a modern, intuitive, more
generalizable, and better-perfoming approach.% \begin{figure}[ht!]
% \centering
% \includegraphics[scale=0.5, trim=30 15 40 0,clip=TRUE]{logit2_obs.pdf}
% \includegraphics[scale=0.5, trim=30 15 95 0,clip=TRUE]{logit2_model1.pdf}
% \includegraphics[scale=0.5, trim=75 15 40 0,clip=TRUE]{logit2_model2.pdf}
% \caption{Poisson draws from a mean process over $x_1$ and $x_2$ (left).
% Evaluations of a computer model at different settings of four parameters
% ($\mu_1$, $\nu_1$, $\mu_2$, $\nu_2$) attempting to represent the true mean
% process (middle, right).
% \label{f:logit2_examp}}
% \end{figure}

%------------------------------------------------------------------------------
%% SECTION 3.2 - Vecchia Approximation
\subsection{Vecchia Approximation}
\label{sec:vecchia}
%------------------------------------------------------------------------------

%% VECCHIA APPROXIMATION
%% - Review improvement in computation
%% - Higdon may have modeled it this way if he had the resources
%% - List GP approximations from past decade
%% - We use Vecchia
%% - Show formulas
%% - Specify that we use Scaled Vecchia approximation, what it is
Perhaps the primary weakness of GPs is the need to decompose an $n \times n$
covariance matrix, making it difficult to scale to large datasets,
increasingly common in modern-day applications. Many methods have been
introduced to address this computational burden. These include, but are not
limited to, fixed-rank kriging \citep{cressie2008fixed}, inducing points
\citep{banerjee2008gaussian}, compactly supported kernels
\citep{kaufman2011efficient}, divide-and-conquer \citep{gramacy2015local}, and
nearest neighbors \citep{datta2016hierarchical, wu2022variational}. In the
context of computer model calibration, \citet{higdoncalib2008} use a basis
representation via principal components to accommodate high-dimensional
output (\ref{sec:sepia}).

As indicated in Section \ref{sec:intro}, we aim to modernize this approach. We
believe treating the data as a functional or multivariate response, then
reducing the dimension using PCA, and fitting a GP on component weights isn't
straightforward or natural. We suggest shifting the dimensions of the response
($x_1, x_2, \ldots$) into the input matrix $X$ for each entry in the response
vector. This clearly inflates the size (e.g. number of rows) of the training
data. But with the dramatic increase in computing capacity and availability
over the past decade and the aforementioned advances in methodology, we
believe there are GP approximations available to model the output of a
computer simulation in this manner. We propose using the recently popularized
Vecchia approximation \citep{vecchia1988estimation, katzfuss2020vecchia,
katzfuss2021general, zhang2022multi, sauer2023vecchia}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5,trim=5 38 65 0,clip=TRUE]{ibex_sim_pmfp_1500_0.005.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1625_0.005.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1750_0.005.pdf}
\includegraphics[scale=0.5,trim=5 38 65 0,clip=TRUE]{ibex_sim_pmfp_1500_0.0075.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1625_0.0075.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1750_0.0075.pdf}
\includegraphics[scale=0.5,trim=5 0 65 0,clip=TRUE]{ibex_sim_pmfp_1500_0.01.pdf}
\includegraphics[scale=0.5,trim=35 0 65 0,clip=TRUE]{ibex_sim_pmfp_1625_0.01.pdf}
\includegraphics[scale=0.5,trim=35 0 65 0,clip=TRUE]{ibex_sim_pmfp_1750_0.01.pdf}
\caption{Simulated and predicted outputs from the IBEX \textit{sky map}
computer model. Corner panels represent the response for four unique
combinations of model inputs. Panels on the cross with bold, dashed borders
are predictions from a GP surrogate fit to $n=66$ runs of the computer model.
Model parameter settings for the predictive panels fall between the four
simulated runs shown.
\label{f:ibex_sim_surr}}
\end{figure}

The Vecchia approximation relies on the ability to write a multivariate normal
likelihood function as the product of univariate normal conditional
likelihoods as shown below.
\begin{align}
L(Y)&=\prod_{i=1}^n L(Y_i \mid Y_{g(i)})
\hspace{0.3em} \mbox{ where } g(1)=\emptyset \hspace{0.3em} \mbox{ and } g(i)=\{1, 2,
\ldots, i-1\} \nonumber \\
&\approx \prod_{i=1}^{n}L(Y_i \mid Y_{h(i)}) \hspace{0.3em} \mbox{ where } h(i)
\subset \{1, 2, \ldots, i-1\} \hspace{0.3em} \mbox{ and } |h(i)| = m \ll n
\label{eq:vecchia}
\end{align}
Reducing the size of the conditioning set $h(i)$ induces sparsity in the
precision matrix, potentially dramatically speeding up the evaluation of the
likelihood. This takes an operation of order $O(n^3)$ down to $O(nm^3)$,
motivating small values of $m$. Logically, as the size of $h(i)$ decreases,
the evaluation of the likelihood becomes faster, but the quality of the
approximation is reduced. On the flip side, as one increases $m$, execution
time grows, but the quality of the approximation improves. This continues
until $n=m-1$, when the likelihood becomes no longer an approximation. We find
a value of $m < 30$ to be more than sufficient, but the choice can be impacted
by the nature of the data and one's computational budget. The conditioning set
can be constructed in many ways, first by how the data itself is ordered, and
second by how neighbors are selected. Many different orderings have been
proposed \citep{guinness2018permutation}, such as a maxi-min distance
strategy. We default to a random ordering of the training data and
nearest-neighbors approach for selecting the conditioning set, which has been
done previously \citep{sauer2023active} with successful results.

%------------------------------------------------------------------------------
%% SECTION 3.2.1 - Scaled Vecchia Approximation
\subsubsection{Scaled Vecchia Approximation}
\label{sec:vecchia_ex}
%------------------------------------------------------------------------------

Several implementations of the Vecchia approximation exist, but we employ one
that has risen to the top in terms of predictive performance and computational
thriftiness in the context of computer experiments, that is, the Scaled
Vecchia approximation introduced by \citet{scaledvecchiakatzfuss2022}. We
refer you to their paper for a more in-depth explanation of their work. But,
the main crux of this implementation is the pre-scaling of inputs by dimension
before the creation of conditioning sets based on nearest neighbors.
\citet{scaledvecchiakatzfuss2022} point out that dimensions affect the
response differently and at varying rates and therefore predictions should be
made based on neighborhoods that account for this heterogeneity in each
dimension's impact. Scaling the data beforehand addresses this. Publicly
available code for this implementation that we directly rely on is readily
accessible at \url{https://github.com/katzfuss-group/scaledVecchia}.

%% EXAMPLES
%% - 2D example with LOTs of data
%%   - Show results. Compare to previous
%% - Show timing test on simulated data
%%   - Graphic should include fitting and prediction
%%   - Include different number of bases for SEPIA, maybe compared with different m
%%   - Show performance of different models
Let's return to the example we kicked down the road in Section
\ref{sec:gen_bayes_inv_large} and see how the Scaled Vecchia approximation
performs. We have access to 66 model runs, each producing an image of 16,200
points. Figure \ref{f:ibex_sim_surr} shows four of the actual simulation
outputs (corner panels displayed with thin, solid borders). A Scaled Vecchia
GP surrogate was fit to all 66 simulated runs. Then, we predicted the output
for five unobserved combinations of the model parameters that fall in between
the actual runs shown. These are depicted in the cross of Figure
\ref{f:ibex_sim_surr}, those panels with thick, dashed borders. To the naked
eye, these predictive surfaces appear to have been generated by the simulator
itself. Moving left to right, or top to bottom, the surfaces change gradually
and as one would expect. But we are not satisfied with simply passing the eye
test, so we also use empirical results to show that our surrogate fit is
accurate and outperforms other current methods.

%------------------------------------------------------------------------------
%% SECTION 3.2.2 - Empirical Results
\subsubsection{Empirical Results}
\label{sec:surr_empir}{}
%------------------------------------------------------------------------------

In order to validate performance of our surrogate model, we run conduct the
following hold-one-out experiment. For each iteration of this bakeoff, we fit
our Scaled Vecchia approximated GP surrogate on 65 of the the $n=66$ model
runs available. Then, we predict at the held out combination of model
parameters, but on the same input grid as the 65 other runs. We then evaluate
how well the model did using root mean square error (RMSE) and the continuous
ranked probability score (CRPS) proposed by \citet{gneiting2007strictly}. For
the purpose of comparison, we execute the same test for three other
competitors: {\tt R} packages {\tt laGP} \citep{gramacy2014lagp} and {\tt
deepgp} \citep{deepGP}, and the current ``status-quo,'' SEPIA
\citep{gattiker2020lanl}, implemented in {\tt Python}. Figure
\ref{f:ibex_surr_metrics} displays the results of this bakeoff.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.45, trim=0 0 0 0,clip=TRUE]{ibex_surr_rmse.pdf}
\includegraphics[scale=0.45, trim=0 0 0 0,clip=TRUE]{ibex_surr_crps.pdf}
\caption{Metrics on the IBEX simulator experiment. Smaller is better for both
RMSE and CRPS.
\label{f:ibex_surr_metrics}}
\end{figure}

For now, just pay attention to left-hand side of each of the panels, where
$pcs=3$ for the SEPIA boxplots and $m=25$ for the Scaled Vecchia boxplots.
We'll come back to the others in a moment. It's clear that the Scaled Vecchia
approximation outperforms {\tt laGP} and {\tt deepgp} in both RMSE and CRPS.
We gave {\tt laGP} a neighborhood size of 50, but the predictions are made
based only on observations local to the predictive location, perhaps
preventing understanding of global trends in the surface. {\tt deepgp} is
fully Bayesian, utilizing MCMC to sample from the posterior distribution. But
that comes with a cost. Because of the significant computational burden, we
were only able to run {\tt deepgp} for 1000 MCMC iterations, burning in 500
and thinning by 10. Even these limitation took a considerable amount of time,
and likely contributed to it's poor performance. It is also important to
mention that because of the large dataset size, we had to employ the built-in
Vecchia approximation for the {\tt deepgp} fits with a conditioning set size
of $m=10$. SEPIA performed similarly to our Scaled Vecchia approximation,
illustrating why it has had such staying power over the past two decades.
Therefore, as a practitioner, it appears to be a toss-up between these two
methods for which to use. But that's not the entire story.

In addition to assessing accuracy and uncertainty quantification through RMSE
and CRPS, we aimed to evaluate the speed of execution for all competitors
involved. In our situation, two factors exist that contribute to a
computational bottleneck. First, the dimension of the simulator response,
which has been the main focus of our work. Additionally, the number of runs
can be varied, based on the expense of a simulation run. To evaluate this, we
conducted two more experiments. For varying dimension, we modified the
dimensionality of the response from 200 to 20,000, keeping the number of runs
constant. For the two methods that are most equipped for large dimensions
(Scaled Vecchia and SEPIA), we attempt to stretch their capacity by cranking
up the length of the response vector from 20-100K, by 10K. Results are
displayed in Figure \ref{f:ibex_surr_timing_dim}.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.57, trim=0 0 0 0, clip=TRUE]{ibex_surr_fit_times.pdf}
\includegraphics[scale=0.57, trim=0 0 0 0, clip=TRUE]{ibex_surr_pred_times.pdf}
\includegraphics[scale=0.57, trim=0 0 0 0, clip=TRUE]{ibex_surr_total_times.pdf}
\caption{Timing metrics on the IBEX simulator experiment.
\label{f:ibex_surr_timing_dim}}
\end{figure}

Figure \ref{f:ibex_surr_timing_dim} makes it apparent that Scaled Vecchia and
SEPIA are both well-suited for increasing dimensionality in the response. {\tt
laGP}, while built for prediction with large-scale datasets, is unable to keep
pace at a certain point, likely due to the need to fit a mounting number of
local GPs. Reducing the neighborhood size (set to the default of 50 in our
experiment) for {\tt laGP} could certainly aid in the computation expense, but
that would further degrade it's performance, which already lags being Scaled
Vecchia and SEPIA (Figure \ref{f:ibex_surr_metrics}). {\tt deepgp}'s need to
decompose an $n \times n$ matrix at each MCMC iteration, even aided by the
Vecchia approximation, causes it's cost to quickly grows as dimensionality
expands. Comparing Scaled Vecchia and SEPIA, we see that Scaled Vecchia comes
out on top, even as the response dimension is pushed to an even greater extent
in the right panel of Figure \ref{f:ibex_surr_timing_dim}. Therefore, although
both Scaled Vecchia and SEPIA perform similarly in predictive performance,
Scaled Vecchia saves on compute time. We'll see that illustrated again in our
next experiment.

%% NEXT EXPERIMENT
Next, we consider modifying the size of the training dataset. SEPIA was
specifically built for computer experiments with high-dimensional output.
While it performs well in these situations, it's also fairly inflexible to
other contexts, such as a large number of simulation runs. Scaled Vecchia does
not face this hurdle, as it treats both high-dimensional output and
large-scale univariate datasets similarly, as a univariate response. We show
this invariance in our next bakeoff. We keep the dimensionality of the
response at 10K, but slide the number of simulator runs from 10 to 100. For
the higher performing methods, we push further up to $n=200$. Results are
shown in Figure \ref{f:ibex_surr_timing_size}.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.57, trim=0 0 0 0, clip=TRUE]{ibex_surr_fit_times_ns.pdf}
\includegraphics[scale=0.57, trim=0 0 0 0, clip=TRUE]{ibex_surr_pred_times_ns.pdf}
\includegraphics[scale=0.57, trim=0 0 0 0, clip=TRUE]{ibex_surr_total_times_ns.pdf}
\caption{Timing metrics on the IBEX simulator experiment.
\label{f:ibex_surr_timing_size}}
\end{figure}

Waiting on results. Insert interpretation here.

% Field observations are made at 40 unique locations via an LHS design
% over $x_1$ and $x_2$. At each location, exposure time (in this simulated
% example, this really means taking a certain number of draws from a Poisson and
% summing the counts together), varies between one and ten. An LHS design of
% size $n=100$ over the four computer model parameters is also made, resulting
% in 40,000 unique responses. We run Algorithm \ref{alg:gibbs} by fitting a GP
% surrogate to the simulator output and running MCMC to sample from the
% posterior distribution of the model parameters. Results are shown in Figure
% \ref{f:logit2_results}.

% \begin{figure}[ht!]
% \centering
% \includegraphics[scale=0.54,trim=35 0 0 0,clip=TRUE]{logit2_stand_in.pdf}
% \caption{Observed rate of energetic neutral atoms (ENA) detected by the
% Interstellar Boundary Explorer (IBEX) satellite (left) and output from two
% runs of the IBEX computer simulation at different settings (middle, right).
% \label{f:logit2_results}}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 4: Simulations
\section{Simulations}
\label{sec:sims}

%------------------------------------------------------------------------------
%% SECTION 4.1 - Heliospheric Science
\subsection{Heliospheric Science}
\label{sec:helio_science}
%------------------------------------------------------------------------------

%% HELIOSPHERIC SCIENCE
%% - Short review of heliosphere
%% - Behavior of Energetic Neutral Atoms
Over the past 15 years, physicists have developed theoretical models
\citep{heerikhuisen2009pick,schwadron2013spatial,mccomas2017seven} to explain
the existence of the \textit{ribbon}, the process by which it is generated,
and it's corresponding shape and intensity. Dozens of theories emerged in the
few years immediately after the IBEX satellite was launched and began
returning data, and more have followed in the ensuing years. Different factors
that have been proposed to affect the shape and intensity of the
\textit{ribbon} include, but are not limited to, the extent to which hydrogen
ions in the solar wind are scattered, the number of interactions the ions have
with other particles, the makeup and affect of the interstellar magnetic
field, and additional secondary sources of ENAs.

Physicists engaged in this work have developed computer models to represent
the theoretical models they have proposed. Therefore, given specific values
for different model parameters, a simulation can be run to output a proposed
\textit{sky map}. As stated previously, Bayesian Inverse problems are
concerned with leveraging simulator output for the purpose of estimating the
distribution of model parameters. This is the primary interest of physicists
involved in this project, as estimation of these parameters will give credence
to what model best represents the data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------
%% SECTION 4.2 - Heliospheric Computer Model
\subsection{Heliospheric Computer Model}
\label{sec:helio_comp_model}
%------------------------------------------------------------------------------

%% HELIOSPHERE COMPUTER MODEL
%% - Introduce computer model
%% - Explanation of parameters in computer model
%% - Introduce the data from the computer model

In this paper, we make use of two models developed by scientists at LANL and
Princeton University. In these models, the shape and intensity of the GDF is
controlled by parameter3 and parameter4, and the shape and intensity of the
\textit{ribbon} is controlled by parallel mean free path and ratio. We leave a
more in-depth physical interpretation and analysis of these parameters to more
appropriate venues \citep{zirnstein2021dependence,zirnsteinGDFsims2015}.
However, for simple reasoning, one can think of the parallel mean free path as
the distance a hydrogen ion travels beyond the heliopause before interacting
with another particle, receiving an electron, and becoming an ENA. The ratio
parameter is closely related, being the fraction of perpendicular mean free
path and parallel mean free path. We can run this model and output a vector of
proposed ENA rates at specific spherical coordinates. The resolution of the
output can be varied, but typical runs output 16,200 rates, corresponding to a
2 degree grid over latitude and longitude.

We have access to $n=66$ runs of the simulator. These runs represent all
combinations of 11 unique values of parallel mean free path and 6 unique
values of ratio. The spread of parameter values was determined by the space
scientists to include all values considered reasonable. Parallel mean free
path ranges from 500-3000 astronomical units (AU) in increments of 500 AU.
Ratio values are unitless, and span from 0.001 to 0.1. Each computer model run
is executed on an identical grid over $x$, $y$, and $z$ (although all our
visuals will be shown after converting to geographical coordinates, latitude
and longitude).

In order to showcase and validate our proposed methodology on the IBEX data,
we will first test on simulated sky maps. To do this, we will treat one output
from the computer model simulation as the ``truth.'' Using this output as the
underlying mean ENA rate, we will draw Poisson counts with exposure time
specified by the real IBEX data. Figure \ref{f:ibex_synth_ex} shows both the
``truth'' from the simulation (top right) and the ``observed'' simulated data
(top left). Note that the locations of the simulated data are the recorded
coordinates for ENAs captured by the IBEX satellite in it's orbit. This
accounts for the unique spread of points and the strings of missing data,
which corresponding to certain orbits of the satellite.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.25,trim=0 0 0 0,clip=TRUE]{sim_ibex_real_calib.png}
\caption{Simulated satellite data based on an output from the computer model (top
left). True simulator output (top right). Estimated calibration parameters (bottom
left). Estimated output via surrogate at estimated calibration parameters (bottom
right).
\label{f:ibex_synth_ex}}
\end{figure}
Now, treating the simulated data as if it came from the satellite, we run our
Poisson Bayesian Inverse Problem framework and estimate the underlying
parameters. In Figure \ref{f:ibex_synth_ex}, the bivariate posterior
distribution of parallel mean free path and ratio are displayed (bottom left).
We can see that our framework does a good job of obtaining the true
combination of parameters, with the posterior mode lying near the truth.
Additionally, the truth falls squarely in the 95\% credible interval. As a
sanity check, we take the value of each parameter at the posterior mode and
insert those into our fitted surrogate model. The predicted output is shown in
the bottom right of Figure \ref{f:ibex_synth_ex}. It is obvious that there are
some differences between the estimated sky map and the ``truth.'' The shape
and intensity of the ribbon in each sky map is slightly different. However, it
is also abundantly clear that each map (truth and estimated) could have
reasonably generated the data we ``observe''in the top left panel. Therefore,
we can confidently assert that our framework recovers the truth.

%------------------------------------------------------------------------------
%% SECTION 4.3 - Experiment
\subsection{Experiment}
\label{sec:ibex_sim_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from simulated data
%% - Lead-in to real data

Before we get ahead of ourselves, we recognize that Figure
\ref{f:ibex_synth_ex} is just one data point. So, to further validate our
framework, we can apply this method to each unique combination of calibration
parameters within our simulator output. Due to the inability for the model to
provide good posterior distributions for boundary parameters, we do not
consider combinations for which at least one parameter is on its edge. For
instance, when parallel mean free path is either 500 or 3000 AU and/or ratio
is either 0.0001 and 0.1. Removing these edge cases results in 36 remaining
unique combinations. For each combination, we follow the same procedure as
described above. First, we elect one unique combination and create simulated
observed satellite data based on the computer model. Next, we remove that
combination from the training data used to fit our Scaled Vecchia Gaussian
Process surrogate model in our calibration framework. Finally, we run MCMC to
sample from the posterior distribution of our calibration parameters and
obtain an estimates for the posterior mean. Our results are shown in Figure
\ref{f:synth_estimates_all}. In each plot of the posterior distributions, we
have included as a blue star the true combination of parameters that generated
the simulated data. Our 95\% credible intervals encompass the truth in 34 of
the 36 of the iterations, near the nominal coverage rate we'd expect.

%%% Can we put something in here that shows using the computer model improves prediction
%%% over just modeling field data? Hard to do without spoiling my next paper?
% In addition to estimating the calibration parameters, we can utilize our framework to
% achieve better prediction at out-of-sample field locations. While we have not dealt
% with real data from the satellite just yet, we can simulate satellite data as before
% and run cross-validation on holdout sets.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3,trim=0 0 0 0,clip=TRUE]{calib_estimates_all.png}
\caption{Posterior distributions for estimated calibration parameters for each unique
combination of values that has been treated as a holdout set.
\label{f:synth_estimates_all}}
\end{figure}

We have demonstrated that our framework can recover the true parameter values
that generated simulated data. However, space scientists are interested in
understanding the behavior of the heliosphere by estimating these parameters
for real data. So, the natural next step is to take our approach and apply it
to data actually observed by the IBEX satellite. This leads naturally into our
next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 5: Real Data
\section{Real Data}
\label{sec:ibex_real}

%------------------------------------------------------------------------------
%% SECTION 5.1 - IBEX
\subsection{IBEX}
\label{sec:ibex}
%------------------------------------------------------------------------------

%% IBEX
%% - Reintroduce satellite
%% - Explain details of data
%% - Explain the years gathered

We have referred to the IBEX satellite throughout this paper without going
into immense detail of just how the data is collected. As stated previously,
while the IBEX satellite orbits Earth, the IBEX-Hi ENA imager detects ENAs and
records their energy level and approximate location of origin. ENAs can be
sorted into six distinct energy levels, labeled as ESA 1, 2, \ldots, 6. At
regular time intervals, the satellite is rotated slightly to point to a new
section of sky. These adjustments, along with the satellite's orbit, allows
IBEX to view the entire sky over a period of six months. Therefore, dating
back to 2008, we have access to $\sim$35 estimated \textit{sky maps}.

Add another paragraph.

%------------------------------------------------------------------------------
%% SECTION 5.2 - EXPERIMENT
\subsection{Experiment}
\label{sec:ibex_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from the real data
%% - Comment on the missalignment between simulator and reality

Our setup is similar to that in Section \ref{sec:ibex_sim_exp}, only
substituting in real data for simulated data. Therefore, we execute Algorithm
\ref{alg:gibbs} by initially fitting a Scaled Vecchia GP surrogate to all
$n=66$ computer model runs. Then, MCMC samples from the posterior distribution
of our model parameters, evaluating the likelihood of the real data given the
surrogate predictions at new proposed parameter values. Some things of note
are that we only use satellite data collected during the years 2011-2013. This
comes recommended by the space scientists, as the models are developed to
reflect the behavior of the Sun and corresponding solar wind during that time
period. Additionally, we only use ENAs collected at energy level ESA 4. This
energy level allows the best context for analysis, whereas lower ESAs have a
much weaker signal and higher ENAs overwhelm the analysis.

Results are shown in Figure \ref{f:real_estimates_all}.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3,trim=0 0 0 0,clip=TRUE]{calib_estimates_all.png}
\caption{Posterior distributions for estimated calibration parameters for each unique
combination of values that has been treated as a holdout set.
\label{f:real_estimates_all}}
\end{figure}

%------------------------------------------------------------------------------
%% SECTION 5.3 - DISCREPANCY
\subsection{Discrepancy}
\label{sec:ibex_discrep}
%------------------------------------------------------------------------------

%% DISCREPANCY
%% - Refer to KOH, Higdon, and their use of discrepancy
%% - Introduce simple scaling discrepancy (iniclude some equations)
%% - Refer to results in Appendix for simulated data where we discover the true scale
%% - Explain set up for IBEX calibration with discrepancy
%% - Show results
%% - Comment on complexity of discrepancy. Scale isn't sufficient.
%% - Perhaps a stationary GP is not even sufficient

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 6: Discussion
\section{Discussion}
\label{sec:discuss}

We have introduced an effective tool for solving inverse problems using
Bayesian methods where 1) data from a field experiment is not normally
distributed and 2) computer model output is extremely high-dimensional,
demanding an approximation for the Gaussian process surrogate. In simulated
experiments, our framework provides accurate predictions at unsampled input
locations. Additionally, when provided with data where the underlying
calibration parameters are known, we can recover the truth at the nominal 95\%
rate. We applied this methodology to data collected from the IBEX satellite,
and corresponding simulator output generating heliospheric sky maps. When
faced with real data from the satellite, our method shows that large
discrepancies exist between the computer model and the physical process it
attempts to represent. Further collaboration is needed to inform the
theoretical astrophysicists responsible for model development where the model
is lacking, and improve its fidelity to the truth.

Our framework is only applied to counts assumed to come from a Poisson
distributed process. We encourage other practitioners to use this work in
other contexts where data from the field may come from some other distribution
(e.g. Negative Binomial, Bernoulli, Exponential, etc.). We recognize the
limitations of our method. First, the Scaled Vecchia GP surrogate for the
computer model fails to provide full uncertainty quantification on its
parameter estimates. A fully Bayesian framework could be developed, perhaps
with the use of a deep Gaussian process to handle the inherent
non-stationarity in the response surface. Finally, this project motivates the
need for a more exhaustive procedure to perform hypothesis testing for which
theoretical model is most probable, considering the data observed.

\subsection*{Acknowledgments}

RBG and SDB are grateful for funding from NSF CMMI 2152679. This work has been
approved for public release under LA-UR-XX-XXXX. SDB, DO and LJB were funded
by Laboratory Directed Research and Development (LDRD) Project 20220107DR.

\bibliography{ibex_bayes_inv}
\bibliographystyle{jasa}

\appendix

\end{document}
