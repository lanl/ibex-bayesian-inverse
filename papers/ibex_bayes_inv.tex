\documentclass[12pt]{article}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, amsfonts, amssymb, mathrsfs}
\usepackage{caption}
\usepackage{dcolumn}
\usepackage{filemod}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{makecell}

\oddsidemargin=0.25in
\evensidemargin=0.25in
\textwidth=7in
\textheight=8.75in
\topmargin=-.5in
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\footskip=0.5in

\begin{document}

\title{Bayesian Statistical Inversion for High-Dimensional Computer Model
Output and Spatially Distributed Counts}
\author{Steven D. Barnett\thanks{Corresponding author
\href{mailto:sdbarnett@vt.edu}{\tt sdbarnett@vt.edu}, Department of
Statistics, Virginia Tech} \and Robert B. Gramacy\thanks{Department of
Statistics, Virginia Tech} \and Lauren J. Beesley\thanks{Statistical Sciences
Group, Los Alamos National Laboratory} \and Dave Osthus\footnotemark[3] \and
Yifan Huang\thanks{ Nuclear and Particle Physics, AstroPhysics and Cosmology,
Los Alamos National Laboratory} \and Fan Guo\footnotemark[4] \and Eric J.
Zirnstein\thanks{ Department of Astrophysical Sciences, Princeton University}
\and Daniel B. Reisenfeld\thanks{Space Science and Applications Group, Los
Alamos National Laboratory}}
\date{}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
The Interstellar Boundary Explorer (IBEX) satellite orbits Earth and detects
heliospheric energetic neutral atoms (ENAs) to determine the rate at which
they are generated in the heliopause. Sophisticated computer models attempt to
represent the physical process that produce ENAs through various model
parameters. Statisticians use a Bayesian approach to solve these types of
inverse problems by using data collected in a field experiment along with
simulator output from different combinations of parameter settings to make
out-of-sample field predictions and learn the posterior distributions of model
parameters. However, inverse problems typically assume both a Gaussian
response for both experimental observations and a limited set of computer
model data. We introduce a novel Markov chain Monte Carlo framework that
accommodates the spatially distributed Poisson counts collected by the IBEX
satellite. We also propose the use of the Scaled Vecchia approximation as a
Gaussian process (GP) surrogate for the large-scale computer model training
data. We demonstrate the capability of our proposed framework through multiple
simulated examples and show that we can consistently recover the true
parameters in a discrepancy-free environment and obtain accurate
out-of-sample prediction. We apply this to the raw IBEX satellite data and the
corresponding computer model output.

\bigskip
\noindent \textbf{Key words:} Gaussian process, surrogate modeling,
Poisson, calibration, heliospheric science, IBEX, Vecchia approximation
\end{abstract}

\doublespacing % no double spacing for arXiv

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

%% IBEX
%% - Talk about the satellite
%% - Explain the mission
%% - Introduce the idea of a computer model
%% - Explain desire to understand the distribution of parameters
The National Aeronautics and Space Administration (NASA) launched the
Interstellar Boundary Explorer (IBEX) satellite in 2008 as part of their Small
Explorer program \citep{mccomas2009aIBEX} to deepen our understanding of the
heliosphere. The heliosphere is the bubble formed by the solar wind that
encompasses our solar system and acts as a barrier between our planetary
system and interstellar space. In particular, the behavior at the edge of the
heliosphere, known as the heliopause, is of interest. Here, highly energized
hydrogen ions that make up the solar wind interact with neutral atoms,
occasionally undergo electron exchange, and become neutral themselves. At this
point, these energetic neutral atoms (ENAs) are unaffected by magnetic fields
and therefore travel in a straight line. As a result, some ENAs eventually
come to Earth and can be detected by IBEX. IBEX contains an instrument called
the IBEX-Hi ENA imager \citep{funsten2009IBEXHiENA}, which records the energy
level and approximate location of origin for each ENA that enters the
satellite, providing sufficient data to estimate the rate at which these
particles are created throughout the heliopause. An example of this estimated
surface or image, referred by space scientists as a \textit{sky map}, can be
found in the left panel of Fig. \ref{f:fig1}. \textit{Sky maps} are a tool
used by space scientists as they research the heliosphere and seek to
understand its creation and evolution.

One can think of the heliosphere as a boat moving through water, the water
here representing interstellar space. At the outset of IBEX's mission,
scientists expected to see an elevated rate of ENAs being generated at the
front (nose) and back (tail), much like the turbulent interaction between a
boat and water at its bow and stern. And indeed, that belief was validated by
IBEX's collected data. These regions of raised ENA rates are termed
\textit{globally distributed flux}, or GDF, by the space science community.
More ENAs are produced at the nose and tail where there is an increased number
of collisions between hydrogen ions and neutral particles in the interstellar
medium. However, in a completely unanticipated finding, IBEX also recorded a
thin string of higher rates of ENAs \citep{fuselier2009IBEXribbon} being
created that wraps around the heliosphere. Scientists now refer to this
phenomenom as the \textit{ribbon}. This unique feature of a \textit{sky map}
is clearly visible in Fig. \ref{f:fig1}. Since this discovery, space
scientists have conducted years of extensive research to explain the existence
of the \textit{ribbon} and have proposed dozens of theories to describe the
physical process that generates it \citep{mccomas2014ibex, zirnstein2018role,
zirnstein2019strong, zirnstein2021dependence}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.74,trim=0 0 65 0,clip=TRUE]{ibex_real1.pdf}
\includegraphics[scale=0.74,trim=35 0 65 0,clip=TRUE]{sim1.pdf}
\includegraphics[scale=0.74,trim=35 0 0 0,clip=TRUE]{sim2.pdf}
\caption{Observed rate of energetic neutral atoms (ENA) detected by the
Interstellar Boundary Explorer (IBEX) satellite (left) and output from two
runs of the IBEX computer simulation at different settings (middle, right).
\label{f:fig1}}
\end{figure}

In order to further explore these theories, scientists have developed computer
models to create synthetic \textit{sky maps} based on certain inputs (Figure
\ref{f:fig1}). These computer simulations rely on a number of parameters that
can be varied, thus modifying the shape and intensity of both ribbon and GDF.
Although many simulations exist, we focus on two that are publicly available:
a GDF model proposed by \citet{zirnstein2021heliosheath} and a ribbon-only
model developed by \citet{zirnsteinGDFsims2015}. The ribbon-only model relies
on two parameters, \textit{parallel mean free path} and \textit{ratio}, and
the GDF model relies on two additional parameters, \textit{parameter1} and
\textit{parameter2}. Understaing the distribution of these variables is
instrumental to determining which proposed theory carries the most weight, or
what postulation corresponds to the most probable combination of parameter
values. Although we acknowledge the oversimplification of this description, we
essentially want to provide the ability for space scientists to conduct
hypothesis testing between competing ideas. But so far, minimal work has been
done to validate theories or further understand the heliosphere by pairing the
computer model output with the raw data collected by IBEX.

%% PROBLEM
%% - Classic Inverse Problem
%% - Mathematical model is expensive (and perhaps not accurate)
%% - Use Bayesian methods
Now, this is a classic inverse problem. Given observed data and an assumed
model that generates the data, the goal is to determine the true values of the
unknown parameters in the model. A classic approach to solving the inverse
problem is to evaluate the model with a multitude of unique combinations of
parameter values. Then, select the set of parameter values that minimize the
distance between model output and real observations. However, there are
limitations to this technique. For one, computer models often involve complex
calculations are therefore expensive, sometimes taking hours, days, or weeks
(as is the case in for the IBEX simulation), precluding the execution of an
extensive set of parameter combinations. With limited data, a Bayesian
approach is often better. Applying Bayesian methods allows practitioners to
inform the model with prior information on the parameters. Additionally,
Bayesian procedures treat the parameters as random variables and provide more
information through an estimated posterior distribution over the most likely
values instead of a single point estimate.

%% OTHER WORK / CHALLENGES
Inverse problems are not new, and statisticians have previously employed
Bayesian methods to solve them
\citep{kaipio2011bayesian,stuart2010inverse,knapik2011bayesian}. But the data
collected by IBEX and its simulated representation present a unique challenge:
a non-Gaussian field response, high-dimensional model output, and the
large-scale field and simulated datasets seen in modern-day computing. Table
\ref{tab:prev_work} summarizes the features of the IBEX inverse problem and
reviews the necessary capabilities that are present in previous work. For
instance, \citep{kennedyohagan2001} offer the canonical computer model
calibration framework, which provides a fully Bayesian approach to estimate
the distribution of calibration parameters and improve prediction
out-of-sample. However, their methodology is limited to Gaussian observations
and small, simulator output. \citep{higdoncalib2008}, and its more recent
incarnation SEPIA \citep{gattiker2020lanl}, also employ Bayesian methods and
account for computer model output that is functional. But in addition to the
previous restriction of Gaussian field data, SEPIA is somewhat rigid and
requires the user to specify a number of basis functions to represent the
simulator response. \citep{gramacy2015calibrating} consider computer model
calibration with large-scale output from a computer experiment, necessitating
an approximation. But their implementation is not Bayesian, the scale of data
is still orders of magnitude less than IBEX, and the field data is small.
\citep{grosskopfcountcalib2020} provide a extension of the Kennedy \& O'Hagan
(hereafter referred to as KOH) framework to non-Gaussian data that maintains a
Bayesian approach. But they are limited to small amounts of computer model
data and do not consider higher dimensional responses. In contrast, our
approach checks all the boxes to achieve the goal of IBEX space scientists and
therefore applies to a wider class of problems than previous work.
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|}
\hline
& Bayesian & \makecell{Non-Gaussian \\ Observations} & \makecell{Large-Scale \\ Computer Model \\ Output} & \makecell{High-Dimensional \\ Response} \\
\hline
Our Contribution & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
\citet{kennedyohagan2001} & \checkmark & & & \\
\hline
\citet{higdoncalib2008} & \checkmark & & & \checkmark \\
\hline
\citet{gramacy2015calibrating} & & & \checkmark & \\
\hline
\citet{grosskopfcountcalib2020} & \checkmark & \checkmark & & \\
\hline
\end{tabular}
\caption{A review of functionality to solve statistical inverse problems
present in previous academic works. Check marks indicate that functionality
exists within the cited paper and/or accompanying software. To save space,
only one citation is listed for each row, although we recognize these may be
replace with other studies. Note that our paper offers all required
capabilities.}
\label{tab:prev_work}
\end{table}

%% OUTLINE
%% - Review Bayesian Inverse Problems, GP Surrogates, SEPIA
%% - Introduce our framework
%% - Explain use of Vecchia approximation
%% - Demonstrate on simulated data
%% - Apply to real IBEX data
Our paper is laid out as follows. First, Section \ref{sec:review} reviews some
of the methodology that form the foundation for our work, namely Bayesian
Inverse Problems and Gaussian process surrogate models. We also give a brief
description of the theory behind SEPIA, which could classified as the current
``status quo'' in Bayesian Inverse Problems for high-dimensional computer
model output. In Section \ref{sec:gen_bayes_inv} we introduce our framework
for solving the inverse problem in non-Gaussian contexts. Section
\ref{sec:vecchia} describes our use of the Vecchia approximation to stretch
the capacity of our GP surrogate beyond small simulator training runs. In
Section \ref{sec:sims}, we illustrate the effectiveness of our framework in
discovering the true, underlying model parameters on simulated data. We then
return to our motivation application and apply our methods to the raw IBEX
satellite data in Section \ref{sec:ibex_real}. To conclude, we discuss any
extensions and future work in this area in Section \ref{sec:discuss}. Our
proposed framework is implemented at
\url{https://github.com/lanl/IBEX_Calibration} along with code reproducing all
included examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 2: Review:
\section{Review}
\label{sec:review}

%------------------------------------------------------------------------------
%% SECTION 2.1 - Bayesian Inverse Problems
\subsection{Bayesian Inverse Problems}
\label{sec:bayes_inv}
%------------------------------------------------------------------------------

%% BAYESIAN INVERSE PROBLEMS
%% - What is an inverse problem and why do we care?
%% - Obtained observations from a physical process to learn more about it
%% - We have some knowledge of the workings of the true process
%% - We can build a mathematical model, and therefore a computer model
%% - Classic IP: evaluate model as much as possible to find parameters/inputs
%%   that produce output aligned with the observations
Inverse problems involve the collection of observations from a physical
experiment in an effort to learn more about the process that generates these
data points. Typically, a scientist or researcher has some knowledge of the
physical or mathematical model (i.e. the data-generating process), but is
seeking to gain a more detailed understanding. The goal is often one of two
cases: 1) estimate the parameters (or distribution of the parameters) that
govern the physical model, and 2) if the research lacks some knowledge about
how the process was initiated, but the parameters are known, reconstruct the
inputs from which the observations originated. In this paper, we are focused
on the former, discovering the distribution of unknown paramters. The
motivation for such a method stems from the inability to either control or
precisely measure the factors (e.g. the force of gravity) in a physical
environment. Therefore, in order to gain more insight, scientists develop
mathematical models that attempt to represent that physical process. Often,
this results in a computer simulation. By comparing observations to simulator
output generated via unique combinations of model parameters, statisticians
attempt to learn the true, underlying parameter values. As mentioned in
Section \ref{sec:intro}, the classical approach is to find the best match
between observed data and extensive computer model via some minimization of
error (e.g. least squares, maximum likelihood).

%% - Observations are noisy
%% - Desire full UQ on parameters
%% - Go Bayesian
%% - Show Prior / Posterior
%% - No analytical form, must resort to MCMC
%% - LEAD IN: Computer model is expensive. Need a surrogate.
Bayes' rule, as put forth in Equation \ref{eq:bsi}, illustrates the Bayesian
approach. First, practioners are able to embed a certain amount of subject
matter knowledge into the analysis by imposing a prior on model parameters.
Second, Bayesians assume that the model parameters themselves are random
variables, and therefore obtain a full joint posterior distribution for the
model parameters, instead of basic point estimates and confidence intervals.
In Equation \ref{eq:bsi}, $\boldsymbol \theta$ represents the parameters of
the mathematical model that we wish to infer, $Y$ is our vector of physical
observations, and $X$ is the set of input locations. Here we assume that the
field data is Gaussian, as indicated by the $\mathcal{N}$ subscript on the
likelihood.
\begin{align}
\pi(\boldsymbol \theta | Y, X) = \frac{\mathcal{L}_\mathcal{N}(Y|\boldsymbol
\theta, X)
\pi(\boldsymbol \theta)}{\pi(Y|X)} \propto \mathcal{L}_\mathcal{N}(Y|\boldsymbol \theta,
X) \pi(\boldsymbol \theta)
\label{eq:bsi}
\end{align}
If $\pi(\boldsymbol \theta)$ is conjugate, inference for $\boldsymbol \theta$
is simple and can be done analytically. However, this is often not the case,
requiring an approximation of the posterior (i.e. Laplace, Variational Bayes).
Our framework utilizes Markov chain Monte Carlo (MCMC) methods to sample from
the posterior.

Before we go further, let's start up a very simple example. Consider the left
panel of Figure \ref{f:toy_calib}. Red stars denote field observations of some
true, unknown process at different locations $X$. Note that these data are
observed with some noise. Additionally, multiple observations are made at each
location. This is common, due to the expense of changing configurations of a
physical experiment, but it need not be the case. Light gray lines denote
individual runs of a computer simulation, at different settings of parameters
$\boldsymbol{\theta}$. Lastly, the black, bold, dashed line shows the true, underlying
process we want to estimate. Modeling the data as \citet{kennedyohagan2001}
did, we can make field predictions out-of-sample and estimate the posterior
distributions of $\boldsymbol{\theta}$. Predictions and posterior estimates
are shown in the middle and right panels of Figure \ref{f:toy_calib}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.51,trim=0 0 25 0,clip=TRUE]{logit1_obs.pdf}
\includegraphics[scale=0.51,trim=30 0 25 0,clip=TRUE]{logit1_est.pdf}
\includegraphics[scale=0.51,trim=30 0 0 0,clip=TRUE]{logit1_post_draws.pdf}
\caption{A simple inverse problem. Field observations and sample of computer
model runs (left), model evaluations at posterior draws and mean (middle), and
posterior distribution of model parameters (right).
\label{f:toy_calib}}
\end{figure}

%% DO A REVIEW OF OTHER METHODS (KOH? BAYES INV? STUART? TECKENTRUP? COLLINS?)

One difficulty in our situation is the expense of the computer model. As a
result of increased access to supercomputing resources, computer models have
increased in complexity over the past two decades and are able to produce
extremely high-fidelity output. However, even with modern-day computing power,
some simulations can take hours, days, or weeks to complete. This limits the
amount of runs that can be executed and therefore restricts the exploration of
the model parameter space. Therefore, we need a model (what we call a
surrogate, or emulator) fit to the simulator output to make accurate and
reliable predictions for out-of-sample values of our computer model
parameters. Any statistical model can serve as a surrogate, ranging from a
linear regression to complex neural networks. However, the canonical surrogate
for computer experiments and computer model calibration has come to be the
Gaussian process \citep{gramacy2020surrogates}.

%------------------------------------------------------------------------------
%% SECTION 2.2 - Gaussian Process Surrogates:
\subsection{Gaussian Process Surrogates}
\label{sec:gp_surr}
%------------------------------------------------------------------------------

%% GAUSSIAN PROCESS SURROGATES
%% - What is the motivation? Expensive computer experiments
%% - Set of n observations that follow a MVN
%% - Often mean-zero. In practice this means subtracting off the mean
%% - Results in all the action being in the covariance function
%% - Covariance depends on pairwise distance
%% - Explain some different kernels
%% - Talk about prediction. Show Kriging equations
%% - Nonparametric, flexible regression tool
%% - A lot of different applications
%% - Become popular in computer experiments because of interpolation
%% - This is what we want
%% - Bottleneck is O(n^3)
%% - LEAD IN: Output can be large. Need an approximation.
%% Review Gaussian processes and their use in surrogate modeling
A Gaussian process, hereafter referred to as a GP, is a realization of a
multivariate normal distribution, governed by some mean and covariance
functions. GPs are fit to observed data and used to make predictions at new
input locations, relying on the conditional multivariate normal or
\textit{kriging} equations \citep{matheron1963principles}. For example,
suppose we have a vector of observations $Y$ at input locations $X$. $Y$ is an
$n \times 1$ vector and $X$ is an $n \times p$ matrix. We make the assumption
that $Y$ is normally distributed with some mean $(\mu(X))$ and covariance
$(\Sigma(X))$, both functions of $X$. Predicting at a new location $x$ is
straightforward. Eq.~(\ref{eq:mvn_eq}) articulates how to obtain mean
predictions $\mu^*(x)$ and corresponding uncertainty quantification
$\Sigma^*(x)$.
\begin{align}
Y(X) &\sim \mathcal{N}\!\left(\mu(X), \Sigma(X)\right) \nonumber \\
y^*(x) \mid Y(X) &\sim \mathcal{N}\!\left(\mu^*(x), \Sigma^*(x)\right) \nonumber \\
\mu^*(x)&=\mu(x) + \Sigma(x, X) \Sigma(X)^{-1} (Y - \mu(X)) \nonumber \\
\Sigma^*(x)&=\Sigma(x, x) - \Sigma(x, X) \Sigma(X)^{-1} \Sigma(X, x) \label{eq:mvn_eq}
\end{align}
Although a generic mean function is specified in Eq.~\ref{eq:mvn_eq}, it is
common to set $\mu(x)=0$. In practice, this simply implies centering $Y$ by
subtracting off the sample mean, $\overline{Y}$. In this zero-mean context,
all the action takes place in the covariance function. The covariance function
typically depends on pairwise distances between input locations $x$ and $x'$.
Many covariance kernels exists, such as the Gaussian, Exponential, Matern. We
use the Matern kernel in our work, of which both the Gaussian and Exponential
kernels are special cases. The behavior of these kernels is determined by
three hyperparameters: a nugget $g$, a lengthscale $\theta$, and a scale
$\tau^2$. In the Matern kernel, an additional parameter ($\nu$) exists, which
controls the smoothness of the estimated surface. But we will not estimate the
smoothness parameter here and instead set $\nu =
\frac{5}{2}$ (\ref{eq:matern}), following common practice due to it allowing
the kernel function to be expressed in a simpler form. All hyperparameters can
be estimated using your preferred method (i.e. maximum likelihood, method of
moments). We will use Bayesian methods, specifically MCMC.
\begin{align}
\Sigma(x, x'|\sigma^2, \theta, \nu, g) &= \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left(\sqrt{2\nu} \frac{\|x-x'\|}{\theta}\right)^2 K_{\nu} \left(\sqrt{2 \nu}
\frac{\|x - x'\|}{\theta}\right) + g \cdot \mathbb{I} \nonumber \\
&= \sigma^2 \left(1 + \frac{\sqrt{5} \cdot \|x - x'\|}{\theta} +
\frac{5\cdot\|x-x'\|^2}{3\theta^2} \right) \text{exp}\left(-\frac{\sqrt{5} \cdot \|x
- x'\|}{\theta}\right) + g \delta_{x, x'}
\label{eq:matern}
\end{align}
GPs provide a flexible, nonparametric regression tool. In addition to computer
experiments, they are utilized in geostatistics
\citep{matheron1963principles}, spatial statistics
\citep{cressie2011statistics}, time series \citep{roberts2013gaussian},
optimization \citep{jones1998efficient}, and machine learning
\citep{rasmussen2003gaussian}. The reason for their popularity as surrogate
models in the world of computer experiments is their ability to interpolate
between observed data points (i.e. setting nugget $g = 0$). Although
stochastic computer models are becoming more common, computer experiments are
often deterministic, motivating the desire for a model that interpolates. GPs
are then useful by providing predictive surfaces that give higher estimates of
variance away from observed data, and virtually no variance at observed
locations. In fact, perhaps the hallmark of GPs is that they naturally provide
excellent uncertainty quantification for out-of-sample predictions, a matter
of importance to scientists exploring these computer models. Therefore, GPs
are widely used in computer experiments and Bayesian Inverse Problems.

%------------------------------------------------------------------------------
%% SECTION 2.3 - SEPIA / GPMSA
\subsection{SEPIA / GPMSA}
\label{sec:sepia}
%------------------------------------------------------------------------------

%% SEPIA / GPMSA
%% - O(n^3) bottleneck is nothing new
%% - Higdon et al. faced this at LANL in the early 2000s
%% - Their situation is unique in that n was small, but dimension is high
%% - Same situation as IBEX
%% - Instead of modeling the output as scalar, they did functional
%% - PCA
%%   - Show some equations
%%   - Maybe a graphic
%% - Must choose basis
%% - Used in calibration (an inverse problem), but does not support Poisson
In the early 2000s, \citet{higdoncalib2008} faced a similar challenge. Expensive
computer models at Los Alamos National Laboratory (LANL) produced limited runs with
extremely high-dimensional output in the form of images, shape description, time
series, etc. In order to efficiently fit a surrogate model to the simulator, they
needed to perform an approximation to limit the size of the data. As a solution, they
employed principal components analysis (PCA) to reduce the dimensionality of the
response. Specifically, they represented the output of a simulator model as the sum
of $n_k$ basis functions $\textbf{k}_1, \textbf{k}_2, \ldots , \textbf{k}_{n_k}$,
each scaled by a corresponding weight $w_i$, as shown in Eq. \ref{eq:higdon_pca}.
Instead of modeling the high-dimensional response, the GP only modeled the small set
of component weights. Bayesian inference was used to obtain the posterior
distribution of those component weights. Combining estimates the weights with the
calculated basis functions allowed for predictions out of sample. As a result of this
approximation, the dimension of the covariance matrix $\Sigma(X)$ in the predictive
equations of \ref{eq:mvn_eq} remained small.
\begin{align}
\textbf{y}(\textbf{x}, \textbf{u}) \approx \sum_{i=1}^{n_k} \textbf{k}_i \cdot w_i (\textbf{x}, \textbf{u}), \quad n_k \ll n \ \nonumber \\
w_i (\textbf{x}, \textbf{u}) \sim GP(\textbf{0}, \Sigma_{w_i}),
\label{eq:higdon_pca}
\end{align}
where $\Sigma_{w_i}$, dependent only on a limited number of combinations of
$\textbf{x}$ and $\textbf{u}$, is a $n_k \times n_k$ matrix. Hence, the decomposition
of $\Sigma_{w_i}$ avoids the O($n^3$) bottleneck in computation and can be completed
in a reasonable amount of time.

We propose a simpler approach, devoid of the definition and selection of how many
basis functions to use. Although this basis representation was successful, we argue
that what \citet{higdoncalib2008} really wanted to do was represent the functional
output as a univariate response. That is, replicate the entry in the input matrix $X$
for each response the number of times equal to the length of the output vector. In
essence, implicitly encode the index (e.g. pixel in image, moment in time series)
into the response vector for each observation in the input matrix. We believe that
had \citet{higdoncalib2008} had access to the capacity of modern-day computing, along
with the novel methodology proposed since the early 2000s that leverages that
compute, they would have steered their implementation in a significantly different
way. We propose that approach next and illustrate the improved performance in both
predictive accuracy and uncertainty quantification, along with a reduction in
computational expense.

%% Limitations:
%% Poisson response. Clunky way to deal with large data. Not generalizable.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 3: Methods
\section{Methods}
\label{sec:methods}

%------------------------------------------------------------------------------
%% SECTION 3.1 - Generalized Bayesian Inverse Problems with GP Surrogates
\subsection{Generalized Bayesian Inverse Problems with GP Surrogates}
\label{sec:gen_bayes_inv}
%------------------------------------------------------------------------------


%% Talk about computer simulations with non-Gaussian response
%% Introduce equation
%% ?? Maybe specify priors here??
%% Talk about need for MCMC <- Algorithm 1 <- POLISH
%%%% Here mention that we are using modular Bayes
%% Go back to 1D toy problem, but with GP surrogate and Poisson data

%% GENERALIZED BAYESIAN INVERSE PROBLEMS WITH GP SURROGATES
%% - Return to review, but with general response
Computer simulations are not limited to modeling physical processes that
generate a continous response. For instance, a field experiment may return a
binary response, such as success or failure. Categorical responses may expand
beyond two classes, representing a multinomial reponse. In this paper we focus
on Poisson-distributed counts due to our motivating application, but our
framework can easily be generalized to any response. Revisiting Eq.
\ref{eq:bsi}, we replace the Normal likelihood with a Poisson likelihood, as
shown below.
\begin{align} %% IDEA: specify priors we are using here?
\pi(\boldsymbol \lambda | \boldsymbol y, X) &= \frac{\mathcal{L}(\boldsymbol y|\boldsymbol \lambda, X)
\pi(\boldsymbol \lambda)}{\pi(\boldsymbol y|X)} \propto \mathcal{L}(\boldsymbol y|\boldsymbol \lambda,
X) \pi(\boldsymbol \lambda) \nonumber \\ &\propto \left[ \prod_{i=1}^N
   \frac{\lambda_i^{y_i}e^{-\lambda_i}}{y_i!} \right] \pi(\boldsymbol \lambda) =
   \left[ \prod_{i=1}^N \frac{(r_i t_i)^{y_i}e^{-r_i t_i}}{y_i!} \right]
   \pi(\boldsymbol
   \lambda)
\label{eq:bsi_pois}
\end{align}
Note that $\lambda_i$, the mean function at a certain input location $x_i$, is
represented as the product of an underlying rate, $r_i$, and an exposure time,
$t_i$. For unit exposure ($t_i=1$), the mean can simply be represented as
$\lambda_i$. However, in our application we are not given unit exposure, but
each data point is a total count over a specific time interval.

Eq. \ref{eq:bsi_pois} gives us the form of the posterior. But without a
conjugate prior to provide an analytical solution, and the infeasibility of
marginalizing out $\boldsymbol \lambda$, we must sample the posterior of
$\boldsymbol \lambda$ using MCMC. The difficulty is in the second step of the
for loop in Algorithm \ref{alg:gibbs}. Here we need to predict $\boldsymbol
\lambda$ at input locations $X$, but with proposed model parameters, $u^*$.
Unfortunately, that falls to our computer model, which is expensive and can
take hours or days to complete, making MCMC impractical. Therefore, we need a
surrogate for the simulator that 1) can be fit to training data in a
reasonable time frame, and 2) can provide good predictions at new values of
the calibration parameters quickly. Here we utilize a GP surrogate
(\ref{sec:gp_surr}).
\begin{algorithm}[ht]
\DontPrintSemicolon
Initialize $u^{(0)}$, $\Sigma^{(0)}$. \\
\For{$t = 1, \dots, T$}{
  Propose $u^{(t)} \mid u^{(t-1)}, \Sigma^{(t-1)}$ via MH, \\
  Predict $\hat{f}(Y, T, X, u^{(t)})$ \\
  Evaluate $\mathcal{L}(Y|\hat{f})$ \\
  Accept or reject $u{(t)}$
  }
\caption{MCMC (Metropolis-within-Gibbs) for Poisson Inverse Bayes.}
\label{alg:gibbs}
\end{algorithm}

%% - 1D example with Poisson response
%%   - note that we are treating the response as a scalar
%%   - response is Poisson
%%   - Graphic showing field / computer model data, fit, and posterior over parameters
Now, before we get ahead of ourselves, letâ€™s build on our toy problem
introduced in \ref{sec:bayes_inv}. Instead of observing the function $f(X)$
with random, Gaussian noise, suppose we are given counts, drawn from a Poisson
process with $\boldsymbol \lambda = f(X)$, as shown in Figure 3. As described
in \ref{sec:bayes_inv}, we know that the $f(X)$ is a function of two
parameters, $\mu$ and $\lambda$. We'll skip the idealized situation where we
have access to unlimited evaluations of the computer model. Instead, we'll fit
a GP surrogate to a limited run of model evaluations to then make a surrogate
prediction at each MCMC iteration. Results are shown in the right panel Figure
\ref{f:toy_calib_pois}. We can see that our framework picks parameter values
that again estimate the simulator output almost exactly. The posterior mean,
shown in the right panel, falls close to the true values of the parameters,
this time as the underlying mean function generating counts $\boldsymbol y
\sim \text{Poisson}(\boldsymbol \lambda)$. We recognize that there is more distance
between the truth and our estimate, but we attribute that extra uncertainty to
using a surrogate rather than the computer model itself.%% IDEA: fit a surrogate to the data itself. maybe use GPVecchia
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.51,trim=0 0 25 0,clip=TRUE]{logit1_pois_obs.pdf}
\includegraphics[scale=0.51,trim=30 0 25 0,clip=TRUE]{logit1_pois_est.pdf}
\includegraphics[scale=0.51,trim=30 0 0 0,clip=TRUE]{logit1_pois_post_draws.pdf}
\caption{An inverse problem for the underlying mean of a Poisson process.
Counts observed in the field and a sample of computer model runs (left), model
evaluations of the underlying mean at posterior draws and the mean estimate
(middle), and posterior distribution of model parameters (right).
\label{f:toy_calib_pois}}
\end{figure}

Note that we adopt the modularized approach of
\citet{bayarri2009modularization}, by both preference and necessity. That is,
we fit our surrogate model to simulator data only, not incorporating field
data. \citet{kennedyohagan2001} only apply their method to situations when
both Gaussian computer model output and Gaussian field responses. This allows
joint modelling of simulator and field data. We aren't able to do this with
Poisson counts as the field response. Additionally, although we do not address
a discrepancy between simulator and reality as \citet{kennedyohagan2001} do,
which is a motivation \citet{bayarri2009modularization} cite to their
modularized approach, we share the belief that predictions for our surrogate
for the computer model should only be informed by the simulator output.

%------------------------------------------------------------------------------
%% SECTION 3.1.1 - A Larger Problem
\subsubsection{A Larger Problem}
\label{sec:gen_bayes_inv_large}
%------------------------------------------------------------------------------

%% Align with IBEX example <- increase dimension of response
%% Explain the need for more training data
%% Explain the difference between functional and scalar
%% Hold that thought on SUPER large data
%% Show panel of responses and simulator output
%% Show panel of results

%% IDEA: consider something similar to our ibex example
%% EXAMPLE
%% - 2D example with Poisson response
%%   - Same Thing as above
%%   - LEAD IN: capacity of GP is stretched
Let's shift gears and jump ahead to our motivating example that includes a
higher dimensional response. Although the problem is harder to visualize with
a higher dimensional response, even in 2D, our framework and approach is
identical.  But the more pressing barrier to overcome is the need for more runs
of the simulation in order to obtain good predictions from a surrogate due to
the increased dimensionality. In the example above, the number of computer
model evaluations was limited ($n < 100$). As mentioned before, computational
limits ensure that this is typically the case in surrogate modeling of
computer simulations. But note that the output of our computer model is
functional, i.e. each response is a vector. Above, each simulation was
evaluated at 20 separate points. Therefore, although the number of runs is
small, we are really modeling 800 ($40
\times 20$) responses. A training set of 800 data points is already stretching
the limitations of a GP (decomposing a $800 \times 800$ matrix). But consider
the example in Figure \ref{f:fig1}. We leave the introduction of the IBEX
simulator to \ref{sec:helio_comp_model}, but it's clear the output is high
resolution. In fact, the two degree grid over latitude and longitude (under
the hood the output is in spherical coordinates $x$, $y$, and $z$) results in
16,200 unique locations, or pixels. So, even modeling one image results in a
$16,200 \times 16,200$ grid. Combine that with $n=66$ unique pairs of model
parameters and our response vector will have greater than one million
elements. Even if we used a smaller LHS space-filling design of size $n=15$
over the model parameters, the response vector would consist of over 200,000
points. In that case, a GP would not feasibly be able to model each individual
point as a separate response. For high dimensional output, we reviewed what
has been done historically in a inverse problem setting in Section
\ref{sec:sepia}. Next, we will propose a modern, intuitive, more
generalizable, and better-perfoming approach.
% \begin{figure}[ht!]
% \centering
% \includegraphics[scale=0.5, trim=30 15 40 0,clip=TRUE]{logit2_obs.pdf}
% \includegraphics[scale=0.5, trim=30 15 95 0,clip=TRUE]{logit2_model1.pdf}
% \includegraphics[scale=0.5, trim=75 15 40 0,clip=TRUE]{logit2_model2.pdf}
% \caption{Poisson draws from a mean process over $x_1$ and $x_2$ (left).
% Evaluations of a computer model at different settings of four parameters
% ($\mu_1$, $\nu_1$, $\mu_2$, $\nu_2$) attempting to represent the true mean
% process (middle, right).
% \label{f:logit2_examp}}
% \end{figure}

%------------------------------------------------------------------------------
%% SECTION 3.2 - Vecchia Approximation
\subsection{Vecchia Approximation}
\label{sec:vecchia}
%------------------------------------------------------------------------------

%% VECCHIA APPROXIMATION
%% - Review improvement in computation
%% - Higdon may have modeled it this way if he had the resources
%% - List GP approximations from past decade
%% - We use Vecchia
%% - Show formulas
%% - Specify that we use Scaled Vecchia approximation, what it is
Perhaps the primary weakness of GPs is the need to decompose an $n \times n$
covariance matrix, making it difficult to scale to large datasets,
increasingly common in modern-day applications. Many methods have been
introduced to address this computational burden. These include, but are not
limited to, fixed-rank kriging \citep{cressie2008fixed}, inducing points
\citep{banerjee2008gaussian}, compactly supported kernels
\citep{kaufman2011efficient}, divide-and-conquer \citep{gramacy2015local}, and
nearest neighbors \citep{datta2016hierarchical, wu2022variational}. In the
context of computer model calibration, \citet{higdoncalib2008} use a basis
representation via principal components to accommodate high-dimensional
output.

As indicated in Section \ref{sec:intro}, we aim to modernize this approach. We
believe treating the data as a functional or multivariate response, then
reducing the dimension using PCA, and fitting a GP on component weights isn't
straightforward or natural. In fact, we argue that had \citet{higdoncalib2008}
had access to modern-day computing, they would have modeled their application
more simply as a univariate response. In other words, shift the dimensions of
the response ($x_1, x_2, \ldots$) into the input matrix $X$ for each entry in
the response vector. This clearly inflates the size (e.g. number of rows) of
the training data. But with the dramatic increase in computing capacity and
availability over the past decade and the aforementioned advances in
methodology, we believe there are GP approximations available to model the
output of a computer simulation in this manner. We propose using the recently
popularized Vecchia approximation \citep{vecchia1988estimation,
katzfuss2020vecchia, katzfuss2021general, zhang2022multi}.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5,trim=5 38 65 0,clip=TRUE]{ibex_sim_pmfp_1500_0.005.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1625_0.005.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1750_0.005.pdf}
\includegraphics[scale=0.5,trim=5 38 65 0,clip=TRUE]{ibex_sim_pmfp_1500_0.0075.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1625_0.0075.pdf}
\includegraphics[scale=0.5,trim=35 38 65 0,clip=TRUE]{ibex_sim_pmfp_1750_0.0075.pdf}
\includegraphics[scale=0.5,trim=5 0 65 0,clip=TRUE]{ibex_sim_pmfp_1500_0.01.pdf}
\includegraphics[scale=0.5,trim=35 0 65 0,clip=TRUE]{ibex_sim_pmfp_1625_0.01.pdf}
\includegraphics[scale=0.5,trim=35 0 65 0,clip=TRUE]{ibex_sim_pmfp_1750_0.01.pdf}
\caption{DGP (top row) and mw-DGP (bottom row) on cross-in-tray.
Columns provide predictive mean (left) and warpings $W^{\cdot 1}$ and
$W^{\cdot 2}$ (middle and right, respectively).
\label{f:ibex_sim_surr}}
\end{figure}

The Vecchia approximation relies on the ability to write a multivariate
likelihood function as the product of univariate conditional likelihoods as
shown below.
\begin{align}
L(Y)&=\prod_{i=1}^n L(Y_i \mid Y_{g(i)})
\hspace{0.3em} \mbox{ where } g(1)=\emptyset \hspace{0.3em} \mbox{ and } g(i)=\{1, 2,
\ldots, i-1\} \nonumber \\
&\approx \prod_{i=1}^{n}L(Y_i \mid Y_{h(i)}) \hspace{0.3em} \mbox{ where } h(i)
\subset \{1, 2, \ldots, i-1\} \hspace{0.3em} \mbox{ and } |h(i)| = m \ll n
\label{eq:vecchia}
\end{align}
Reducing the size of the conditioning set ($h(i)$ in Eq. \ref{eq:vecchia})
induces sparsity in the precision matrix, speeding up the evaluation of the
likelihood. This takes an operation of order $O(n^3)$ down to $O(nm^3)$,
motivating small values of $m$. Logically, as the size of $h(i)$ decreases,
the evaluation of the likelihood becomes faster, but the quality of the
approximation is reduced. On the flip side, as one increases $m$, execution
time grows, but the quality of the approximation improves. This continues
until $n=m-1$, when the likelihood becomes no longer an approximation. We find
a value of $m < 30$ to be more than sufficient, but the choice can be impacted
by the nature of the data and one's computational budget. The conditioning set
can be constructed in many ways, first by how the data itself is ordered, and
second by how neighbors are selected. Many different orderings have been
proposed \citep{guinness2018permutation}, such as a maxi-min distance
strategy. We default to a random ordering of the training data and
nearest-neighbors approach for selecting the conditioning set, which has been
done previously \citep{sauer2023active} with successful results.

%------------------------------------------------------------------------------
%% SECTION 3.2.1 - Scaled Vecchia Approximation
\subsubsection{Scaled Vecchia Approximation}
\label{sec:vecchia_ex}
%------------------------------------------------------------------------------

Several implementations of the Vecchia approximation exist, but we employ one
that has risen to the top in terms of predictive performance and computational
thriftiness in the context of computer experiments, that is, the Scaled
Vecchia approximation introduced by \citet{scaledvecchiakatzfuss2022}. We
refer you to their paper for a more in-depth explanation of their work. But,
the main crux of this implementation is the pre-scaling of inputs by dimension
before the creation of conditioning sets based on nearest neighbors.
\citet{scaledvecchiakatzfuss2022} suggest that dimensions affect the response
differently and at varying rates, and scaling the data beforehand addresses
this. Publicly available code for this implementation that we directly rely on
is readily accessible at
\url{https://github.com/katzfuss-group/scaledVecchia}.

%% EXAMPLES
%% - 2D example with LOTs of data
%%   - Show results. Compare to previous
%% - Show timing test on simulated data
%%   - Graphic should include fitting and prediction
%%   - Include different number of bases for SEPIA, maybe compared with different m
%%   - Show performance of different models
Let's return to the example we kicked down the road in Section
\ref{sec:gen_bayes_inv_large} and see how the Scaled Vecchia approximation
performs. We have access to 66 model runs, each producing an image of 16,200
points. Figure \ref{f:ibex_sim_surr} shows four of the actual simulation
outputs (those displayed with thin, solid borders). A Scaled Vecchia GP
surrogate was fit to all 66 simulated runs. Then, we predicted the output for
five unobserved combinations of the model parameters. These are depicted in
the cross of Figure \ref{f:ibex_sim_surr}, those panels with thick, dashed
borders. To the naked eye, these predictive surfaces appear to have been
generated by the simulator itself. Moving left to right, or top to bottom, the
surfaces change gradually and as one would expect. But we also use empirical
results to show that our surrogate fit is accurate.

%------------------------------------------------------------------------------
%% SECTION 3.2.2 - Empirical Results
\subsubsection{Empirical Results}
\label{sec:surr_empir}{}
%------------------------------------------------------------------------------

In order to validate performance of our surrogate model, we run conduct the
following hold-one-out experiment. For each iteration of the test, we fit our
Scaled Vecchia approximated GP surrogate on 65 of the the $n=66$ model runs
available. Then, we predict at the held out combination of model parameters,
but on the same input grid as the 65 other runs. We evaluate using RMSE and
the continuous ranked probability score (CRPS) proposed by
\citet{gneiting2007strictly}. For the purpose of comparison, we execute the
same test for three other competitors: {\tt R} packages {\tt laGP}
\citep{gramacy2014lagp} and {\tt deepgp} \citep{deepGP}, and the current
state-of-the-art, SEPIA \citep{gattiker2020lanl}, implemented in {\tt Python}.
Figure \ref{f:ibex_surr_metrics} displays the results of this bakeoff.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.35, trim=75 150 75 150,clip=TRUE]{ibex_surr_rmse.pdf}
\includegraphics[scale=0.35, trim=75 150 75 150,clip=TRUE]{ibex_surr_crps.pdf}
\caption{Metrics on the IBEX simulator experiment.  Smaller is better for both.
\label{f:ibex_surr_metrics}}
\end{figure}

For now, just pay attention to the SEPIA boxplots where $\#pcs=3$ and the
Scaled Vecchia boxplots where $m=25$. We'll come back to the others in a
moment. It's clear that the Scaled Vecchia approximation outperforms {\tt
laGP} and {\tt deepgp} in both RMSE and CRPS. {\tt deepgp} is fully Bayesian,
utilizing MCMC to sample from the posterior distribution. But that comes with
a cost. Because of the significant computational burden, we were only able to
run {\tt deepgp} for 1000 MCMC iterations, burning in 500 and thinning by 10.
Even this limitation took a considerable amount of time. SEPIA performed
similarly to our Scaled Vecchia approximation, illustrating why it's had such
staying power over the past two decades. As a practitioner, it appears to be a
toss-up between these two methods for which to use. But that's not the entire
story.

In addition to assessing accuracy and uncertainty quantification through RMSE
and CRPS, respectively, we aimed to evaluate the speed of execution for all
competitors involved. In our situation, two factors exist that contribute to a
computational bottleneck. First, the dimension of the simulator response,
which has been the main focus of our work. Additionally, the number of runs
can be varied, based on the expense of a simulation run. To evaluate this, we
conducted two more experiments. For varying dimension, we modified the
dimensionality of the response from 200 to 20,000, keeping the number of runs
constant. For the two methods that are most equipped for large dimensions
(Scaled Vecchia and SEPIA), we attempt to stretch their capacity by cranking
up the length of the response vector from 20-100K, by 10K. Next, we consider
modifying the size of the training dataset. We keep the dimensionality of the
response at 10K, but slide the number of simulator runs from 10 to 100. Again,
for the higher performing methods, we push further up to $n=200$. Results are
displayed in Figure \ref{f:ibex_surr_timing}.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.35, trim=75 150 75 150,clip=TRUE]{ibex_surr_timing.pdf}
\includegraphics[scale=0.35, trim=75 150 75 150,clip=TRUE]{ibex_surr_timing_zoom.pdf}
\includegraphics[scale=0.35, trim=75 150 75 150,clip=TRUE]{ibex_surr_timing_zoom.pdf}
\caption{Timing metrics on the IBEX simulator experiment.
\label{f:ibex_surr_timing}}
\end{figure}

% Field observations are made at 40 unique locations via an LHS design
% over $x_1$ and $x_2$. At each location, exposure time (in this simulated
% example, this really means taking a certain number of draws from a Poisson and
% summing the counts together), varies between one and ten. An LHS design of
% size $n=100$ over the four computer model parameters is also made, resulting
% in 40,000 unique responses. We run Algorithm \ref{alg:gibbs} by fitting a GP
% surrogate to the simulator output and running MCMC to sample from the
% posterior distribution of the model parameters. Results are shown in Figure
% \ref{f:logit2_results}.

% \begin{figure}[ht!]
% \centering
% \includegraphics[scale=0.54,trim=35 0 0 0,clip=TRUE]{logit2_stand_in.pdf}
% \caption{Observed rate of energetic neutral atoms (ENA) detected by the
% Interstellar Boundary Explorer (IBEX) satellite (left) and output from two
% runs of the IBEX computer simulation at different settings (middle, right).
% \label{f:logit2_results}}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 4: Simulations
\section{Simulations}
\label{sec:sims}

%------------------------------------------------------------------------------
%% SECTION 4.1 - Heliospheric Science
\subsection{Heliospheric Science}
\label{sec:helio_science}
%------------------------------------------------------------------------------

%% HELIOSPHERIC SCIENCE
%% - Short review of heliosphere
%% - Behavior of Energetic Neutral Atoms
Over the past 15 years, physicists at Princeton University and Los Alamos
National Lab have developed theoretical models to explain the existence of the
GDF and \textit{ribbon}, along with their corresponding shape and intensity.
In these models, the shape and intensity of the GDF and ribbon are controlled
by four parameters: parallel mean free path, ratio, parameter3, and
parameter4. We leave a more in depth physical interpretation of these
parameters to more appropriate venues
\citep{zirnstein2021dependence,zirnsteinGDFsims2015}. However, as an example,
the parallel mean free path represents the distance a hydrogen ion travels
beyond the heliopause before interacting with another particle, receiving an
electron, and becoming an ENA. Physicists engaged in this work have developed
computer models to represent the physical model they have proposed. Therefore,
given specific values for our four paramters, we can run the model and output
a vector of proposed ENA rates at specific spherical coordinates. The
resolution of the output can be varied, but typical runs output ~16,000 rates,
corresponding to a 2 degree grid over latitude and longitude.

As stated previously, Bayesian Inverse problems are concerned with leveraging
simulator output to make more accurate predictions at out-of-sample field
locations. Additionally, the distribution of calibration parameters can be
estimated. The latter purpose is the primary interest of physicists involved
in this project, as estimation of these parameters will give credence to what
model best represents the data. However, we will also show that using computer
model output enables us to make better predictions on holdout satellite data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Heliospheric Computer Model}
\label{sec:helio_comp_model}

%% HELIOSPHERE COMPUTER MODEL
%% - Introduce computer model
%% - Explanation of parameters in computer model
%% - Introduce the data from the computer model

We have access to 66 runs of the simulator. I will explain the rest of the data.

In order to showcase and validate our proposed methodology on the IBEX data, we will
first test on simulated sky maps. To do this, we will treat one output from the
computer model simulation as the ``truth.'' Using this output as the underlying mean
ENA rate, we will draw Poisson counts with exposure time specified by the real IBEX
data. Figure \ref{f:ibex_synth_ex} shows both the ``truth'' from the simulation (top
right) and the ``observed'' simulated data. Note that the spread of the data is as if
it came from orbits of the IBEX satellite, along with strings of missing data along
certain orbits.

Now, treating the simulated data as if it came from the satellite, we run our Poisson
computer model calibration code and estimate the underlying parameters. In Figure
\ref{f:ibex_synth_ex}, the bivariate posterior distribution of parallel mean free
path and ratio are displayed. We can see that our framework does a good job of
obtaining the true combination of parameters, with the posterior mode lying near the
truth. Additionally, the truth falls squarely in the 95\% credible interval. As a
sanity check, we take the value of each parameter at the posterior mode and insert
those into our fitted surrogate model. The predicted output is shown in the bottom
right of Figure \ref{f:ibex_synth_ex}. It is obvious that there are some differences
between the estimated sky map and the ``truth.'' The shape and intensity of the
ribbon in each sky map is slightly different. However, it is also abundantly clear
that each map (truth and estimated) could have reasonably generated the data we
``observe''in the top left plot. Therefore, we can confidently assert that our
framework recovers the truth.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.25,trim=0 0 0 0,clip=TRUE]{sim_ibex_real_calib.png}
\caption{Simulated satellite data based on an output from the computer model (top
left). True simulator output (top right). Estimated calibration parameters (bottom
left). Estimated output via surrogate at estimated calibration parameters (bottom
right).
\label{f:ibex_synth_ex}}
\end{figure}

We can apply this method to each unique combination of calibration parameters within
our simulator output. Due to the inability for the model to provide good
distributions for bounday parameters, we don't consider combinations for which at
least one parameter is on its edge. For instance, when parallel mean free path is
either 500 or 300 and/or ratio is either 0.0001 and 0.1. This results in 36 unique
combinations. For each combination, we follow the same procedure as described above.
First, we elect one unique combination and create simulated observed satellite data
based on the computer model. Next, we remove that combination from the training data
used to fit our Scaled Vecchia Gaussian Process surrogate model. And finally, we run
our calibration framework to retrieve estimates for the calibration parameters. Our
results are shown in Figure \ref{f:synth_estimates_all}. In each plot of the
posterior distributions, we have included the true combination of parameters that
generated the simulated data. Our 95\% credible intervals encompass the truth in 34
of the 36 of the iterations, near the nominal coverage rate we'd expect.

In addition to estimating the calibration parameters, we can utilize our framework to
achieve better prediction at out-of-sample field locations. While we have not dealt
with real data from the satellite just yet, we can simulate satellite data as before
and run cross-validation on holdout sets.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.3,trim=0 0 0 0,clip=TRUE]{calib_estimates_all.png}
\caption{Posterior distributions for estimated calibration parameters for each unique
combination of values that has been treated as a holdout set.
\label{f:synth_estimates_all}}
\end{figure}

%------------------------------------------------------------------------------
%% SECTION 4.2 - Experiment
\subsection{Experiment}
\label{sec:ibex_sim_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from simulated data
%% - Lead-in to real data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 5: Real Data
\section{Real Data}
\label{sec:ibex_real}

%------------------------------------------------------------------------------
%% SECTION 5.1 - IBEX
\subsection{IBEX}
\label{sec:ibex}
%------------------------------------------------------------------------------

%% IBEX
%% - Reintroduce satellite
%% - Explain details of data
%% - Explain the years gathered

%------------------------------------------------------------------------------
%% SECTION 5.2 - EXPERIMENT
\subsection{Experiment}
\label{sec:ibex_exp}
%------------------------------------------------------------------------------

%% EXPERIMENT
%% - Explain set up of the experiment
%% - Show results from the real data
%% - Comment on the missalignment between simulator and reality

%------------------------------------------------------------------------------
%% SECTION 5.3 - DISCREPANCY
\subsection{Discrepancy}
\label{sec:ibex_discrep}
%------------------------------------------------------------------------------

%% DISCREPANCY
%% - Refer to KOH, Higdon, and their use of discrepancy
%% - Introduce simple scaling discrepancy (iniclude some equations)
%% - Refer to results in Appendix for simulated data where we discover the true scale
%% - Explain set up for IBEX calibration with discrepancy
%% - Show results
%% - Comment on complexity of discrepancy. Scale isn't sufficient.
%% - Perhaps a stationary GP is not even sufficient

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION 6: Discussion
\section{Discussion}
\label{sec:discuss}

Discussion.

\subsection*{Acknowledgments}

RBG and SDB are grateful for funding from NSF CMMI 2152679. This work has been
approved for public release under LA-UR-??-?????. SDB, DO and LJB were funded
by Laboratory Directed Research and Development (LDRD) Project 20220107DR.

\bibliography{ibex_bayes_inv}
\bibliographystyle{jasa}

\appendix

\end{document}
